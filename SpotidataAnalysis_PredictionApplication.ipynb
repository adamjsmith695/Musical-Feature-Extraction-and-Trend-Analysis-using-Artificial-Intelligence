{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fM-9ubS8TajR",
      "metadata": {
        "id": "fM-9ubS8TajR"
      },
      "source": [
        "#**Section 1** - Introduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KVzxa12vS7Dd",
      "metadata": {
        "id": "KVzxa12vS7Dd"
      },
      "source": [
        "This is a report on a **data analysis**, including **regression** and a **neural network** trained on a large preprocessed dataset originally extracted from the music streaming platform *Spotify*.\n",
        "\n",
        "**The aim is to be able to predict the popularity based on musical data**, and **to reveal and retrieve informative insights into the realtionships between musical data and popularity**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LPFtYj5xSDqv",
      "metadata": {
        "id": "LPFtYj5xSDqv"
      },
      "source": [
        "##**1.1**  Instantiation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OIU6zCKySfIQ",
      "metadata": {
        "id": "OIU6zCKySfIQ"
      },
      "source": [
        "###***1.1.1*** - External Library Importation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1RcgZ4lVVdLX",
      "metadata": {
        "id": "1RcgZ4lVVdLX"
      },
      "source": [
        "The following section initialises the necessary libraries and dependencies used throughout this project. Each module serves a specific role in supporting data manipulation, visualisation, machine learning and deep learning.\n",
        "\n",
        "Identifying module imports. This includes **additional** modules:\n",
        "> - **numpy** as *np*\n",
        "> - **random** from ***numpy***\n",
        "> - **pyplot** from ***matplotlib*** as *plt*\n",
        "> - **pandas** as *pd*\n",
        "> - **seaborn** as *sns*\n",
        "> - **tensorflow** as *tf*\n",
        "\n",
        "> - **sklearn** - including specifically:\n",
        ">> - ***OneHotEncoder*** from *preprocessing*,\n",
        ">> - ***train_test_split*** from *model_selection*,\n",
        ">> - ***metrics*** as *mt*,\n",
        ">> - ***LinearRegression*** from *linear_model*\n",
        ">> - and ***RandomForestRegressor*** from *ensemble*\n",
        "\n",
        "> - **xgboost** as *xgb*\n",
        "\n",
        "> - **tqdm** from *tqdm*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cf28c95",
      "metadata": {
        "id": "1cf28c95"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy import random\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics as mt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import xgboost as xgb\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "from tqdm import tqdm   #Progress bar\n",
        "#if loading bars are not working properly, change to this version:\n",
        "# from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xB8ovXs2p9aQ",
      "metadata": {
        "id": "xB8ovXs2p9aQ"
      },
      "source": [
        "> **NumPy** - numerical array operations and randomness for reproducibility\n",
        "\n",
        "> **Pandas** - efficient tabular data storage, manipulation and cleaning\n",
        "\n",
        "> **Matplotlib and Seaborn** - essential tools for static and interactive plotting\n",
        "\n",
        "> **TensorFlow** - enables construction, training and deployment of deep neural networks\n",
        "\n",
        "> **scikit-learn** - for classical machine learning, including regression and data splitting\n",
        "\n",
        "> **XGBoost** - an advanced gradient-boosting framework highly effective for structured data\n",
        "\n",
        "> **TQDM** - provides loading bars for any iterable, useful during long computations\n",
        "\n",
        "The combination of these libraries forms the foundation of the analysis workflow, especially given the scale of the dataset (~1,000,000 entries), and the multistage processing pipeline required for this prediction task."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_tBs7FMXn1St",
      "metadata": {
        "id": "_tBs7FMXn1St"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JyQkoSSNO5lk",
      "metadata": {
        "id": "JyQkoSSNO5lk"
      },
      "source": [
        "This sets our results up to be more interpretable when using **Pandas**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hefsEFOhO4RG",
      "metadata": {
        "id": "hefsEFOhO4RG"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
        "\n",
        "#https://stackoverflow.com/questions/55394854/how-to-change-the-format-of-describe-output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Fevg3yxRPek4",
      "metadata": {
        "id": "Fevg3yxRPek4"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "F0xekmaQTlOx",
      "metadata": {
        "id": "F0xekmaQTlOx"
      },
      "source": [
        "### ***1.1.2*** - Data Extraction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sBcyfMopWIQ9",
      "metadata": {
        "id": "sBcyfMopWIQ9"
      },
      "source": [
        "The raw dataset consists of approximately one million songs originally sourced from Spotify's API, and then sourced from *Kaggle* - https://www.kaggle.com/datasets/amitanshjoshi/spotify-1million-tracks/code. Each entry includes both numerical audio features (e.g. danceability, energy, loudness, valence) and categorical metadata (e.g. artist name, genre, release year). The dataset also includes the song's popularity score, which serves as the primary target variable for prediction throughout this project.\n",
        "\n",
        "**ENSURE TO LOAD 'spotidata.csv' (the *spotify-1million-tracks* dataset) INTO THE CURRENT WORK SPACE OF THE NOTEBOOK BEFORE RUNNING.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff68117a",
      "metadata": {
        "id": "ff68117a"
      },
      "outputs": [],
      "source": [
        "#import the dataset\n",
        "df = pd.read_csv('spotidata.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tY3q4qHqKu5C",
      "metadata": {
        "id": "tY3q4qHqKu5C"
      },
      "source": [
        "**[END]**\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "g1y0YgtuFtHP",
      "metadata": {
        "id": "g1y0YgtuFtHP"
      },
      "source": [
        "##**1.2**  Initial Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "k0HEjiLatV82",
      "metadata": {
        "id": "k0HEjiLatV82"
      },
      "source": [
        "Before any form of cleaning or transformation, it is important to gain a clear understanding of the raw dataset's structure and contents. This stage acts as a first-pass check for any unusual entries or issues that might affect downstream processing. Exploring the general shape, statistical distribution, and feature completeness can often reveal inconsistencies, outliers or early indicators of correlation.\n",
        "\n",
        "At this point, we are primarily interested in:\n",
        "\n",
        "> **Basic structure** - how many rows and features are present, and whether the data types appear appropriate (e.g. numerical for audio features, categorical for genre and key).\n",
        "\n",
        "> **Distribution of values** - inspecting histograms helps identify skew, clustering or potentially problematic values such as extreme outliers in tempo, loudness or popularity.\n",
        "\n",
        "> **Missing or null data** - a quick check ensures that we understand what proportion of the data may need to be dropped or imputed later.\n",
        "\n",
        "> **Coverage across categories** - such as genre and year, as sparse classes or unusual categories may need to be removed.\n",
        "\n",
        "This early analysis helps guide decisions in the preprocessing stage, where more deliberate shaping of the dataset takes place."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GUDi3O8IHwe6",
      "metadata": {
        "id": "GUDi3O8IHwe6"
      },
      "source": [
        "###***1.2.1*** - Basic Structure"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V_uA4-cgN7vb",
      "metadata": {
        "id": "V_uA4-cgN7vb"
      },
      "source": [
        "Previewing the Dataset and the Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gYiDWEEnN78N",
      "metadata": {
        "id": "gYiDWEEnN78N"
      },
      "outputs": [],
      "source": [
        "display(df)\n",
        "\n",
        "print(f\"Number of songs: {df.shape[0]}\")\n",
        "print(f\"Number of features: {df.shape[1]}\")\n",
        "print(\"\\n=== Data Types ===\")\n",
        "print(df.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n"
      ],
      "metadata": {
        "id": "5GcKZaADmYc6"
      },
      "id": "5GcKZaADmYc6"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Unnormalised Min. and Max. Values ===\\n\")\n",
        "# Get the max row\n",
        "max_values = df.describe().loc['max']\n",
        "\n",
        "cols_to_display = max_values > 1\n",
        "\n",
        "# Display the descriptive statistics DataFrame filtered by the boolean mask\n",
        "display(df.describe().loc[:, cols_to_display])"
      ],
      "metadata": {
        "id": "D-KSonIQmZGv"
      },
      "id": "D-KSonIQmZGv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "5sYe99qMNlUo",
      "metadata": {
        "id": "5sYe99qMNlUo"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PZjuidk1Id83",
      "metadata": {
        "id": "PZjuidk1Id83"
      },
      "source": [
        "###***1.2.2*** - Distribution of Values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-qai80azIegh",
      "metadata": {
        "id": "-qai80azIegh"
      },
      "source": [
        "Summary statistics for numerical columns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ydicaWB-IeC3",
      "metadata": {
        "id": "ydicaWB-IeC3"
      },
      "outputs": [],
      "source": [
        "print(\"\\n=== Descriptive Statistics ===\\n\")\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "N1OqyrzHNneA",
      "metadata": {
        "id": "N1OqyrzHNneA"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IX3UIu1SIHR1",
      "metadata": {
        "id": "IX3UIu1SIHR1"
      },
      "source": [
        "###***1.2.3*** - Missing or null Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IDpki-doISsQ",
      "metadata": {
        "id": "IDpki-doISsQ"
      },
      "source": [
        "Missing data check:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QvYP6k72HYf_",
      "metadata": {
        "id": "QvYP6k72HYf_"
      },
      "outputs": [],
      "source": [
        "print(\"\\n=== Null values per column ===\\n\")\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hk-EMNL3No0o",
      "metadata": {
        "id": "hk-EMNL3No0o"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HZhyPkbaJbwM",
      "metadata": {
        "id": "HZhyPkbaJbwM"
      },
      "source": [
        "###***1.2.4*** - Coverage across Categories"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GWtUoToLezEU",
      "metadata": {
        "id": "GWtUoToLezEU"
      },
      "source": [
        "Printing all unique categories, keys and years that appear in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3APoxZ1zHYnj",
      "metadata": {
        "id": "3APoxZ1zHYnj"
      },
      "outputs": [],
      "source": [
        "genres = []\n",
        "keys = []\n",
        "years = []\n",
        "\n",
        "\n",
        "for genre in df['genre'].unique():\n",
        "  genres.append(genre)\n",
        "\n",
        "for key in df['key'].unique():\n",
        "  keys.append(key)\n",
        "\n",
        "for year in df['year'].unique():\n",
        "  years.append(year)\n",
        "\n",
        "\n",
        "print(len(genres),\"different Genres in total:\",df['genre'].unique(),\"\\n\\n\")\n",
        "print(len(keys),\"different Keys in total:\",df['key'].unique(),\"\\n\\n\")\n",
        "print(len(years),\"different Years in total:\",df['year'].unique(),\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mKayn9CtWUbJ",
      "metadata": {
        "id": "mKayn9CtWUbJ"
      },
      "source": [
        "**[END]**\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V7oSLtdP7KQm",
      "metadata": {
        "id": "V7oSLtdP7KQm"
      },
      "source": [
        "##**1.3**  Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Oh3FVjqd7XI3",
      "metadata": {
        "id": "Oh3FVjqd7XI3"
      },
      "source": [
        "###***1.3.1*** - Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZMw4X2Z7QMGV",
      "metadata": {
        "id": "ZMw4X2Z7QMGV"
      },
      "source": [
        "This section removes any incomplete or redundant entries from the dataset. Null values are dropped to avoid issues during model training, particularly because imputation is not appropriate for our audio-based features in this circumstance. Additionally, The duplicate index column - introduced during CSV export - is also removed to ensure clean column alignment throughout.\n",
        "\n",
        "In addition to removing null values and redundant index columns, this section also filters out genres that are either underrepresented or unsuitable for generalised modelling and poorly identified time signatures. For genres, this includes edge-case categories (e.g. *children*, *comedy*) and culturally localised genres with insufficient representation. Removing these helps focus the analysis on mainstream and musically consistent content, improving overall model reliability."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "L1eNpOcBIU3Q",
      "metadata": {
        "id": "L1eNpOcBIU3Q"
      },
      "source": [
        "**Removing null data:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0QgObJ4S749V",
      "metadata": {
        "id": "0QgObJ4S749V"
      },
      "outputs": [],
      "source": [
        "# Drop any rows with null (missing) values\n",
        "df = df.dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f0Ma_oUITCC",
      "metadata": {
        "id": "8f0Ma_oUITCC"
      },
      "source": [
        "**removing duplicate index column:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FUjN4FBUIQMv",
      "metadata": {
        "id": "FUjN4FBUIQMv"
      },
      "outputs": [],
      "source": [
        "#removing duplicate index column\n",
        "df = df.drop(df.columns[0],axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vtzwT80rIpO_",
      "metadata": {
        "id": "vtzwT80rIpO_"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "I3bgs8ujETRL",
      "metadata": {
        "id": "I3bgs8ujETRL"
      },
      "source": [
        "**Removing rows with erroneous time signatures:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GrNcClA4CZps",
      "metadata": {
        "id": "GrNcClA4CZps"
      },
      "outputs": [],
      "source": [
        "df = df[df.time_signature != 0]\n",
        "df = df[df.time_signature != 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6j4yAkwHfOcl",
      "metadata": {
        "id": "6j4yAkwHfOcl"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dAEZLJae7pD9",
      "metadata": {
        "id": "dAEZLJae7pD9"
      },
      "source": [
        "### ***1.3.2*** - Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_4V5SyR3WQ3A",
      "metadata": {
        "id": "_4V5SyR3WQ3A"
      },
      "source": [
        "This section introduces new features that capture group-level patterns within the data. By calculating average popularity and song count for each artist, genre and year, we embed contextual information that helps models generalise more effectively. These engineered variables help capture popularity trends not immediately obvious from individual song-level features alone."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TpmNSKYqfi1d",
      "metadata": {
        "id": "TpmNSKYqfi1d"
      },
      "outputs": [],
      "source": [
        "def generate_average_popularity(df, category, popularity, title=\"\"):\n",
        "\n",
        "  i = 0  # counter to track how many songs in current group\n",
        "  prevVal = category[0]  # start with the first category value\n",
        "  total_pop = 0  # running total of popularity for group\n",
        "\n",
        "  category_avg_popularity = []  # will store per-row average popularity\n",
        "  category_song_count = []      # will store per-row song count\n",
        "\n",
        "  for x in range(0, len(category) + 1):  # loop over all rows (+1 to handle last group)\n",
        "\n",
        "    if x < len(category):  # still inside bounds\n",
        "\n",
        "      if category[x] == prevVal:  # still in the same group\n",
        "        total_pop += int(popularity[x])  # add this song's popularity\n",
        "        i += 1  # increment group size\n",
        "\n",
        "      else:  # new category group starts\n",
        "\n",
        "        # fill the previous group’s values for each row\n",
        "        for z in range(i):\n",
        "          category_avg_popularity.append(total_pop / i)\n",
        "          category_song_count.append(i)\n",
        "\n",
        "        # reset values for the new group\n",
        "        total_pop = int(popularity[x])\n",
        "        i = 1\n",
        "\n",
        "      prevVal = category[x]  # update tracker to new category\n",
        "\n",
        "    else:  # this is after the final row – need to flush the last group\n",
        "      for z in range(i):\n",
        "        category_avg_popularity.append(total_pop / i)\n",
        "        category_song_count.append(i)\n",
        "\n",
        "  # assign the calculated values as new columns in the dataframe\n",
        "  df[f'{title}_avg_popularity'] = category_avg_popularity\n",
        "  df[f'{title}_song_count'] = category_song_count\n",
        "\n",
        "  return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9D7-wx8gVzS",
      "metadata": {
        "id": "b9D7-wx8gVzS"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JtLf78P-9bdV",
      "metadata": {
        "id": "JtLf78P-9bdV"
      },
      "source": [
        "**Introducing *artist_avg_popularity*  into the DataFrame**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "t4Wtwdmk8A4C",
      "metadata": {
        "id": "t4Wtwdmk8A4C"
      },
      "outputs": [],
      "source": [
        "df = df.sort_values(by='artist_name')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oPGeKPIP8Ck9",
      "metadata": {
        "id": "oPGeKPIP8Ck9"
      },
      "outputs": [],
      "source": [
        "artists = df['artist_name'].to_numpy()\n",
        "popularity = df['popularity'].to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0vDaesYf8Gkq",
      "metadata": {
        "id": "0vDaesYf8Gkq"
      },
      "outputs": [],
      "source": [
        "df = generate_average_popularity(df, artists, popularity, 'artist')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "I3Ok3qcm8k2k",
      "metadata": {
        "id": "I3Ok3qcm8k2k"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Wm4Cv7lc8Qb1",
      "metadata": {
        "id": "Wm4Cv7lc8Qb1"
      },
      "source": [
        "**Introducing *genre_avg_popularity* into the DataFrame**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TXbHYFXR8oUj",
      "metadata": {
        "id": "TXbHYFXR8oUj"
      },
      "outputs": [],
      "source": [
        "df = df.sort_values(by='genre')\n",
        "#df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Zae3eqrL8wwx",
      "metadata": {
        "id": "Zae3eqrL8wwx"
      },
      "outputs": [],
      "source": [
        "genre = df['genre'].to_numpy()\n",
        "popularity = df['popularity'].to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9nwlkfJvhSkM",
      "metadata": {
        "id": "9nwlkfJvhSkM"
      },
      "outputs": [],
      "source": [
        "df = generate_average_popularity(df, genre, popularity, 'genre')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9izEjbrF8nZV",
      "metadata": {
        "id": "9izEjbrF8nZV"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "j9YYjZAYAwZZ",
      "metadata": {
        "id": "j9YYjZAYAwZZ"
      },
      "source": [
        "**Introducing *year_avg_popularity* into the DataFrame**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tBK6t2Y4Ay15",
      "metadata": {
        "id": "tBK6t2Y4Ay15"
      },
      "outputs": [],
      "source": [
        "df = df.sort_values(by='year')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bhiSDn5qBFoE",
      "metadata": {
        "id": "bhiSDn5qBFoE"
      },
      "outputs": [],
      "source": [
        "year = df['year'].to_numpy()\n",
        "popularity = df['popularity'].to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EINFfiGphYRi",
      "metadata": {
        "id": "EINFfiGphYRi"
      },
      "outputs": [],
      "source": [
        "df = generate_average_popularity(df, year, popularity, 'year')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YQ58QteTijYq",
      "metadata": {
        "id": "YQ58QteTijYq"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ufG5QDdzikdl",
      "metadata": {
        "id": "ufG5QDdzikdl"
      },
      "outputs": [],
      "source": [
        "columnsToDisplay = ['artist_name', 'popularity', 'year', 'genre', 'artist_avg_popularity',\n",
        "                    'artist_song_count', 'year_avg_popularity', 'year_song_count',\n",
        "                    'genre_avg_popularity', 'genre_song_count']\n",
        "\n",
        "display(df[columnsToDisplay].sort_values(by='year', ascending=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64bnX9N0AyL5",
      "metadata": {
        "id": "64bnX9N0AyL5"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "z9v9RyiKzCtO",
      "metadata": {
        "id": "z9v9RyiKzCtO"
      },
      "source": [
        "###***1.3.3*** - Data Shaping"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bsdcoj22Wxmf",
      "metadata": {
        "id": "bsdcoj22Wxmf"
      },
      "source": [
        "The `duration_ms` column is converted to minutes to improve readability and interpretability. This is particularly useful for plotting and UI presentation later on. The original column is dropped after conversion to avoid redundancy."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WGwv7C3gpsjP",
      "metadata": {
        "id": "WGwv7C3gpsjP"
      },
      "source": [
        "**Re-scaling *'duration_ms'* :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AaiUgSF7zP6c",
      "metadata": {
        "id": "AaiUgSF7zP6c"
      },
      "outputs": [],
      "source": [
        "df['duration_mins'] = df['duration_ms'] / 60000 #becomes duration in minutes\n",
        "df = df.drop(columns=['duration_ms'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "o3sUMVkOpXnH",
      "metadata": {
        "id": "o3sUMVkOpXnH"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cXc7V6JXHN3",
      "metadata": {
        "id": "6cXc7V6JXHN3"
      },
      "source": [
        "**Setting *'year'* to Integer DataType:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RL7zx3aDXHoG",
      "metadata": {
        "id": "RL7zx3aDXHoG"
      },
      "outputs": [],
      "source": [
        "df['year'] = df['year'].astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "USHeDulUcKtw",
      "metadata": {
        "id": "USHeDulUcKtw"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1PzVkYmGcLiF",
      "metadata": {
        "id": "1PzVkYmGcLiF"
      },
      "source": [
        "**Removing 2023 (Incomplete year may skew results inaccurately):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PnPClasElFAs",
      "metadata": {
        "id": "PnPClasElFAs"
      },
      "outputs": [],
      "source": [
        "# Copy the dataset\n",
        "data = df.copy()\n",
        "\n",
        "# Drop duplicates so only one entry per year remains\n",
        "data = data.drop_duplicates(subset='year')\n",
        "\n",
        "# Sort by popularity\n",
        "data = data.sort_values(by='year_avg_popularity', ascending=False)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='year', y='year_avg_popularity', data=data, hue='year_avg_popularity', palette='viridis')\n",
        "plt.xticks(rotation=45)\n",
        "plt.title('Average Popularity by Year')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-0tXxeAAl4Es",
      "metadata": {
        "id": "-0tXxeAAl4Es"
      },
      "source": [
        "\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LEqQivmtcW_-",
      "metadata": {
        "id": "LEqQivmtcW_-"
      },
      "outputs": [],
      "source": [
        "df = df[df['year'] != 2023]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0Hxnk8ZSXenU",
      "metadata": {
        "id": "0Hxnk8ZSXenU"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rHhyneaIXmAV",
      "metadata": {
        "id": "rHhyneaIXmAV"
      },
      "source": [
        "###***1.3.4*** - Data Filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jd4k8J7tYEfO",
      "metadata": {
        "id": "jd4k8J7tYEfO"
      },
      "source": [
        "This section filters out entries that fall outside the scope of meaningful modelling. First, niche and non-musical genres are removed to reduce noise and improve the generalisability of the dataset. Then, numerical features are clipped to realistic bounds based on observed statistical distributions from the initial analysis. This ensures that extreme outliers do not distort learning or affect model scaling.\n",
        "\n",
        "Niche, non-musical and geographically or culturally specific genres (e.g. *gospel*, *anime*, *cantopop*) are removed to reduce noise and improve model generalisability. These genres tend to be underrepresented or structurally inconsistent with mainstream trends, limiting their value in training.\n",
        "\n",
        "Several continuous features are also clipped to realistic bounds based on the distributions observed in **Section 1.2**. This includes notably `loudness` (-60 to 0 dB), `tempo` (50-220 BPM), and `duration_mins` (0-20 mins). Count-based features such as `artist_song_count` and `year_song_count` are similarly clipped to prevent extreme outliers from distorting scale or model behaviour or to provide realistic limits for later use in **Section 5**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "M-Yyx1pMZePK",
      "metadata": {
        "id": "M-Yyx1pMZePK"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uCoGUUT1hkCG",
      "metadata": {
        "id": "uCoGUUT1hkCG"
      },
      "source": [
        "**Clipping unclipped features' decided bounds:**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eJAUgSZ1kvu1",
      "metadata": {
        "id": "eJAUgSZ1kvu1"
      },
      "source": [
        "This helps to ensure that there are no outliers, which can skew the data. Additionally, we need specified bounds determined and placed for normalisation, which is a very important part of data preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VM4e6jyiivOO",
      "metadata": {
        "id": "VM4e6jyiivOO"
      },
      "outputs": [],
      "source": [
        "#                            min           max\n",
        "# acousticness               0.000000      0.996000\n",
        "# danceability               0.000000      0.993000\n",
        "# energy                     0.000000      1.000000\n",
        "# instrumentalness           0.000000      1.000000\n",
        "# liveness                   0.000000      0.700000\n",
        "# loudness---------------- -47.283000      6.172000  BOUND\n",
        "# speechiness                0.000000      0.967000\n",
        "# tempo-------------------   0.000000    249.993000  BOUND\n",
        "# valence                    0.000000      1.000000\n",
        "# artist_avg_popularity      0.000000     85.000000\n",
        "# artist_song_count-------   1.000000   1786.000000  BOUND\n",
        "# genre_avg_popularity       0.001277     55.772119\n",
        "# genre_song_count-------- 525.000000  20964.000000  BOUND\n",
        "# year_avg_popularity       11.955374     33.103448\n",
        "# year_song_count------- 25736.000000  37295.000000  BOUND\n",
        "# duration_mins-----------   0.034550     96.347433  BOUND\n",
        "# popularity                 0.000000    100.000000"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-muP2jIcZiRz",
      "metadata": {
        "id": "-muP2jIcZiRz"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4l3AYc1dZCNS",
      "metadata": {
        "id": "4l3AYc1dZCNS"
      },
      "source": [
        "The following bounds are applied to constrain feature values to realistic and representative ranges:\n",
        "\n",
        "- **Loudness**: clipped to-60 dB (min) and 0 dB (max)  \n",
        "   Reflects the typical range of recorded music, avoiding distortion from extremely quiet or anomalously loud entries.\n",
        "\n",
        "- **Tempo**: clipped to 50 BPM (min) and 220 BPM (max)  \n",
        "  Covers the full practical spectrum of commercial music tempo without skewing towards outliers.\n",
        "\n",
        "- **Duration (minutes)**: clipped to 0 (min) and 20 (max)  \n",
        "  Removes excessively long tracks that behave differently (e.g. live recordings, classical pieces).\n",
        "\n",
        "- **Artist Song Count**: clipped between 1 and 3000  \n",
        "  Prevents disproportionately prolific artists from skewing representation.\n",
        "\n",
        "- **Genre Song Count**: clipped between 0 and 30,000  \n",
        "  Normalises influence of dominant genres without fully discarding them.\n",
        "\n",
        "- **Year Song Count**: clipped between 0 and 50,000  \n",
        "  Ensures even temporal distribution while trimming anomalies in heavily active years.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "v76psxDGhBuD",
      "metadata": {
        "id": "v76psxDGhBuD"
      },
      "outputs": [],
      "source": [
        "# --- Loudness: clip to [-60, 0]\n",
        "df['loudness'] = df['loudness'].clip(lower=-60, upper=0)\n",
        "\n",
        "# --- Tempo: clip to [60, 220]\n",
        "df['tempo'] = df['tempo'].clip(lower=50, upper=220)\n",
        "\n",
        "# --- Duration: clip to [0, 20] (minutes)\n",
        "df['duration_mins'] = df['duration_mins'].clip(lower=0, upper=20)\n",
        "\n",
        "# --- Artist Song Count: clip to [1, 3000]\n",
        "df['artist_song_count'] = df['artist_song_count'].clip(lower=1, upper=3000)\n",
        "\n",
        "# --- Genre Song Count: clip to [1, 30000]\n",
        "df['genre_song_count'] = df['genre_song_count'].clip(lower=0, upper=30000)\n",
        "\n",
        "# --- Year Song Count: clip to [0, 50000]\n",
        "df['year_song_count'] = df['year_song_count'].clip(lower=0, upper=50000)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "214zRgV5iTmY",
      "metadata": {
        "id": "214zRgV5iTmY"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "urcwrqCbX3mG",
      "metadata": {
        "id": "urcwrqCbX3mG"
      },
      "source": [
        "**Removing songs from irrelevant genres**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hTSjx2l9X4UU",
      "metadata": {
        "id": "hTSjx2l9X4UU"
      },
      "source": [
        "We remove niche, geographically-specific genres and non-musical categories to reduce noise and improve the consistency of the dataset for modelling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbgpputDX4Zs",
      "metadata": {
        "id": "dbgpputDX4Zs"
      },
      "outputs": [],
      "source": [
        "genresToDrop = ['study','children', 'comedy', 'show-tunes','turkish',\n",
        "       'german', 'french', 'swedish', 'k-pop',\n",
        "       'j-rock', 'j-pop', 'cantopop','brazil','latino',\n",
        "       'disney','mpb','indian','j-dance','anime','mandopop',\n",
        "       'j-idol','pagode','gospel','afrobeat','ambient',\n",
        "       'classical','malay','pop-film','iranian','sleep',\n",
        "       'world-music','spanish','british','salsa','bluegrass',\n",
        "       'tango','opera','samba','kids','latin','forro','romance',\n",
        "       'sertanejo','chicago-house','detroit-techno']\n",
        "\n",
        "\n",
        "for x in range(len(genresToDrop)):\n",
        "  df = df[df.genre != genresToDrop[x]]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kxPDR8buZpKV",
      "metadata": {
        "id": "kxPDR8buZpKV"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EYaP_uX-92Rz",
      "metadata": {
        "id": "EYaP_uX-92Rz"
      },
      "source": [
        "###***1.3.5*** - Data Separation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "i1JIaRXsbw38",
      "metadata": {
        "id": "i1JIaRXsbw38"
      },
      "source": [
        "The dataset is separated into three logical components to simplify downstream processing:\n",
        "\n",
        "- `descriptors_df` stores non-numeric metadata such as song and artist names. These are excluded from modelling but retained for interpretation and output.\n",
        "- `categorical_df` stores discrete variables such as key, mode, genre and year, which will be one-hot encoded separately.\n",
        "- The remaining dataset (`df`) contains only continuous features and derived statistics. This later becomes the foundation for feature scaling and model training.\n",
        "\n",
        "Separating the data at this stage improves modularity and avoids accidental re-use of irrelevant identifiers during model learning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SnAPQmiGcm0B",
      "metadata": {
        "id": "SnAPQmiGcm0B"
      },
      "source": [
        ".\n",
        "\n",
        "."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YCfdpVhFcH3C",
      "metadata": {
        "id": "YCfdpVhFcH3C"
      },
      "source": [
        "Resetting the index after filtering to ensure all of the indexes are cleanly lined up after seperation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ROq-Tx40lgix",
      "metadata": {
        "id": "ROq-Tx40lgix"
      },
      "outputs": [],
      "source": [
        "df.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lZy0dk_4cZLx",
      "metadata": {
        "id": "lZy0dk_4cZLx"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        "."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "noSd52n6caX0",
      "metadata": {
        "id": "noSd52n6caX0"
      },
      "source": [
        "**Descriptive Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Uu-I5Y9n-Bda",
      "metadata": {
        "id": "Uu-I5Y9n-Bda"
      },
      "source": [
        "Separating non-numerical descriptive data into a separate **DataFrame** *descriptors_df*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Da8ayPUB-Ar5",
      "metadata": {
        "id": "Da8ayPUB-Ar5"
      },
      "outputs": [],
      "source": [
        "#Listing the columns we want to separate\n",
        "columnsToDrop=[\"track_id\",\"artist_name\",\"track_name\"]\n",
        "\n",
        "\n",
        "#creating new empty DataFrame\n",
        "descriptors_df = pd.DataFrame()\n",
        "\n",
        "\n",
        "#adding columns to new DataFrame\n",
        "for column in columnsToDrop:\n",
        "    descriptors_df[column] = df[column]\n",
        "\n",
        "\n",
        "#removing the columns from original DataFrame\n",
        "df = df.drop(labels=columnsToDrop,axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XePWXioe4hxt",
      "metadata": {
        "id": "XePWXioe4hxt"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Gk2zsujM4h2d",
      "metadata": {
        "id": "Gk2zsujM4h2d"
      },
      "outputs": [],
      "source": [
        "descriptors_df['popularity'] = df['popularity']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rOFglmhtpUhC",
      "metadata": {
        "id": "rOFglmhtpUhC"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "U1x-wGYNcu-a",
      "metadata": {
        "id": "U1x-wGYNcu-a"
      },
      "source": [
        "**Categorical Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JUs4J4MFIrWL",
      "metadata": {
        "id": "JUs4J4MFIrWL"
      },
      "source": [
        "Separating categorical value data into a separate **DataFrame** *categorical_df*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "StGeTkCeIqz9",
      "metadata": {
        "id": "StGeTkCeIqz9"
      },
      "outputs": [],
      "source": [
        "#Listing the columns we want to separate\n",
        "columnsToDrop=[\"key\",\"mode\",\"time_signature\",\"year\",\"genre\"]\n",
        "\n",
        "\n",
        "#creating new empty DataFrame\n",
        "categorical_df = pd.DataFrame()\n",
        "\n",
        "\n",
        "#adding columns to new DataFrame\n",
        "for column in columnsToDrop:\n",
        "    categorical_df[column] = df[column]\n",
        "\n",
        "\n",
        "#removing the columns from original DataFrame\n",
        "df = df.drop(labels=columnsToDrop,axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccE78sqR4b07",
      "metadata": {
        "id": "ccE78sqR4b07"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VZ2CcTbi4dN4",
      "metadata": {
        "id": "VZ2CcTbi4dN4"
      },
      "outputs": [],
      "source": [
        "categorical_df['popularity'] = df['popularity']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oJA5h3KCJO-O",
      "metadata": {
        "id": "oJA5h3KCJO-O"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SYa7QRbLox9K",
      "metadata": {
        "id": "SYa7QRbLox9K"
      },
      "source": [
        "###***1.3.6*** - Additional Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "M_6nE-Maa2oW",
      "metadata": {
        "id": "M_6nE-Maa2oW"
      },
      "source": [
        "After cleaning and shaping the main dataset, we will split it into multiple specialised versions, each tailored to a different pipeline within the project. These are not random train/test splits, but structured data variants optimised for specific types of analysis or modelling.\n",
        "\n",
        "- **`pruned_df`** -> *Bias Minimisation Pipeline*  \n",
        "  This version removes songs whose popularity deviates significantly from the artist's average. It is used in early-stage visualisations and exploratory correlation analysis (Section 2) where pruning improves clarity.\n",
        "\n",
        "- **`train_df`** -> *Model Training Pipeline*  \n",
        "  The dataset is one-hot encoded and normalised to allow use in regression and deep learning models. This is the dataset used in all model training tasks (Section 3 onwards).\n",
        "\n",
        "- **`temporal_df`** -> *Temporal Metadata Pipeline*  \n",
        "  Here, a randomised decimal is added to each year value to emulate real-world release distribution across time and to reduce temporal data leakage in the dataset. This enables calculation of more realistic running averages for `year_avg_popularity`, `artist_avg_popularity`,`genre_song_count`, etc. which is used in temporal modelling and timeline-based experiments.\n",
        "\n",
        "Each dataset supports a separate analytical path within the overall architecture of the project, improving both modularity and performance at later stages.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WioU_eGhc_qL",
      "metadata": {
        "id": "WioU_eGhc_qL"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jrW1YCViElPR",
      "metadata": {
        "id": "jrW1YCViElPR"
      },
      "source": [
        "****"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mc3odKJXol78",
      "metadata": {
        "id": "mc3odKJXol78"
      },
      "source": [
        "####**A. Introducing rich-featured *pruned_df***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5N0j9PRDf7sk",
      "metadata": {
        "id": "5N0j9PRDf7sk"
      },
      "source": [
        "This branch of the pipeline generates a refined subset of the dataset, created by removing entries whose popularity is overly explained by artist-level momentum. Unlike traditional denoising — which focuses on removing noisy or extreme outliers — this pruning procedure actively filters *inliers* that align too closely with the dominant feature `artist_avg_popularity`.\n",
        "\n",
        "The goal is to eliminate instances where a song's popularity can be largely predicted from the artist's prior success, thereby improving the diversity and learning value of the remaining dataset. A custom filtering function is used to compare each song's actual popularity with its artist's running average, and remove songs that fall within a predefined scale-sensitive symmetry zone. This ensures that models trained later are encouraged to learn from meaningful variation in the features, rather than overfitting to artist prestige.\n",
        "\n",
        "The result is a more balanced, explanatory subset of the data with fewer redundant or momentum-driven entries. This dataset is used primarily in exploratory correlation analysis in Section *2*, where fairness and feature independence are of interest.\n",
        "\n",
        "- **`pruned_df`** -> *Bias Minimisation Pipeline*  \n",
        "  Removes over-aligned inliers based on artist momentum. Used for more balanced correlation inspection and to reduce popularity bias.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bxLp5jfymD-x",
      "metadata": {
        "id": "bxLp5jfymD-x"
      },
      "outputs": [],
      "source": [
        "pruned_df = df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96AXVrvhR8LS",
      "metadata": {
        "id": "96AXVrvhR8LS"
      },
      "outputs": [],
      "source": [
        "#\"\"\"\n",
        "ax1 = df.plot.scatter(x='popularity',\n",
        "                      y='artist_avg_popularity',\n",
        "                      c='popularity',\n",
        "                      colormap='viridis')\n",
        "#\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SJV4GAyedZFn",
      "metadata": {
        "id": "SJV4GAyedZFn"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "o2Bj84VPdJq3",
      "metadata": {
        "id": "o2Bj84VPdJq3"
      },
      "source": [
        "#####**Engineering Pruning Function**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HlLeWK_o-D63",
      "metadata": {
        "id": "HlLeWK_o-D63"
      },
      "source": [
        "This custom pruning algorithm dynamically removes data points that fall too close to the diagonal correlation between popularity and artist momentum. It identifies a top-right reference point in the feature space — representing the most extreme joint popularity — and constructs a scaled diamond boundary around it. Songs falling within this shape are filtered out as over-aligned, helping to retain only those tracks whose popularity diverges meaningfully from their artist's historical trend.\n",
        "\n",
        "*Note: This logic is best visualised by referring to the explanatory plots at the top and bottom of this section.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iebdT2zBj9m5",
      "metadata": {
        "id": "iebdT2zBj9m5"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IPuyFNwLj4P_",
      "metadata": {
        "id": "IPuyFNwLj4P_"
      },
      "source": [
        "**Defining *X_Max()* :**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TYasYU6Mj2pF",
      "metadata": {
        "id": "TYasYU6Mj2pF"
      },
      "source": [
        "The `X_Max()` function identifies the most extreme composite point in the dataset based on the sum of `popularity` and `artist_avg_popularity`. This point — typically in the top-right of the correlation space — serves as a reference for pruning. It anchors the scaled boundary used to filter out overly momentum-aligned songs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XS5nJMlsyOow",
      "metadata": {
        "id": "XS5nJMlsyOow"
      },
      "outputs": [],
      "source": [
        "#finding the maximum composite popularity between popularity and the average popularity\n",
        "def X_Max(P,A):\n",
        "  x_max = 0\n",
        "\n",
        "  #finds the largest function of  popularity + artist avg. popularity within the dataset\n",
        "  for x in range(len(P)):\n",
        "    p = P[x]\n",
        "    a = A[x]\n",
        "    p_max = P[x_max]\n",
        "    a_max = A[x_max]\n",
        "\n",
        "    if p + a > p_max + a_max:\n",
        "      x_max = x\n",
        "\n",
        "  return x_max"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "F0DOeRobk0S0",
      "metadata": {
        "id": "F0DOeRobk0S0"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7MKz_rW3kd1Y",
      "metadata": {
        "id": "7MKz_rW3kd1Y"
      },
      "source": [
        "**Defining *AbsoluteDiff()* :**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "j3MfWHUFkdHx",
      "metadata": {
        "id": "j3MfWHUFkdHx"
      },
      "source": [
        "The `AbsoluteDiff()` function returns the absolute difference between a song's actual popularity and its artist's average popularity. It is used as the core measure of deviation when determining whether a track should be pruned based on over-alignment with momentum.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "h2-GmIRvOhVU",
      "metadata": {
        "id": "h2-GmIRvOhVU"
      },
      "outputs": [],
      "source": [
        "def AbsoluteDiff(p,a):\n",
        "  return abs(p-a)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6pkE7l4Ak8cZ",
      "metadata": {
        "id": "6pkE7l4Ak8cZ"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ArGhXfe3k4UQ",
      "metadata": {
        "id": "ArGhXfe3k4UQ"
      },
      "source": [
        "**Defining *Filter()* :**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tXUzmsdelC2g",
      "metadata": {
        "id": "tXUzmsdelC2g"
      },
      "source": [
        "The `Filter()` function determines whether a song should be kept or pruned based on its deviation from artist momentum. It compares the absolute difference between a track's popularity and its artist's average against dynamically scaled thresholds, anchored to the maximum observed values. If the deviation exceeds either boundary, the track is retained; otherwise, it is filtered out as overly momentum-aligned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FxTkdqgVPjDv",
      "metadata": {
        "id": "FxTkdqgVPjDv"
      },
      "outputs": [],
      "source": [
        "def Filter(p, a, p_max, a_max, scale):\n",
        "  if(AbsoluteDiff(p, a) > 0.5 * (abs((0.5 * p_max * scale) - (abs(0.5 * p_max - p)))) or\n",
        "    AbsoluteDiff(p, a) > 0.5 * (abs((0.5 * p_max * scale) - (abs(0.5 * a_max - a))))):\n",
        "    return True\n",
        "  else:\n",
        "    return False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WMPds8Edk9br",
      "metadata": {
        "id": "WMPds8Edk9br"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LEXlZ4H6k-x_",
      "metadata": {
        "id": "LEXlZ4H6k-x_"
      },
      "source": [
        "**Defining *Prune()* :**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98_MpziDloIA",
      "metadata": {
        "id": "98_MpziDloIA"
      },
      "source": [
        "The `Prune()` function applies the pruning logic across the entire dataset. It uses `X_Max()` to locate the most extreme joint popularity point and passes each row through the `Filter()` function. Only songs that deviate significantly from their artist's average popularity are retained. The result is a new pruned DataFrame `Q`, containing only entries that carry meaningful, independent popularity signals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "F3U91aE81yJJ",
      "metadata": {
        "id": "F3U91aE81yJJ"
      },
      "outputs": [],
      "source": [
        "def Prune(df,P,A,scale):\n",
        "  p = P.to_numpy()\n",
        "  a = A.to_numpy()\n",
        "  Q = []\n",
        "\n",
        "  x_max = X_Max(p,a)\n",
        "  p_max = p[x_max]\n",
        "  a_max = a[x_max]\n",
        "\n",
        "  for x in range(0,len(p)):\n",
        "    if (Filter(p[x], a[x], p_max, a_max, scale) == True):\n",
        "      Q.append(df.iloc[x])\n",
        "\n",
        "  Q = pd.DataFrame(Q)\n",
        "  Q.columns = df.columns\n",
        "\n",
        "  return Q"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8pWW9W5wwbGj",
      "metadata": {
        "id": "8pWW9W5wwbGj"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eaW1if4zoIKM",
      "metadata": {
        "id": "eaW1if4zoIKM"
      },
      "source": [
        "#####**Pruning the DataFrame**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BBLzF9UlwQQD",
      "metadata": {
        "id": "BBLzF9UlwQQD"
      },
      "source": [
        "After applying the `Prune()` function with a calibrated scale value, the resulting `pruned_df` retains only those tracks whose popularity deviates meaningfully from their artist's average. The scatter plot below visualises this relationship, where each point reflects a song's absolute popularity versus its artist's average at release. The pruning effectively removes the tightly clustered, momentum-aligned middle region, revealing two distinct slopes of valid variability.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-ZwZgeF2mPkD",
      "metadata": {
        "id": "-ZwZgeF2mPkD"
      },
      "outputs": [],
      "source": [
        "pruned_df = Prune(pruned_df,pruned_df['popularity'],pruned_df['artist_avg_popularity'],scale=1.55)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w56_7xWbPeSS",
      "metadata": {
        "id": "w56_7xWbPeSS"
      },
      "outputs": [],
      "source": [
        "#\"\"\"\n",
        "ax1 = pruned_df.plot.scatter(x='popularity',\n",
        "                      y='artist_avg_popularity',\n",
        "                      c='popularity',\n",
        "                      colormap='viridis')\n",
        "#\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bEpPDn3dOOi4",
      "metadata": {
        "id": "bEpPDn3dOOi4"
      },
      "source": [
        "**[END]**\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "M6Y_ZFxOEr5v",
      "metadata": {
        "id": "M6Y_ZFxOEr5v"
      },
      "source": [
        "****"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "L4scBdq_oyzn",
      "metadata": {
        "id": "L4scBdq_oyzn"
      },
      "source": [
        "####**B. Introducing one-hot-encoded normalised *train_df***\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2UPPqhjv3Q2C",
      "metadata": {
        "id": "2UPPqhjv3Q2C"
      },
      "source": [
        "This branch of the pipeline is used to prepare the dataset for all supervised learning tasks. It combines both manual one-hot encoding of categorical variables and normalisation of continuous variables, producing a clean, model-ready format referred to as `train_df`.\n",
        "\n",
        "Manual one-hot encoding is applied to categorical variables including genre, key, mode, and time signature. This is done mostly manually — rather than using automated library functions — to retain full control over the naming of resulting features and to better understand how categorical values are mapped. This step improves both interpretability and downstream UI integration. Each categorical variable is encoded separately and added back to the dataset.\n",
        "\n",
        "Next, continuous features that fall outside of the 0-1 range are normalised. These variables are scaled based on their known min-max values or within defined bounds to ensure that no feature dominates the model due to scale. The features include loudness, tempo, duration, artist and genre statistics, and temporal context variables.\n",
        "\n",
        "This version of the dataset forms the core input to all modelling stages in Sections *3* and *4*. It represents the most refined and standardised structure available in the pipeline — fully numerical, bounded, and containing no nulls or unused metadata.\n",
        "\n",
        "- **`train_df`** -> *Model Training Pipeline*  \n",
        "  This is the final dataset used for model training. All categorical features are one-hot encoded, continuous values are scaled to [0, 1]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PpMyPVqR0Pdz",
      "metadata": {
        "id": "PpMyPVqR0Pdz"
      },
      "outputs": [],
      "source": [
        "train_df = df.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "okiERShXgDYD",
      "metadata": {
        "id": "okiERShXgDYD"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_vGAgzYAztqv",
      "metadata": {
        "id": "_vGAgzYAztqv"
      },
      "source": [
        "#####**One-Hot-Encoding**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "u0JI_x9w4Wod",
      "metadata": {
        "id": "u0JI_x9w4Wod"
      },
      "source": [
        "Manual one-hot encoding is applied to selected categorical features such as genre, key, mode, and time signature. This approach is used instead of a library-based method (e.g. `OneHotEncoder` from `sklearn.preprocessing`) for two main reasons:\n",
        "\n",
        "- **Naming Control**: Manual encoding allows complete control over the naming of the resulting variables. This is useful for interpretability, presentation, and consistency with later UI integration.\n",
        "\n",
        "- **Understanding**: Implementing one-hot encoding manually also offers visibility into the structure of the transformation. It ensures a clearer understanding of how categorical information is converted into model-ready numeric format, which is useful for both debugging and learning.\n",
        "\n",
        "Each categorical variable is encoded independently and merged back into `train_df` once complete."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8PTBW2GIxCx7",
      "metadata": {
        "id": "8PTBW2GIxCx7"
      },
      "source": [
        "**For *'key'* :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "g7e_8kc09LXW",
      "metadata": {
        "id": "g7e_8kc09LXW"
      },
      "outputs": [],
      "source": [
        "#print(categorical_df[\"key\"].unique())\n",
        "# 0=C 1=Db 2=D 3=Eb 4=E 5=F 6=Gb 7=G 8=Ab 9=A 10=Bb 11=B\n",
        "\n",
        "keyOfC = []\n",
        "keyOfDb = []\n",
        "keyOfD = []\n",
        "keyOfEb = []\n",
        "keyOfE = []\n",
        "keyOfF = []\n",
        "keyOfGb = []\n",
        "keyOfG = []\n",
        "keyOfAb = []\n",
        "keyOfA = []\n",
        "keyOfBb = []\n",
        "keyOfB = []\n",
        "\n",
        "#https://www.kaggle.com/code/alankarmahajan/exploring-spotify-dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "A1OcqlDZ9uCm",
      "metadata": {
        "id": "A1OcqlDZ9uCm"
      },
      "source": [
        "Loop over each row, and mark the correct *key* value with `1.0` :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pkYgAEhH-kxl",
      "metadata": {
        "id": "pkYgAEhH-kxl"
      },
      "outputs": [],
      "source": [
        "n = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XMUFOrkr9LbT",
      "metadata": {
        "id": "XMUFOrkr9LbT"
      },
      "outputs": [],
      "source": [
        "for key in categorical_df['key']:\n",
        "\n",
        "  # Start every column as 0\n",
        "  keyOfC.append(0)\n",
        "  keyOfDb.append(0)\n",
        "  keyOfD.append(0)\n",
        "  keyOfEb.append(0)\n",
        "  keyOfE.append(0)\n",
        "  keyOfF.append(0)\n",
        "  keyOfGb.append(0)\n",
        "  keyOfG.append(0)\n",
        "  keyOfAb.append(0)\n",
        "  keyOfA.append(0)\n",
        "  keyOfBb.append(0)\n",
        "  keyOfB.append(0)\n",
        "\n",
        "  # Switch the correct key column to 1 if the key is present for that row\n",
        "  if key == 0:\n",
        "    keyOfC[n] = 1.0\n",
        "  elif key == 1:\n",
        "    keyOfDb[n] = 1.0\n",
        "  elif key == 2:\n",
        "    keyOfD[n] = 1.0\n",
        "  elif key == 3:\n",
        "    keyOfEb[n] = 1.0\n",
        "  elif key == 4:\n",
        "    keyOfE[n] = 1.0\n",
        "  elif key == 5:\n",
        "    keyOfF[n] = 1.0\n",
        "  elif key == 6:\n",
        "    keyOfGb[n] = 1.0\n",
        "  elif key == 7:\n",
        "    keyOfG[n] = 1.0\n",
        "  elif key == 8:\n",
        "    keyOfAb[n] = 1.0\n",
        "  elif key == 9:\n",
        "    keyOfA[n] = 1.0\n",
        "  elif key == 10:\n",
        "    keyOfBb[n] = 1.0\n",
        "  elif key == 11:\n",
        "    keyOfB[n] = 1.0\n",
        "  else:\n",
        "    print(\"ERROR: Reason='erroneous value'\")\n",
        "    break\n",
        "\n",
        "  # Accumulator increases\n",
        "  n += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vhFqqxZq9jMV",
      "metadata": {
        "id": "vhFqqxZq9jMV"
      },
      "source": [
        "Adding the finished arrays to the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UrRQzGEJ9LfS",
      "metadata": {
        "id": "UrRQzGEJ9LfS"
      },
      "outputs": [],
      "source": [
        "train_df['keyOfC'] = keyOfC\n",
        "train_df['keyOfDb'] = keyOfDb\n",
        "train_df['keyOfD'] = keyOfD\n",
        "train_df['keyOfEb'] = keyOfEb\n",
        "train_df['keyOfE'] = keyOfE\n",
        "train_df['keyOfF'] = keyOfF\n",
        "train_df['keyOfGb'] = keyOfGb\n",
        "train_df['keyOfG'] = keyOfG\n",
        "train_df['keyOfAb'] = keyOfAb\n",
        "train_df['keyOfA'] = keyOfA\n",
        "train_df['keyOfBb'] = keyOfBb\n",
        "train_df['keyOfB'] = keyOfB"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Gcc0jWAdxFU5",
      "metadata": {
        "id": "Gcc0jWAdxFU5"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JS0GbLFo1BPg",
      "metadata": {
        "id": "JS0GbLFo1BPg"
      },
      "source": [
        "**For *'mode'* :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kyzzxOxN9-VL",
      "metadata": {
        "id": "kyzzxOxN9-VL"
      },
      "outputs": [],
      "source": [
        "#print(categorical_df[\"mode\"].unique())\n",
        "# 0=Minor 1=Major\n",
        "\n",
        "Minor = []\n",
        "Major = []\n",
        "\n",
        "#https://www.kaggle.com/code/alankarmahajan/exploring-spotify-dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gJ2RgFL5-nOQ",
      "metadata": {
        "id": "gJ2RgFL5-nOQ"
      },
      "source": [
        "*Loop* over each row, and mark the correct *mode* value with `1.0` :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jbd1ABRt9-ZH",
      "metadata": {
        "id": "jbd1ABRt9-ZH"
      },
      "outputs": [],
      "source": [
        "n = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mnbLBCIM9-c7",
      "metadata": {
        "id": "mnbLBCIM9-c7"
      },
      "outputs": [],
      "source": [
        "#we have our options for mode - 0(minor) or major(1) - so we split mode into the following individual arrays\n",
        "#that will contain a 1(true) or a 0(false) in them to note if that row is the correct mode or not.\n",
        "\n",
        "for x in categorical_df['mode']:\n",
        "\n",
        "  if x == 0:\n",
        "    Minor.append(1.0)             #if the mode is Minor, then it will be set to a 1 for this row.\n",
        "    Major.append(0)\n",
        "\n",
        "  elif x == 1:\n",
        "    Minor.append(0)\n",
        "    Major.append(1.0)             #if the mode is Major, then it will be set to a 1 for this row.\n",
        "\n",
        "  else:\n",
        "    print(\"ERROR: Reason='erroneous value'\")\n",
        "    break\n",
        "\n",
        "    #accumulator increases\n",
        "    n += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ExjwTjvp_H7v",
      "metadata": {
        "id": "ExjwTjvp_H7v"
      },
      "source": [
        "Adding the finished arrays to the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KAO006bJ9-rc",
      "metadata": {
        "id": "KAO006bJ9-rc"
      },
      "outputs": [],
      "source": [
        "train_df['minor'] = Minor\n",
        "train_df['major'] = Major"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "h4pXl9YWxG3j",
      "metadata": {
        "id": "h4pXl9YWxG3j"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jBdl7jMg1q5R",
      "metadata": {
        "id": "jBdl7jMg1q5R"
      },
      "source": [
        "**For *'time_signature'* :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DoM-kQLKA57D",
      "metadata": {
        "id": "DoM-kQLKA57D"
      },
      "outputs": [],
      "source": [
        "#print(categorical_df[\"time_signature\"].unique())\n",
        "# 3 = 3/4,    4 = 4/4,    5 = 5/4\n",
        "\n",
        "_34time = []\n",
        "_44time = []\n",
        "_54time = []\n",
        "\n",
        "#https://community.spotify.com/t5/Spotify-for-Developers/Section-time-signature/td-p/5180561"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kPkcmDv1F8S8",
      "metadata": {
        "id": "kPkcmDv1F8S8"
      },
      "source": [
        "Loop over each row, and mark the correct *time_signature* value with `1.0` :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xOOfjAekGCOD",
      "metadata": {
        "id": "xOOfjAekGCOD"
      },
      "outputs": [],
      "source": [
        "n = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WtkoS0Tb1rpK",
      "metadata": {
        "id": "WtkoS0Tb1rpK"
      },
      "outputs": [],
      "source": [
        "#this for loop splits each letter into one column.\n",
        "for x in categorical_df['time_signature']:\n",
        "\n",
        "  if x == 3:\n",
        "    _34time.append(1.0)     #if the time signature is 3/4 it will be set to a 1 for this row.\n",
        "    _44time.append(0)\n",
        "    _54time.append(0)\n",
        "\n",
        "  elif x == 4:\n",
        "    _34time.append(0)\n",
        "    _44time.append(1.0)     #if the time signature is 4/4, it will be set to a 1 for this row.\n",
        "    _54time.append(0)\n",
        "\n",
        "  elif x == 5:\n",
        "    _34time.append(0)\n",
        "    _44time.append(0)\n",
        "    _54time.append(1.0)     #if the time signature is 5/4, it will be set to a 1 for this row.\n",
        "\n",
        "  else:\n",
        "    print(\"ERROR: Reason='erroneous value'\")\n",
        "    break\n",
        "\n",
        "  #accumulator increases\n",
        "  n += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6JiNOPLvGGBb",
      "metadata": {
        "id": "6JiNOPLvGGBb"
      },
      "outputs": [],
      "source": [
        "#adding the finished arrays to the dataset\n",
        "train_df['time_signature_34'] = _34time\n",
        "train_df['time_signature_44'] = _44time\n",
        "train_df['time_signature_54'] = _54time"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "medQ6fKRxIqd",
      "metadata": {
        "id": "medQ6fKRxIqd"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wZ_C-rbF4CG-",
      "metadata": {
        "id": "wZ_C-rbF4CG-"
      },
      "source": [
        "**For *'genre'* and *'year'* :**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3eMPJyAeJdvd",
      "metadata": {
        "id": "3eMPJyAeJdvd"
      },
      "source": [
        "For the `genre` and `year` columns, the existing category names are already clear and interpretable, so there's no need to manually encode them. Instead, we use scikit-learn's built-in `OneHotEncoder`, which efficiently handles the rest of the one-hot transforming."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3Gg2mbNv7lfp",
      "metadata": {
        "id": "3Gg2mbNv7lfp"
      },
      "outputs": [],
      "source": [
        "genre_year = ['genre','year']\n",
        "encoder = OneHotEncoder(sparse_output=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44XU6CBSulZJ",
      "metadata": {
        "id": "44XU6CBSulZJ"
      },
      "outputs": [],
      "source": [
        "one_hot_genre_year = pd.DataFrame(\n",
        "    (encoder.fit_transform(categorical_df[genre_year])),\n",
        "    columns = encoder.get_feature_names_out(genre_year))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kLQ7zSNIulnV",
      "metadata": {
        "id": "kLQ7zSNIulnV"
      },
      "outputs": [],
      "source": [
        "train_df = pd.concat([train_df, one_hot_genre_year],axis=1)\n",
        "one_hot_genre_year['popularity'] = df['popularity']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GKcmfnlj4MDd",
      "metadata": {
        "id": "GKcmfnlj4MDd"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FMDeREgbTxUS",
      "metadata": {
        "id": "FMDeREgbTxUS"
      },
      "source": [
        "#####**Normalisation**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94Z3sb2NS3so",
      "metadata": {
        "id": "94Z3sb2NS3so"
      },
      "source": [
        "In this subsection, we scale a subset of continuous features so that they fall within the 0-1 range. This is an essential step before modelling, particularly for algorithms like neural networks, which are sensitive to the scale of their input features. Without normalisation, features with larger numeric ranges (like song count or loudness) could disproportionately affect the model and reduce training stability.\n",
        "\n",
        "The following variables are selected for normalisation based on their observed min-max values or known practical bounds. Features are clipped already from *1.3.4 Data Filtering*, giving us bounds to normalise within to ensure that the scaled values remain meaningful.\n",
        "\n",
        "This ensures that all features contribute proportionally to model training and that the optimisation process remains stable and efficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "__SM_vPWUsi4",
      "metadata": {
        "id": "__SM_vPWUsi4"
      },
      "outputs": [],
      "source": [
        "#                            min           max\n",
        "# acousticness               0.000000      0.996000\n",
        "# danceability               0.000000      0.993000\n",
        "# energy                     0.000000      1.000000\n",
        "# instrumentalness           0.000000      1.000000\n",
        "# liveness                   0.000000      0.700000\n",
        "# loudness---------------- -47.283000      0.000000  NORMALISE\n",
        "# speechiness                0.000000      0.967000\n",
        "# tempo-------------------  50.000000    220.000000  NORMALISE\n",
        "# valence                    0.000000      1.000000\n",
        "# artist_avg_popularity---   0.000000     85.000000  NORMALISE\n",
        "# artist_song_count-------   1.000000   1786.000000  NORMALISE\n",
        "# genre_avg_popularity----   0.001277     55.772119  NORMALISE\n",
        "# genre_song_count-------- 525.000000  20964.000000  NORMALISE\n",
        "# year_avg_popularity-----  11.955374     33.103448  NORMALISE\n",
        "# year_song_count------- 25736.000000  37295.000000  NORMALISE\n",
        "# duration_mins-----------   0.034550     20.000000  NORMALISE\n",
        "# popularity--------------   0.000000    100.000000  NORMALISE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pT9nHnRHM9jy",
      "metadata": {
        "id": "pT9nHnRHM9jy"
      },
      "source": [
        "The following features require normalisation because they do not naturally fall within a 0-1 range. Each is scaled to [0, 1] based on its minimum and maximum observed values, or within known bounds. This ensures all features contribute proportionally during model training, avoiding dominance from variables with large raw scales.\n",
        "\n",
        "- **Loudness**: min = -47.28, max = 0  \n",
        "  Measured in decibels. Normalised after clipping from [−60, 0] to [0, 1] to reflect realistic audio volume bounds.\n",
        "\n",
        "- **Tempo**: min = 50, max = 220  \n",
        "  Measured in beats per minute. Normalised after clipping from [50, 220] to [0, 1] to cover the practical tempo range used in most recorded music.\n",
        "\n",
        "- **Duration (minutes)**: min = 0.03, max = 20  \n",
        "  Converted from milliseconds. Normalised after clipping from [0, 20] to [0, 1] to exclude extreme outlier durations (e.g. live sets or experimental pieces).\n",
        "\n",
        "- **Artist Average Popularity**: min = 0, max = 85  \n",
        "  Represents the average popularity score across each artist's discography. Normalised from [0, 100] to [0, 1] to keep in proportion with other model features.\n",
        "\n",
        "- **Artist Song Count**: min = 1, max = 1786  \n",
        "  Indicates the number of songs attributed to an artist. Normalised after clipping from [0, 3000] to [0, 1] to prevent prolific artists from skewing the distribution.\n",
        "\n",
        "- **Genre Average Popularity**: min = 0.001, max = 55.77  \n",
        "  Reflects the average popularity of all songs within a genre. Normalised after clipping from [0, 100] to [0, 1] to balance genre-level influence.\n",
        "\n",
        "- **Genre Song Count**: min = 525, max = 20,964  \n",
        "  Number of songs attributed to a genre. Normalised after clipping from [0, 30,000] to [0, 1] to avoid dominance by the most common genres.\n",
        "\n",
        "- **Year Average Popularity**: min = 11.96, max = 33.10  \n",
        "  Represents the average popularity of songs released in a given year. Normalised after clipping from [0, 100] to [0, 1] to align with other inputs.\n",
        "\n",
        "- **Year Song Count**: min = 25,736, max = 37,295  \n",
        "  The number of songs released per year. Normalised after clipping from [0, 50,000] to [0, 1] to mitigate overrepresentation from more recent years.\n",
        "\n",
        "- **Popularity**: min = 0, max = 100  \n",
        "  The target variable used for prediction. Normalised from [0, 100] to [0, 1] for consistency with the rest of the feature set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TGXuCc_rUVXo",
      "metadata": {
        "id": "TGXuCc_rUVXo"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Rjqdu1MbUQu6",
      "metadata": {
        "id": "Rjqdu1MbUQu6"
      },
      "source": [
        "**Manual Scaling** unnormalised features to [0,1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "A9xmf6s2UQ0h",
      "metadata": {
        "id": "A9xmf6s2UQ0h"
      },
      "outputs": [],
      "source": [
        "# Manual scaling for selected features\n",
        "\n",
        "# --- Loudness: [-60, 0] scale to [0, 1]\n",
        "train_df['loudness'] = (train_df['loudness'] + 60) / 60\n",
        "\n",
        "# --- Tempo: [60, 220] scale to [0,1]\n",
        "train_df['tempo'] = (train_df['tempo'] - 50) / (220 - 50)\n",
        "\n",
        "# --- Duration: [0, 20] scale to [0,1]\n",
        "train_df['duration_mins'] = (train_df['duration_mins']) / 20\n",
        "\n",
        "# --- Popularity: scale from [0, 100] to [0, 1]\n",
        "train_df['popularity'] = train_df['popularity'] / 100\n",
        "\n",
        "# --- Artist Average Popularity: scale from [0, 100] to [0, 1]\n",
        "train_df['artist_avg_popularity'] = train_df['artist_avg_popularity'] / 100\n",
        "\n",
        "# --- Genre Average Popularity: scale from [0, 100] to [0, 1]\n",
        "train_df['genre_avg_popularity'] = train_df['genre_avg_popularity'] / 100\n",
        "\n",
        "# --- Year Average Popularity: scale from [0, 100] to [0, 1]\n",
        "train_df['year_avg_popularity'] = train_df['year_avg_popularity'] / 100\n",
        "\n",
        "# --- Artist Song Count: [0, 3000] scale to [0, 1]\n",
        "train_df['artist_song_count'] = (train_df['artist_song_count']) / 3000\n",
        "\n",
        "# --- Genre Song Count: [0, 30000] scale to [0, 1]\n",
        "train_df['genre_song_count'] = (train_df['genre_song_count']) / 30000\n",
        "\n",
        "# --- Year Song Count: [0, 50000] scale to [0, 1]\n",
        "train_df['year_song_count'] = (train_df['year_song_count']) / 50000\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n"
      ],
      "metadata": {
        "id": "SxFKyvXg2zmm"
      },
      "id": "SxFKyvXg2zmm"
    },
    {
      "cell_type": "code",
      "source": [
        "display(train_df)"
      ],
      "metadata": {
        "id": "epXYay9W25Q3"
      },
      "id": "epXYay9W25Q3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "wibMdr_6ORXf",
      "metadata": {
        "id": "wibMdr_6ORXf"
      },
      "source": [
        "**[END]**\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7Gf5F_zwE0YU",
      "metadata": {
        "id": "7Gf5F_zwE0YU"
      },
      "source": [
        "****"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qc7FCYu5-fdk",
      "metadata": {
        "id": "qc7FCYu5-fdk"
      },
      "source": [
        "####**C. Introducing time emulated *temporal_df***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5YiiCMhp-2ih",
      "metadata": {
        "id": "5YiiCMhp-2ih"
      },
      "source": [
        "This part of the pipeline prepares `temporal_df`, a time-aware version of the dataset used for exploring trends and building features that respect chronological order.\n",
        "\n",
        "\n",
        "\n",
        "A `year_decimal` column is added to simulate realistic release distribution across the calendar year, allowing smoother time-based analysis. We have to do it this way because the dataset being used doesn't include a release date feature - the finest temporal data included is by year - so we essentially just impute our release dates. A custom `calculate_running_average()` function is then used to compute expanding averages and song counts globally, and within artist, genre, and year groups. These values exclude the current song using a `.shift(1)` to ensure that the data is calculated from immediately before the release, thus fully respecting time.\n",
        "\n",
        "The resulting dataset supports temporally safe exploration and helps avoid leakage in downstream analysis. It's used mainly in Sections ***2*** and ***4*** for visualisations and interpretive correlation work.\n",
        "\n",
        "- **`temporal_df`** -> *Temporal Metadata Pipeline*  \n",
        "  Contains `year_decimal`, plus historical averages and counts for artist, genre, and year.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "s-uc4XsmOQD7",
      "metadata": {
        "id": "s-uc4XsmOQD7"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ro_oBF6z-_cE",
      "metadata": {
        "id": "ro_oBF6z-_cE"
      },
      "outputs": [],
      "source": [
        "temporal_df = df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iyWjVf4AiTXz",
      "metadata": {
        "id": "iyWjVf4AiTXz"
      },
      "outputs": [],
      "source": [
        "temporal_df['year'] = categorical_df['year']\n",
        "temporal_df['artist_name'] = descriptors_df['artist_name']\n",
        "temporal_df['genre'] = categorical_df['genre']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ygu68h0dZ2rK",
      "metadata": {
        "id": "ygu68h0dZ2rK"
      },
      "outputs": [],
      "source": [
        "#\"\"\"\n",
        "# Popularity against Year\n",
        "ax1 = temporal_df.plot.scatter('year', 'popularity', c='year', cmap='viridis')\n",
        "ax1.set_title('Popularity vs Year')\n",
        "ax1.set_xlabel('Year')\n",
        "ax1.set_ylabel('Popularity')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "#\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hquZY0Ei7_oW",
      "metadata": {
        "id": "hquZY0Ei7_oW"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        "."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EtAC3wBL2DA9",
      "metadata": {
        "id": "EtAC3wBL2DA9"
      },
      "source": [
        "#####**Calculating Time Emulated Popularity (Year-Based Running Averages):**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YiXGKQC7fVG6",
      "metadata": {
        "id": "YiXGKQC7fVG6"
      },
      "source": [
        "To simulate a realistic time-based context for each song, a fractional component is added to the release year (`year_decimal`) to emulate intra-year variability. This enabled the calculation of temporally-aware running averages and song counts using a custom `calculate_running_average()` function. These statistics are computed globally, and within groups (artist, genre, year), using `.expanding().mean().shift(1)` to ensure each value only reflects prior data — preventing temporal leakage.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcV0g1Vr2Oss",
      "metadata": {
        "id": "fcV0g1Vr2Oss"
      },
      "source": [
        "**Adding a realistic decimal for intra-year time spreading:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BG1ogt5_2Dkb",
      "metadata": {
        "id": "BG1ogt5_2Dkb"
      },
      "outputs": [],
      "source": [
        "year_decimal = []\n",
        "for year in temporal_df['year']:\n",
        "    randomDecimal = round((1/365) * random.randint(0, 364), 3)\n",
        "    year_decimal.append(year + randomDecimal)\n",
        "\n",
        "temporal_df['year_decimal'] = year_decimal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "z3-njzb72DnL",
      "metadata": {
        "id": "z3-njzb72DnL"
      },
      "outputs": [],
      "source": [
        "display(temporal_df[['artist_name', 'popularity', 'year', 'year_decimal']])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uvqnXS-P9Bjw",
      "metadata": {
        "id": "uvqnXS-P9Bjw"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ChjiHDakoX35",
      "metadata": {
        "id": "ChjiHDakoX35"
      },
      "source": [
        "**Defining Function *calculate_running_average()*** for calculating the running-average and running total (by group, or by whole dataset):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aCPD-sWATah8",
      "metadata": {
        "id": "aCPD-sWATah8"
      },
      "outputs": [],
      "source": [
        "def calculate_running_average(df, group_name=None, display_name=\"running\"):\n",
        "\n",
        "  # (IF GROUP_NAME SELECTED): Sort by group and time\n",
        "  if group_name != None:\n",
        "    df = df.sort_values(by=[group_name, 'year_decimal', 'running_song_count']).copy()\n",
        "\n",
        "    # Compute expanding average and count\n",
        "    grouped = df.groupby(group_name)['popularity']\n",
        "\n",
        "    avg_popularity = grouped.expanding().mean().shift(1).reset_index(level=0, drop=True)\n",
        "    song_count = grouped.expanding().count().shift(1).reset_index(level=0, drop=True)\n",
        "\n",
        "    # Assign to DataFrame\n",
        "    df[f'{display_name}_avg_popularity'] = avg_popularity\n",
        "    df[f'{display_name}_song_count'] = song_count\n",
        "\n",
        "    # Explicitly force 0 for first song of each artist\n",
        "    first_song_index = df.groupby(group_name).head(1).index\n",
        "\n",
        "    for index in first_song_index:\n",
        "        df.loc[index, f'{display_name}_avg_popularity'] = 0\n",
        "        df.loc[index, f'{display_name}_song_count'] = 0\n",
        "    return df\n",
        "\n",
        "\n",
        "  # (NO GROUP_NAME SELECTED): In this instance, we just compute the running values on the whole dataset\n",
        "  else:\n",
        "    df = df.sort_values(by='year_decimal').copy()\n",
        "\n",
        "    # Calculate expanding average and count across the full dataset\n",
        "    popularity = df['popularity']\n",
        "\n",
        "    avg_popularity = popularity.expanding().mean().shift(1)\n",
        "    song_count = popularity.expanding().count().shift(1)\n",
        "\n",
        "    # Assign to DataFrame\n",
        "    df[f'{display_name}_avg_popularity'] = avg_popularity\n",
        "    df[f'{display_name}_song_count'] = song_count\n",
        "\n",
        "    # First row (index 0 after sort) gets forced 0\n",
        "    first_index = df.index[0]\n",
        "    df.loc[first_index, f'{display_name}_avg_popularity'] = 0\n",
        "    df.loc[first_index, f'{display_name}_song_count'] = 0\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fGTEWonCYoKR",
      "metadata": {
        "id": "fGTEWonCYoKR"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aCCnYK-EpDIb",
      "metadata": {
        "id": "aCCnYK-EpDIb"
      },
      "source": [
        "#####**Adding Time-Respective Running Metadata to *temporal_df***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UE71hR9tQGfd",
      "metadata": {
        "id": "UE71hR9tQGfd"
      },
      "source": [
        "To enrich `temporal_df` with realistic historical context and minimise temporal data leakage, group-based running averages and song counts are calculated across key metadata dimensions: artist, genre, and year. Each uses an expanding mean excluding the current row, ensuring temporal causality. These variables simulate what would have been historically knowable at each point in time, and are used for interpretive analysis and correlation visualisation in the project."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mdgsCMmgQXJQ",
      "metadata": {
        "id": "mdgsCMmgQXJQ"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating Global Running Metadata:"
      ],
      "metadata": {
        "id": "ATHs-HreS5sX"
      },
      "id": "ATHs-HreS5sX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qRK-q7pzeTbu",
      "metadata": {
        "id": "qRK-q7pzeTbu"
      },
      "outputs": [],
      "source": [
        "temporal_df = calculate_running_average(temporal_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zhYPL-Y7eUGE",
      "metadata": {
        "id": "zhYPL-Y7eUGE"
      },
      "outputs": [],
      "source": [
        "display(temporal_df[['year', 'artist_name', 'year_decimal', 'popularity', 'running_avg_popularity', 'running_song_count']].sort_index().sort_values(by=['year_decimal', 'running_song_count']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bSrxUfNBeTlN",
      "metadata": {
        "id": "bSrxUfNBeTlN"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating Running Artist Metadata:"
      ],
      "metadata": {
        "id": "WAhnVIh_TEAM"
      },
      "id": "WAhnVIh_TEAM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "I4Epc86Vi7ni",
      "metadata": {
        "id": "I4Epc86Vi7ni"
      },
      "outputs": [],
      "source": [
        "temporal_df = calculate_running_average(temporal_df, group_name='artist_name', display_name='artist')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "A8yPTGcbhmlI",
      "metadata": {
        "id": "A8yPTGcbhmlI"
      },
      "outputs": [],
      "source": [
        "#specific artist's progression\n",
        "new_df = temporal_df[temporal_df['artist_name'] == 'Taylor Swift']\n",
        "display(new_df[['artist_name', 'genre', 'year_decimal', 'popularity', 'artist_avg_popularity', 'artist_song_count', 'running_song_count']].sort_index().sort_values(by='artist_song_count'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cJhU-UwkfGs",
      "metadata": {
        "id": "9cJhU-UwkfGs"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating Running Genre Metadata:"
      ],
      "metadata": {
        "id": "qMKEmKKzTSYi"
      },
      "id": "qMKEmKKzTSYi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KiKkcJGoj9Bu",
      "metadata": {
        "id": "KiKkcJGoj9Bu"
      },
      "outputs": [],
      "source": [
        "temporal_df = calculate_running_average(temporal_df, group_name='genre', display_name='genre')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y-d9YIu5j9nP",
      "metadata": {
        "id": "y-d9YIu5j9nP"
      },
      "outputs": [],
      "source": [
        "#specific genre's progression\n",
        "display(temporal_df[temporal_df['genre'] == 'rock'][['genre', 'artist_name', 'year_decimal', 'popularity', 'genre_avg_popularity', 'genre_song_count']].sort_index().sort_values(by='genre_song_count'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iMywh2aJkjng",
      "metadata": {
        "id": "iMywh2aJkjng"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating Running Year Metadata:"
      ],
      "metadata": {
        "id": "k7uFr3AsTeBE"
      },
      "id": "k7uFr3AsTeBE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SE2UAz6LkkcP",
      "metadata": {
        "id": "SE2UAz6LkkcP"
      },
      "outputs": [],
      "source": [
        "temporal_df = calculate_running_average(temporal_df, group_name='year', display_name='year')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kLZEtSzXkmy7",
      "metadata": {
        "id": "kLZEtSzXkmy7"
      },
      "outputs": [],
      "source": [
        "#a specific years' progression\n",
        "new_df = temporal_df[temporal_df['year'] > 2020]\n",
        "display(new_df[['year', 'artist_name', 'year_decimal', 'popularity', 'year_avg_popularity', 'year_song_count']].sort_index().sort_values(by=['year', 'year_song_count']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9_mdiM9TZNjn",
      "metadata": {
        "id": "9_mdiM9TZNjn"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kCH47W7LSjBr",
      "metadata": {
        "id": "kCH47W7LSjBr"
      },
      "source": [
        "`artist_name` and `genre` columns were just for generating the metadata - now we can remove them\n",
        "so that we only have continuous data in temporal_df:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WO5RqcOrTMBf",
      "metadata": {
        "id": "WO5RqcOrTMBf"
      },
      "outputs": [],
      "source": [
        "temporal_df = temporal_df.drop(columns=['artist_name','genre'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WNUgfkfTuAZH",
      "metadata": {
        "id": "WNUgfkfTuAZH"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uqEimShqoDDG",
      "metadata": {
        "id": "uqEimShqoDDG"
      },
      "source": [
        "**Displaying *temporal_df* :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YaVUcovJHMaE",
      "metadata": {
        "id": "YaVUcovJHMaE"
      },
      "outputs": [],
      "source": [
        "display(temporal_df[['popularity', 'year_decimal', 'artist_avg_popularity', 'artist_song_count',\n",
        "                     'genre_avg_popularity', 'genre_song_count', 'year_avg_popularity',\n",
        "                     'year_song_count', 'running_avg_popularity', 'running_song_count']\n",
        "                    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lJjfj2Jk-KZ_",
      "metadata": {
        "id": "lJjfj2Jk-KZ_"
      },
      "outputs": [],
      "source": [
        "#\"\"\"\n",
        "# Popularity against Year_Decimal\n",
        "ax1 = temporal_df.plot.scatter('year_decimal', 'popularity', c='year', cmap='viridis')\n",
        "ax1.set_title('Popularity vs Year_Decimal')\n",
        "ax1.set_xlabel('Year_Decimal')\n",
        "ax1.set_ylabel('Popularity')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "#\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "R4g3ADV3DINb",
      "metadata": {
        "id": "R4g3ADV3DINb"
      },
      "source": [
        "**[END]**\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uDXGe8I22VY_",
      "metadata": {
        "id": "uDXGe8I22VY_"
      },
      "source": [
        "#**Section 2** - Exploratory Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TsSg_rWh3MkV",
      "metadata": {
        "id": "TsSg_rWh3MkV"
      },
      "source": [
        "##**2.1** Distribution and Basic Statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "y3EL-Chn4oT_",
      "metadata": {
        "id": "y3EL-Chn4oT_"
      },
      "source": [
        "In this section, we examine the distributions and statistical properties of the dataset's features to understand their underlying characteristics. This analysis provides insights into the central tendencies, variability, and shape of the data distributions, which are crucial for subsequent modelling, research and interpretation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BQoOvgqm6Yyt",
      "metadata": {
        "id": "BQoOvgqm6Yyt"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V3dT7plB2b7u",
      "metadata": {
        "id": "V3dT7plB2b7u"
      },
      "source": [
        "###***2.1.1*** - Histograms of All Features (Distributed by Genre)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CNwKhvkA1ej-",
      "metadata": {
        "id": "CNwKhvkA1ej-"
      },
      "source": [
        "These histograms provide a first look at the distribution of each numerical feature in the dataset. Most variables (e.g. `danceability`, `energy`, `valence`) are already bounded between 0 and 1 and display reasonable spread, although some lean towards skewed distributions (e.g. `acousticness`, `instrumentalness`, `liveness`).\n",
        "\n",
        "Others, like `loudness`, `tempo`, and various *_count* or *_popularity* fields, span wider numerical ranges. These distributions justify the need for normalisation, especially in features where outliers or long tails are present.\n",
        "\n",
        "From a modelling perspective, understanding the spread helps anticipate which features may require transformation or carry more weight during prediction. It also highlights where features might dominate due to scale, or where certain values cluster unnaturally (e.g. high concentration at zero for `instrumentalness`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Hll5N_o01fOJ",
      "metadata": {
        "id": "Hll5N_o01fOJ"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SBAGup442zEF",
      "metadata": {
        "id": "SBAGup442zEF"
      },
      "source": [
        "This code iterates through all numerical columns in the dataset, plotting their distributions to identify any skewness, outliers, or unusual patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7BLrR4V3G0Va",
      "metadata": {
        "id": "7BLrR4V3G0Va"
      },
      "outputs": [],
      "source": [
        "# Count how many times each genre appears\n",
        "data = df.copy()\n",
        "f64_col = data.columns.tolist()\n",
        "data['genre'] = categorical_df['genre']\n",
        "tmp = data['genre'].value_counts()\n",
        "tmp = pd.DataFrame(tmp)\n",
        "tmp.columns = ['count']\n",
        "\n",
        "# Add percentage column\n",
        "tmp['frequency'] = tmp['count'] / tmp['count'].sum()\n",
        "tmp['frequency'] = tmp['frequency'].apply(lambda x: f'{x*100:.2f}%')\n",
        "\n",
        "# Calculate average popularity per genre\n",
        "genre_popularity = data.groupby('genre')['popularity'].mean()\n",
        "genre_popularity = pd.DataFrame(genre_popularity)\n",
        "\n",
        "# Merge into tmp\n",
        "tmp = tmp.merge(genre_popularity, left_index=True, right_index=True)\n",
        "\n",
        "# Get top 15 genre names\n",
        "all_genre = list(tmp.sort_values(by='popularity', ascending=False).index)\n",
        "top15_genre = list(tmp.sort_values(by='popularity', ascending=False).head(15).index)\n",
        "\n",
        "#f64_col = data.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "\n",
        "# This code at it's core is not mine - the source can be found below.\n",
        "#https://www.kaggle.com/code/hmdmin357/spotify-classification-analysis/notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9lR0LJMSG0bi",
      "metadata": {
        "id": "9lR0LJMSG0bi"
      },
      "outputs": [],
      "source": [
        "def make_hist(pal, genre):\n",
        "    data15=data[data['genre'].isin(genre)]\n",
        "    c_order={x:i+1 for i,x in enumerate(genre)}\n",
        "    palette=sns.color_palette('Set3',n_colors=len(genre))\n",
        "    palette2=sns.color_palette('hsv',n_colors=len(genre))[::-1]\n",
        "    palette3=[]\n",
        "    for i in range(len(genre)):\n",
        "        palette3.append((palette[i][0]*0.8+palette2[i][0]*0.2,palette[i][1]*0.8+palette2[i][1]*0.2,palette[i][2]*0.8+palette2[i][2]*0.2))\n",
        "    data15=data15.assign(c_order=data15['genre'].map(c_order)).sort_values('c_order').drop('c_order',axis=1)\n",
        "    plt.figure(figsize=(40,30))\n",
        "    sns.histplot(data=data15,x=pal,hue='genre',multiple='stack',binwidth=0.02*(data15[pal].max()-data15[pal].min()),palette=palette3)\n",
        "    plt.tight_layout()\n",
        "    plt.title(pal.title() +' Histogram in the Top-15 Genres')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# This code is not mine - the source can be found below.\n",
        "#https://www.kaggle.com/code/hmdmin357/spotify-classification-analysis/notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VHdNKN6h0bmw",
      "metadata": {
        "id": "VHdNKN6h0bmw"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TL1t45nl0YFT",
      "metadata": {
        "id": "TL1t45nl0YFT"
      },
      "source": [
        "**Feature Distribution by Genre:**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2BAlCAJduVXi",
      "metadata": {
        "id": "2BAlCAJduVXi"
      },
      "source": [
        "Below we generate histograms for all continuous audio features, segmented by the top 15 most popular genres. This allows us to visually compare how these features are distributed across genre types and observe stylised distinctions between categories.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63XyMDscHNkg",
      "metadata": {
        "id": "63XyMDscHNkg"
      },
      "outputs": [],
      "source": [
        "for pal in f64_col:\n",
        "    if pal!='genre':\n",
        "        make_hist(pal,all_genre)\n",
        "\n",
        "# This code is not mine - the source can be found below.\n",
        "#https://www.kaggle.com/code/hmdmin357/spotify-classification-analysis/notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n"
      ],
      "metadata": {
        "id": "0ETp1HapWha6"
      },
      "id": "0ETp1HapWha6"
    },
    {
      "cell_type": "code",
      "source": [
        "for pal in f64_col:\n",
        "    if pal!='genre':\n",
        "        make_hist(pal,top15_genre)\n",
        "\n",
        "# This code is not mine - the source can be found below.\n",
        "#https://www.kaggle.com/code/hmdmin357/spotify-classification-analysis/notebook"
      ],
      "metadata": {
        "id": "dpaD3RmWWjSp"
      },
      "id": "dpaD3RmWWjSp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "N_4MXYy06W7O",
      "metadata": {
        "id": "N_4MXYy06W7O"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mXg76LB826KY",
      "metadata": {
        "id": "mXg76LB826KY"
      },
      "source": [
        "###***2.1.2*** - Summary Statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f-m2z3D29Uy",
      "metadata": {
        "id": "5f-m2z3D29Uy"
      },
      "source": [
        "Following the visual inspection of feature distributions, this summary statistics table provides a more quantitative view of central tendencies and spread. Features like `danceability`, `energy`, and `valence` continue to show compact, bounded ranges, which is expected given their design as normalised values.\n",
        "\n",
        "In contrast, variables such as `artist_song_count`, `year_song_count`, `loudness`, and `duration_mins` span much wider scales. Their interquartile ranges and high maximum values reinforce the earlier observation that scale disparities could distort model behaviour without proper normalisation.\n",
        "\n",
        "We also calculate skewness and kurtosis to quantify distribution shape. Features like `instrumentalness` and `loudness` exhibit significant skew or heavy tails, suggesting non-normal distributions with outlier influence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yHNc7T132_Jv",
      "metadata": {
        "id": "yHNc7T132_Jv"
      },
      "outputs": [],
      "source": [
        "# Display summary statistics for numerical features\n",
        "summary_stats = df.describe().T\n",
        "summary_stats['skewness'] = df.skew()\n",
        "summary_stats['kurtosis'] = df.kurtosis()\n",
        "summary_stats = summary_stats[['mean', 'std', 'min', '25%', '50%', '75%', 'max', 'skewness', 'kurtosis']]\n",
        "summary_stats"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ],
      "metadata": {
        "id": "Ue11SORKlWB0"
      },
      "id": "Ue11SORKlWB0"
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "TFUjUXkzlW-K"
      },
      "id": "TFUjUXkzlW-K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ],
      "metadata": {
        "id": "-3FCNVjUXx6S"
      },
      "id": "-3FCNVjUXx6S"
    },
    {
      "cell_type": "markdown",
      "id": "tRRS3Tvn6VGT",
      "metadata": {
        "id": "tRRS3Tvn6VGT"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dkbIKv7F3C69",
      "metadata": {
        "id": "dkbIKv7F3C69"
      },
      "source": [
        "###***2.1.3*** - Skewness and Kurtosis Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nuz-aMlq1p8d",
      "metadata": {
        "id": "nuz-aMlq1p8d"
      },
      "source": [
        "To extend the statistical view beyond central tendency and spread, skewness and kurtosis were calculated for each continuous feature. Skewness captures asymmetry — whether values lean heavily to one side of the mean — while kurtosis reflects how extreme or concentrated the tails are compared to a normal distribution.\n",
        "\n",
        "Several features show significant skew (>|1|), including `loudness`, `speechiness`, `acousticness`, `liveness`, `artist_song_count`, and `duration_mins`. These are not symmetrically distributed, with many skewed to the right — indicating a large cluster of lower values and a long tail of higher ones. The same features (especially `artist_song_count`) also show high kurtosis (>|3|), suggesting sharp peaks or heavy tails with influential outliers.\n",
        "\n",
        "These patterns reinforce earlier findings from the histograms and summary stats, and support the later decision to clip extreme values and apply normalisation — particularly for features likely to distort model training if left unscaled.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vD0K3GMU3RE_",
      "metadata": {
        "id": "vD0K3GMU3RE_"
      },
      "outputs": [],
      "source": [
        "# Identify features with high skewness or kurtosis\n",
        "high_skew = summary_stats[summary_stats['skewness'].abs() > 1]\n",
        "high_kurtosis = summary_stats[summary_stats['kurtosis'].abs() > 3]\n",
        "\n",
        "print(\"Features with high skewness:\")\n",
        "display(high_skew)\n",
        "\n",
        "print(\"\\n\\nFeatures with high kurtosis:\")\n",
        "display(high_kurtosis)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Q7CGOcvrEppr",
      "metadata": {
        "id": "Q7CGOcvrEppr"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eOdScu0KEreR",
      "metadata": {
        "id": "eOdScu0KEreR"
      },
      "source": [
        "###***2.1.4*** Distribution by Average Popularity"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f6AJrb3wrxT",
      "metadata": {
        "id": "9f6AJrb3wrxT"
      },
      "source": [
        "Here we break down popularity by genre, year, and artist to get a sense of broader trends and patterns in the data. This helps identify whether certain genres or time periods consistently perform better, and whether some artists or genres, etc. dominate the dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OznNSZvJ5Y6l",
      "metadata": {
        "id": "OznNSZvJ5Y6l"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KeboZWe9Exjb",
      "metadata": {
        "id": "KeboZWe9Exjb"
      },
      "source": [
        "**By average genre popularity *genre_avg_popularity* :**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97G-1FU55cVQ",
      "metadata": {
        "id": "97G-1FU55cVQ"
      },
      "source": [
        "This plot shows the average popularity of songs across all genres in the dataset. Genres like pop, hip-hop, and dance dominate in terms of average popularity, which likely reflects broader listening trends and platform bias. Less mainstream or instrumental-heavy genres like trip-hop, dubstep, and black-metal sit at the lower end.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gjlODlciEwur",
      "metadata": {
        "id": "gjlODlciEwur"
      },
      "outputs": [],
      "source": [
        "genre_columns = [col for col in train_df.drop(columns='genre_avg_popularity',axis=1).columns if col.startswith('genre_')]\n",
        "\n",
        "\n",
        "genre_popularity = train_df.groupby(\n",
        "    train_df[genre_columns].idxmax(axis=1).str.replace('genre_', '')\n",
        ")['popularity'].mean().reset_index()\n",
        "\n",
        "# Rename the index column to 'genre'\n",
        "genre_popularity.rename(columns={'index': 'genre'}, inplace=True)\n",
        "\n",
        "# Sort by popularity\n",
        "genre_popularity = genre_popularity.sort_values(by='popularity', ascending=False)\n",
        "\n",
        "# Create the bar plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='genre', y='popularity', data=genre_popularity, hue='genre', palette='viridis')\n",
        "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for readability\n",
        "plt.title('Average Popularity by Genre')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "o__o34CFE02b",
      "metadata": {
        "id": "o__o34CFE02b"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20RauNbwE3ek",
      "metadata": {
        "id": "20RauNbwE3ek"
      },
      "source": [
        "**By average yearly popularity *year_avg_popularity* :**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TltQSvcT5tkX",
      "metadata": {
        "id": "TltQSvcT5tkX"
      },
      "source": [
        "This bar chart shows a clear upward trend in average popularity over time, with more recent years achieving higher values. This reflects both recency bias in Spotify's popularity metric, and potentially the increasing influence of recommendation systems favouring newer releases.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ty617PTkE1ma",
      "metadata": {
        "id": "ty617PTkE1ma"
      },
      "outputs": [],
      "source": [
        "# Get year columns\n",
        "year_columns = [col for col in train_df.drop(\n",
        "    columns=['popularity','year_song_count'], axis=1\n",
        ").columns if col.startswith('year_')]\n",
        "\n",
        "# Create DataFrame for year popularity\n",
        "year_popularity = train_df.groupby(\n",
        "    train_df[year_columns].idxmax(axis=1).str.replace('year_', '')\n",
        ")['popularity'].mean().reset_index()\n",
        "\n",
        "# Rename for clarity\n",
        "year_popularity.columns = ['year', 'popularity']\n",
        "\n",
        "# Convert year to int for sorting\n",
        "year_popularity['year'] = year_popularity['year'].astype(int)\n",
        "\n",
        "# Sort by popularity\n",
        "year_popularity = year_popularity.sort_values(by='popularity', ascending=False)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "ax = sns.barplot(x='year', y='popularity', data=year_popularity, hue='year', palette='viridis', legend=False)\n",
        "plt.xticks(rotation=45, ha='center')\n",
        "\n",
        "\n",
        "\n",
        "plt.title('Average Popularity by Year (2000-2022)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oUB-EwUrFBd8",
      "metadata": {
        "id": "oUB-EwUrFBd8"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Y6dlL2G1FCQi",
      "metadata": {
        "id": "Y6dlL2G1FCQi"
      },
      "source": [
        "**Average Artist Popularity *artist_avg_popularity* by Artist *artist_name* :**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KWOsXPzu502r",
      "metadata": {
        "id": "KWOsXPzu502r"
      },
      "source": [
        "These are the top 30 most popular artists in the dataset based on average popularity across all of their songs. The list is dominated by newer, high-performing artists with viral hits or consistent playlist presence. This result isn't weighted by number of songs - it's purely based on artist-level average popularity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WIvGxBYOFCXV",
      "metadata": {
        "id": "WIvGxBYOFCXV"
      },
      "outputs": [],
      "source": [
        "# Group by artist name and calculate average popularity with a limit of atleast 10 songs\n",
        "\n",
        "# Add artist names into df\n",
        "artist_popularity = df.copy()\n",
        "artist_popularity['artist_name'] = descriptors_df['artist_name']\n",
        "artist_popularity = artist_popularity[artist_popularity['artist_song_count'] > 10]\n",
        "\n",
        "# Drop duplicates so each artist appears once with their true artist_avg_popularity\n",
        "artist_popularity = artist_popularity.drop_duplicates(subset='artist_name')\n",
        "\n",
        "# Sort by artist_avg_popularity\n",
        "artist_popularity = artist_popularity.sort_values(by='artist_avg_popularity', ascending=False)\n",
        "\n",
        "# Get top 30 artists\n",
        "top30_artists = artist_popularity.head(30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mP9t8K7G6u7p",
      "metadata": {
        "id": "mP9t8K7G6u7p"
      },
      "outputs": [],
      "source": [
        "# Create the bar plot for top 30 Artists\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='artist_name', y='artist_avg_popularity', data=top30_artists, hue='artist_name', palette='viridis')\n",
        "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for readability\n",
        "plt.title('Top 30 Artists by Artist Average Popularity (2000-2023)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xkrHLQgA6DXg",
      "metadata": {
        "id": "xkrHLQgA6DXg"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "biX8CVe756HS",
      "metadata": {
        "id": "biX8CVe756HS"
      },
      "source": [
        "This adjusted list includes only the top 30 artists with larger catalogues (> 100 songs), helping to filter out viral or short-term outliers. It highlights artists who have sustained popularity over time, such as Drake, Taylor Swift, Ariana Grande, and Kendrick Lamar.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ArUIerT1FFri",
      "metadata": {
        "id": "ArUIerT1FFri"
      },
      "outputs": [],
      "source": [
        "# Add artist names into df\n",
        "matured_artist_popularity = df.copy()\n",
        "matured_artist_popularity['artist_name'] = descriptors_df['artist_name']\n",
        "matured_artist_popularity = matured_artist_popularity[matured_artist_popularity['artist_song_count'] > 100]\n",
        "\n",
        "# Drop duplicates so each artist appears once with their true artist_avg_popularity\n",
        "matured_artist_popularity = matured_artist_popularity.drop_duplicates(subset='artist_name')\n",
        "\n",
        "# Sort by artist_avg_popularity\n",
        "matured_artist_popularity = matured_artist_popularity.sort_values(by='artist_avg_popularity', ascending=False)\n",
        "\n",
        "# Get top 50 artists\n",
        "top30_matured_artists = matured_artist_popularity.head(30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4GtWgDj6sJU",
      "metadata": {
        "id": "b4GtWgDj6sJU"
      },
      "outputs": [],
      "source": [
        "# Create the bar plot for top 30 Artists (Matured)\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='artist_name', y='artist_avg_popularity', data=top30_matured_artists, hue='artist_name', palette='viridis')\n",
        "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for readability\n",
        "plt.title('Top 30 Artists (Matured) by Artist Average Popularity (2000-2023)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0iazDZPi6bDB",
      "metadata": {
        "id": "0iazDZPi6bDB"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4-oRVnFD6cjh",
      "metadata": {
        "id": "4-oRVnFD6cjh"
      },
      "source": [
        "This is a broad sample of the full artist distribution, ordered by artist average popularity. The long-tail distribution is clearly visible, where a few artists sit at the top, but most have relatively low average popularity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1jI2kE3PxUYc",
      "metadata": {
        "id": "1jI2kE3PxUYc"
      },
      "outputs": [],
      "source": [
        "all_artist_popularity = df.sample(n=1000).copy()\n",
        "all_artist_popularity['artist_name'] = descriptors_df['artist_name'].loc[all_artist_popularity.index]\n",
        "all_artist_popularity = all_artist_popularity.drop_duplicates(subset='artist_name')\n",
        "all_artist_popularity = all_artist_popularity.sort_values(by='artist_avg_popularity')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kkFDwKut6k_j",
      "metadata": {
        "id": "kkFDwKut6k_j"
      },
      "outputs": [],
      "source": [
        "# Create the bar plot for artist distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='artist_name', y='artist_avg_popularity', data=all_artist_popularity, legend=False)\n",
        "plt.title('All Artists (Represented by Sample of ~1000) by Artist Average Popularity (2000-2023)')\n",
        "plt.xticks([], [])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Cfk0S6qMFfMX",
      "metadata": {
        "id": "Cfk0S6qMFfMX"
      },
      "source": [
        "**[END]**\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UBbOvY_g5YsJ",
      "metadata": {
        "id": "UBbOvY_g5YsJ"
      },
      "source": [
        "##**2.2** Correlation and Feature Pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jn2ZjyZw5bTi",
      "metadata": {
        "id": "jn2ZjyZw5bTi"
      },
      "source": [
        "In this section, we examine how individual features relate to each other and, more importantly, to the target variable - `popularity`. Understanding correlation is vital for identifying redundant features, multicollinearity, and variables that may strongly or weakly influence model predictions.\n",
        "\n",
        "This process heavily supports fundamental understanding of the data's inner relationships."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lq6AsjUv5oYI",
      "metadata": {
        "id": "lq6AsjUv5oYI"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0GJ9qfbA5jZm",
      "metadata": {
        "id": "0GJ9qfbA5jZm"
      },
      "source": [
        "###***2.2.1*** - Correlation Matrix (Continuous)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dDZgJwCL5n24",
      "metadata": {
        "id": "dDZgJwCL5n24"
      },
      "source": [
        "This correlation matrix reveals the linear relationships between all continuous features in the dataset. As expected, `artist_avg_popularity` shows the strongest positive correlation with `popularity` (r = 0.84), confirming its role as a dominant predictor. Similarly, `genre_avg_popularity` and `year_avg_popularity` are also positively correlated, supporting the idea that broader trends (artist, genre, time) significantly influence a song's success.\n",
        "\n",
        "Conversely, features like `instrumentalness`, `duration_mins`, and `artist_song_count` have slight to moderate negative correlations with `popularity`, suggesting that long tracks, instrumentals, or overly prolific artists may be less associated with mainstream appeal.\n",
        "\n",
        "These findings guide feature prioritisation for modelling and help identify variables where multicollinearity may be present (e.g. `energy` and `loudness`, r = 0.79).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GTXnTMfP5twO",
      "metadata": {
        "id": "GTXnTMfP5twO"
      },
      "outputs": [],
      "source": [
        "# Create a correlation matrix for continuous features\n",
        "corr_matrix = df.corr(numeric_only=True)\n",
        "\n",
        "# Plot the correlation matrix\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', square=True)\n",
        "plt.title(\"Correlation Matrix (Continuous Features)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fBAxQCrCJdr",
      "metadata": {
        "id": "8fBAxQCrCJdr"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6IE8MkjG9bRK",
      "metadata": {
        "id": "6IE8MkjG9bRK"
      },
      "source": [
        "This bar chart shows the top 5 strongest positive correlations with popularity from the main correlation matrix (`df`). As we know already, `artist_avg_popularity` and `genre_avg_popularity` are the most dominant - these are heavily biased metadata features that reflect external popularity factors. `year_avg_popularity` and `year_song_count` show a temporal influence, while `danceability` is the strongest musical feature here, though its correlation is much weaker overall.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GrgMWyE5HAyA",
      "metadata": {
        "id": "GrgMWyE5HAyA"
      },
      "outputs": [],
      "source": [
        "pop_corr_df = corr_matrix['popularity'].drop('popularity')\n",
        "pop_corr_df = pop_corr_df.sort_values()\n",
        "\n",
        "\n",
        "# Plot top 5 positive\n",
        "plt.figure(figsize=(10, 5))\n",
        "pop_corr_df.tail(5).plot(kind='barh', color='green')\n",
        "plt.title('Top 5 Positive Correlations (Continuous)')\n",
        "plt.xlabel('Correlation')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Plot top 5 negative\n",
        "plt.figure(figsize=(10, 5))\n",
        "pop_corr_df.head(5).plot(kind='barh', color='red')\n",
        "plt.title('Top 5 Negative Correlations (Continuous)')\n",
        "plt.xlabel('Correlation')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ODrm2FV_6FjH",
      "metadata": {
        "id": "ODrm2FV_6FjH"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uvmtP4X7zbL1",
      "metadata": {
        "id": "uvmtP4X7zbL1"
      },
      "source": [
        "###***2.2.2*** - Correlation Matrix (Categorical)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UnKoC51p-Qpd",
      "metadata": {
        "id": "UnKoC51p-Qpd"
      },
      "source": [
        "This extended correlation matrix reveals the linear relationships between continuous features and one-hot encoded categorical indicators such as genre and year. As expected, `artist_avg_popularity` again shows the strongest positive correlation with `popularity` (r = 0.84), followed by `genre_avg_popularity` and `year_avg_popularity`, reinforcing the influence of broader metadata signals on track success.\n",
        "\n",
        "Notably, strong correlations also emerge between `genre_hip-hop`, `genre_dance`, and recent years like `year_2022`, suggesting modern genres are more aligned with popularity. This supports the idea of recency bias and genre-driven platform trends shaping outcomes.\n",
        "\n",
        "Conversely, categorical features such as `genre_black-metal` or earlier year indicators show weaker or negative correlations with `popularity`, implying a lower likelihood of mainstream visibility. These insights help validate the inclusion of one-hot categorical inputs and inform which features may offer the most predictive value without introducing redundancy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f834NeWH1umz",
      "metadata": {
        "id": "f834NeWH1umz"
      },
      "outputs": [],
      "source": [
        "# Create a correlation matrix for continuous and categorical features\n",
        "one_hot_corr_matrix = train_df.corr(numeric_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gZwqP09nRh3m",
      "metadata": {
        "id": "gZwqP09nRh3m"
      },
      "outputs": [],
      "source": [
        "# Get correlations with popularity only\n",
        "pop_corr = one_hot_corr_matrix['popularity'] # remove self-correlation\n",
        "\n",
        "# Take absolute values to find strongest correlations regardless of direction\n",
        "pop_corr = pop_corr.abs()\n",
        "\n",
        "# Sort and get top 30\n",
        "top_features = pop_corr.sort_values(ascending=False).head(30)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Votqt4w1Q3Ny",
      "metadata": {
        "id": "Votqt4w1Q3Ny"
      },
      "outputs": [],
      "source": [
        "selected = one_hot_corr_matrix.loc[top_features.index, top_features.index]\n",
        "\n",
        "# Plot the correlation matrix\n",
        "plt.figure(figsize=(24, 24))\n",
        "sns.heatmap(selected, annot=True, fmt=\".2f\", cmap='coolwarm', square=True)\n",
        "plt.title(\"Sample of Correlation Matrix (inc. Categorical Features)\") # includes a sample of the top 30 relevant features,\n",
        "plt.show()                                                            # as there is too many features to display fully effectively\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "j7p1OocSBdau",
      "metadata": {
        "id": "j7p1OocSBdau"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OAfQpGJS-7Gx",
      "metadata": {
        "id": "OAfQpGJS-7Gx"
      },
      "source": [
        "This plot highlights the top 10 categorical one-hot features most positively correlated with `popularity`. Genre tags like `genre_hip-hop`, `genre_dance`, and `genre_pop` show the strongest relationships, aligning with dominant commercial genres on streaming platforms. Temporal indicators such as `year_2022` and `year_2021` also appear, reinforcing the influence of recency. These correlations help justify the inclusion of selected genre and year indicators as input features during modelling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1O6VDEF5HM-J",
      "metadata": {
        "id": "1O6VDEF5HM-J"
      },
      "outputs": [],
      "source": [
        "filtered_cols = []\n",
        "for col in one_hot_corr_matrix.columns:\n",
        "    if col != 'popularity':\n",
        "        if col not in df.columns:\n",
        "            filtered_cols.append(col)\n",
        "\n",
        "pop_corr_one_hot = one_hot_corr_matrix.loc[filtered_cols, 'popularity']\n",
        "pop_corr_one_hot = pop_corr_one_hot.sort_values()\n",
        "\n",
        "\n",
        "# Plot top 10 positive\n",
        "plt.figure(figsize=(10, 5))\n",
        "pop_corr_one_hot.tail(10).plot(kind='barh', color='green')\n",
        "plt.title('Top 10 Positive Correlations (Categorical)')\n",
        "plt.xlabel('Correlation')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Plot top 10 negative\n",
        "plt.figure(figsize=(10, 5))\n",
        "pop_corr_one_hot.head(10).plot(kind='barh', color='red')\n",
        "plt.title('Top 10 Negative Correlations')\n",
        "plt.xlabel('Correlation')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Mjdf1UsazwP2",
      "metadata": {
        "id": "Mjdf1UsazwP2"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5UG_RYOzxes",
      "metadata": {
        "id": "f5UG_RYOzxes"
      },
      "source": [
        "###***2.2.3*** Correlation Matrix (Temporal)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yoba-9fc_2F_",
      "metadata": {
        "id": "yoba-9fc_2F_"
      },
      "source": [
        "This extended correlation matrix visualises the linear relationships between all continuous features within the temporally-enhanced dataset `temporal_df`. Compared to the standard version derived from `df`, this view incorporates time-respective metadata — including `artist_avg_popularity`, `genre_avg_popularity`, `year_avg_popularity`, and their respective song counts — calculated using running averages at each point in time.\n",
        "\n",
        "As expected, `artist_avg_popularity` shows a strong positive correlation with `popularity` (r = 0.75), though notably lower than in the static correlation matrix (r = 0.85). This suggests that time-awareness and data leakage minimisation has weakened the direct dependency on artist momentum, which will in turn also enhance the independence of feature influence.\n",
        "\n",
        "Similarly, `year_avg_popularity` exhibits a more moderate correlation with `popularity` (r = 0.39) compared to their static counterparts, wheras `genre_avg_popualrity` has transformed to become the most influencial correlation of all — a surprisingly durastic change.\n",
        "\n",
        "Temporal variables like `year_decimal` and `running_song_count` show weaker yet steady correlations with `popularity`, further consolidating the implicit influence of recency and global growth trends in the dataset.\n",
        "\n",
        "Compared to the original matrix, the temporal version reveals a more balanced and temporally faithful view of feature relationships — essentially avoiding leakage in modelling, and for understanding which features genuinely carries explanatory power in a realistic, time-aware context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yYpvxHLypjsY",
      "metadata": {
        "id": "yYpvxHLypjsY"
      },
      "outputs": [],
      "source": [
        "temporal_corr_matrix = temporal_df.corr(numeric_only=True)\n",
        "plt.figure(figsize=(19, 13))\n",
        "sns.heatmap(temporal_corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', square=True)\n",
        "plt.title(\"Correlation Matrix (Temporal Continuous Features)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Kv_VFgMbIfno",
      "metadata": {
        "id": "Kv_VFgMbIfno"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fuOCtPyAX87",
      "metadata": {
        "id": "5fuOCtPyAX87"
      },
      "source": [
        "This bar chart confirms that the strongest temporal correlations with `popularity` align with metadata-driven variables. `artist_avg_popularity` and `genre_avg_popularity` remain the top predictors, while `year_avg_popularity`, `running_avg_popularity`, and `running_song_count` also contribute meaningfully. These results support the potential for inclusion of time-aware engineered features from `temporal_df` in modelling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kzNjWqlWIgpI",
      "metadata": {
        "id": "kzNjWqlWIgpI"
      },
      "outputs": [],
      "source": [
        "pop_corr_temporal = temporal_corr_matrix['popularity'].drop('popularity')\n",
        "pop_corr_temporal = pop_corr_temporal.sort_values()\n",
        "\n",
        "\n",
        "# Plot top 5 positive\n",
        "plt.figure(figsize=(10, 5))\n",
        "pop_corr_temporal.tail(5).plot(kind='barh', color='green')\n",
        "plt.title('Top 5 Positive Correlations (temporal_df - Temporal)')\n",
        "plt.xlabel('Correlation')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Plot top 5 negative\n",
        "plt.figure(figsize=(10, 5))\n",
        "pop_corr_temporal.head(5).plot(kind='barh', color='red')\n",
        "plt.title('Top 5 Negative Correlations (temporal_df - Temporal)')\n",
        "plt.xlabel('Correlation')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0OOV_3IU7qZx",
      "metadata": {
        "id": "0OOV_3IU7qZx"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oPqCTIQ45xIo",
      "metadata": {
        "id": "oPqCTIQ45xIo"
      },
      "source": [
        "###***2.2.4*** - Exploring Correlated Feature Pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bG2pkPAVw5xS",
      "metadata": {
        "id": "bG2pkPAVw5xS"
      },
      "source": [
        "This section presents the key pairwise correlations extracted from the main correlation matrix (`corr_matrix`), categorised by type. Each group has been filtered to remove duplicates, mutually exclusive pairs, and low-signal relationships. The goal is to isolate feature interactions that are meaningful for modelling or interpretation.\n",
        "\n",
        "Pairs are split into the following categories:\n",
        "- Popularity correlations (with `popularity` as one of the features)\n",
        "- Metadata correlations (excluding inter-metadata)\n",
        "- Inter-metadata correlations (e.g. *_avg_popularity*  vs *_song_count* )\n",
        "- Audio feature correlations (between non-metadata continuous features)\n",
        "- Categorical one-hot correlations (e.g. genre/year indicators)\n",
        "- Temporal correlations (derived from `temporal_df`)\n",
        "\n",
        "These structured correlation groups form the foundation for selecting key relationships explored in later subsections and also highlight redundancy, dominance, or dependency across engineered features.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NF6tBSdIw6Cd",
      "metadata": {
        "id": "NF6tBSdIw6Cd"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NGb5P-g9wnuV",
      "metadata": {
        "id": "NGb5P-g9wnuV"
      },
      "source": [
        "####Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZVI4uE2MFpJS",
      "metadata": {
        "id": "ZVI4uE2MFpJS"
      },
      "source": [
        "This block contains all of the helper functions used to filter, clean, and categorise correlation pairs before displaying them. The key idea here is to separate correlation outputs into logical groups (e.g. popularity, metadata, feature-to-feature, etc.) and remove redundant or irrelevant relationships - especially mutually exclusive combinations like genre A <-> genre B.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-YsDMhrTFpqT",
      "metadata": {
        "id": "-YsDMhrTFpqT"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XmkSh_b0T-Mu",
      "metadata": {
        "id": "XmkSh_b0T-Mu"
      },
      "source": [
        "**Defining Function for Categorising and Filtering by Correlation for a given Correlation Matrix**:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tlWunba5wDGL",
      "metadata": {
        "id": "tlWunba5wDGL"
      },
      "source": [
        "code flattens and categorises a given correlation matrix into 4 sections - highly positively, positively, negatively and highly negatively correlated based on given bounds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zpvCkjeXd9jn",
      "metadata": {
        "id": "zpvCkjeXd9jn"
      },
      "outputs": [],
      "source": [
        "def correlate_categorise(corr_matrix,high_bound=0.5,low_bound=0.1):\n",
        "  # Flatten the correlation matrix\n",
        "  correlated_pairs = corr_matrix.unstack()\n",
        "\n",
        "  # Convert to DataFrame and reset index\n",
        "  correlated_pairs = correlated_pairs.reset_index()\n",
        "  correlated_pairs.columns = ['Feature_1', 'Feature_2', 'Correlation']\n",
        "\n",
        "  # Remove self-pairs\n",
        "  correlated_pairs = correlated_pairs[correlated_pairs['Feature_1'] != correlated_pairs['Feature_2']]\n",
        "\n",
        "  # Remove duplicate pairs (A, B) and (B, A)\n",
        "  correlated_pairs['sorted_pair'] = correlated_pairs.apply(lambda row: tuple(sorted([row['Feature_1'], row['Feature_2']])), axis=1)\n",
        "  correlated_pairs = correlated_pairs.drop_duplicates(subset='sorted_pair').drop(columns='sorted_pair')\n",
        "\n",
        "  # Round correlation values\n",
        "  correlated_pairs['Correlation'] = correlated_pairs['Correlation'].round(2)\n",
        "\n",
        "  # Segment into categories\n",
        "  correlated_categorised = correlated_pairs.copy()\n",
        "\n",
        "  correlated_categorised.loc[correlated_categorised['Correlation'] >= high_bound, 'category'] = 'very_positive'\n",
        "  correlated_categorised.loc[(correlated_categorised['Correlation'] >= low_bound) & (correlated_categorised['Correlation'] < high_bound),'category'] = 'positive'\n",
        "  correlated_categorised.loc[(correlated_categorised['Correlation'] <= -low_bound) & (correlated_categorised['Correlation'] > -high_bound),'category'] = 'negative'\n",
        "  correlated_categorised.loc[correlated_categorised['Correlation'] <= -high_bound, 'category'] = 'very_negative'\n",
        "\n",
        "  return correlated_categorised.dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8Qozf22uysqz",
      "metadata": {
        "id": "8Qozf22uysqz"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "izzf3ZDRyt8e",
      "metadata": {
        "id": "izzf3ZDRyt8e"
      },
      "source": [
        "**Functions for Displaying and Filtering:**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XgMPsPcsHm-S",
      "metadata": {
        "id": "XgMPsPcsHm-S"
      },
      "source": [
        "This group of functions handles the filtering, exclusion, and final display of correlation pairs based on logical groupings. These utilities ensure that only meaningful, non-redundant feature pairs are shown, while avoiding pairs that are either mutually exclusive or already explored in a different category.\n",
        "\n",
        "- `remove_mutually_exclusive(df, ...)`: Filters out any correlation pair where both features belong to the same category (e.g. two genres or two year indicators). It relies on `extract()` to determine which category each feature belongs to, and can be configured to match or exclude specific groupings.\n",
        "\n",
        "- `extract(col_name, ...)`: Given a feature name, this helper function classifies the feature based on inclusion/exclusion rules. It's designed to support flexible category tagging for things like `*_avg_popularity`, `*_song_count`, and one-hot columns.\n",
        "\n",
        "- `exclude_pairs_from_table(base_df, exclude_df)`: Removes feature pairs from one correlation table if they already exist in another - useful when ensuring categorical or temporal analyses don't duplicate feature-level pairs.\n",
        "\n",
        "- `display_pairs(df, display_pair_name)`: Organises a correlation DataFrame into four bins (`very_positive`, `positive`, `negative`, `very_negative`) and prints each one cleanly using the given pair label. This ensures clarity when viewing structured results grouped by context (e.g. popularity, metadata, categorical).\n",
        "\n",
        "These functions form the backbone of the structured correlation filtering system used throughout Section 2.2.4, allowing flexible group definitions and precise pair curation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4YjAEi6BwOUb",
      "metadata": {
        "id": "4YjAEi6BwOUb"
      },
      "outputs": [],
      "source": [
        "def remove_mutually_exclusive(df,included_columns=[''],excluded_columns=[''],inter=True):\n",
        "  df = df.copy()\n",
        "\n",
        "  df['category_1'] = df['Feature_1'].apply(lambda col_name: extract(col_name, included_columns, excluded_columns, inter))\n",
        "  df['category_2'] = df['Feature_2'].apply(lambda col_name: extract(col_name, included_columns, excluded_columns, inter))\n",
        "\n",
        "  df = df[df['category_1'] != df['category_2']]  # exclude pairs within the same category\n",
        "\n",
        "  return df.drop(columns=['category_1', 'category_2'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tULRU5Anl2v7",
      "metadata": {
        "id": "tULRU5Anl2v7"
      },
      "outputs": [],
      "source": [
        "#extract metadata values\n",
        "def extract(col_name, included_columns=[''], excluded_columns=[''], inter=True):\n",
        "\n",
        "  if excluded_columns != ['']:\n",
        "    if inter == True:\n",
        "      if col_name in excluded_columns:\n",
        "        return 'other'\n",
        "      elif col_name in included_columns:\n",
        "        return str(col_name)\n",
        "      else:\n",
        "        return 'other'\n",
        "    elif inter == False:\n",
        "      if col_name in excluded_columns:\n",
        "        return 'other'\n",
        "      elif col_name in included_columns:\n",
        "        return 'included'\n",
        "      else:\n",
        "        return 'other'\n",
        "\n",
        "  if included_columns != ['']:\n",
        "    if inter == True:\n",
        "      if col_name in included_columns:\n",
        "        return str(col_name)\n",
        "      else:\n",
        "        return 'other'\n",
        "    elif inter == False:\n",
        "      if col_name in included_columns:\n",
        "        return 'included'\n",
        "      else:\n",
        "        return 'other'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LCKzBNu4v28D",
      "metadata": {
        "id": "LCKzBNu4v28D"
      },
      "outputs": [],
      "source": [
        "def exclude_pairs_from_table(base_df, exclude_df):\n",
        "\n",
        "  cols_to_merge = ['Feature_1', 'Feature_2']\n",
        "\n",
        "  merged_df = pd.merge(base_df, exclude_df[cols_to_merge], how='left', on=cols_to_merge, indicator=True)\n",
        "  base_df = merged_df[merged_df['_merge'] == 'left_only'].drop('_merge', axis=1)\n",
        "\n",
        "  return base_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RqciIfeWwO8o",
      "metadata": {
        "id": "RqciIfeWwO8o"
      },
      "outputs": [],
      "source": [
        "def display_pairs(df, display_pair_name):\n",
        "\n",
        "  very_positive = df[df['category'] == 'very_positive']\n",
        "  positive = df[df['category'] == 'positive']\n",
        "  negative = df[df['category'] == 'negative']\n",
        "  very_negative = df[df['category'] == 'very_negative']\n",
        "\n",
        "  if len(very_positive) != 0:\n",
        "    print(f\"\\nVery Positively Correlated {display_pair_name} Pairs:\")\n",
        "    display(very_positive.sort_values(by='Correlation', ascending=False).drop(columns='category').reset_index(drop=True))\n",
        "\n",
        "  if len(positive) != 0:\n",
        "    print(f\"\\n\\n\\nPositively Correlated {display_pair_name} Pairs:\")\n",
        "    display(positive.sort_values(by='Correlation', ascending=False).drop(columns='category').reset_index(drop=True))\n",
        "\n",
        "  if len(negative) != 0:\n",
        "    print(f\"\\n\\n\\nNegatively Correlated {display_pair_name} Pairs:\")\n",
        "    display(negative.sort_values(by='Correlation', ascending=True).drop(columns='category').reset_index(drop=True))\n",
        "\n",
        "  if len(very_negative) != 0:\n",
        "    print(f\"\\n\\n\\nVery Negatively Correlated {display_pair_name} Pairs:\")\n",
        "    display(very_negative.sort_values(by='Correlation', ascending=True).drop(columns='category').reset_index(drop=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9stLLsjaakSe",
      "metadata": {
        "id": "9stLLsjaakSe"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EKgKrFZMalq5",
      "metadata": {
        "id": "EKgKrFZMalq5"
      },
      "source": [
        "**Function for Scatter-Plotting:**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TUOsl7yVIsZE",
      "metadata": {
        "id": "TUOsl7yVIsZE"
      },
      "source": [
        "This function generates a grid of scatter plots for a list of feature pairs, coloured by `popularity`. It's used to quickly explore how pairs of features interact and whether any visible clustering, gradient, or shape exists in relation to popularity. The function supports a custom colour map, and lays out the plots in rows of three for readability. A colour bar is included in each subplot to show the popularity gradient.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-XDQ-alAarDi",
      "metadata": {
        "id": "-XDQ-alAarDi"
      },
      "outputs": [],
      "source": [
        "def plot_scatter_grid(df, scatter_pairs, title_suffix='', cmap='viridis'):\n",
        "    num_plots = len(scatter_pairs)\n",
        "    cols = 3\n",
        "\n",
        "    # use ceiling division to get number of rows\n",
        "    rows = (num_plots + cols - 1) // cols\n",
        "\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(18, 5 * rows))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    # plot each scatter pair using simple index-based loop\n",
        "    for i in range(num_plots):\n",
        "        x = scatter_pairs[i][0]\n",
        "        y = scatter_pairs[i][1]\n",
        "\n",
        "        scatter = axes[i].scatter(df[x], df[y], c=df['popularity'], cmap=cmap)\n",
        "        axes[i].set_title(f'{x} vs {y} {title_suffix}')\n",
        "        axes[i].set_xlabel(x)\n",
        "        axes[i].set_ylabel(y)\n",
        "        fig.colorbar(scatter, ax=axes[i], label='Popularity')\n",
        "\n",
        "    # remove unused subplot axes if there are any\n",
        "    for j in range(num_plots, len(axes)):\n",
        "        fig.delaxes(axes[j])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LCYdvxeKJMFI",
      "metadata": {
        "id": "LCYdvxeKJMFI"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "T-Z0pxczyGFs",
      "metadata": {
        "id": "T-Z0pxczyGFs"
      },
      "source": [
        "####**Popularity Correlation Pairs**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iuqnV0NDKdKd",
      "metadata": {
        "id": "iuqnV0NDKdKd"
      },
      "source": [
        "This table isolates all feature pairs where `popularity` is one of the variables. As expected, the strongest correlations are with `artist_avg_popularity` and `genre_avg_popularity`, both of which reflect external metadata influence. We also observe moderate correlations with `year_avg_popularity` and `year_song_count`, indicating a temporal influence, and minor musical feature relationships like `danceability` and `loudness`. Negatively correlated features include `instrumentalness` and `duration_mins`, suggesting that instrumentals and longer tracks tend to underperform in popularity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "l-fvCLniw2aa",
      "metadata": {
        "id": "l-fvCLniw2aa"
      },
      "outputs": [],
      "source": [
        "popularity_pairs = remove_mutually_exclusive(correlate_categorise(corr_matrix), included_columns=['popularity'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XnAaT19-xECj",
      "metadata": {
        "id": "XnAaT19-xECj"
      },
      "outputs": [],
      "source": [
        "display_pairs(popularity_pairs, display_pair_name='Popularity')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wCQX_VGLKf4b",
      "metadata": {
        "id": "wCQX_VGLKf4b"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ovd54EnXUex7",
      "metadata": {
        "id": "ovd54EnXUex7"
      },
      "source": [
        "####**Metadata Correlation Pairs (Excluding Inter-Metadata Pairs)**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "T-5wi_QoKeBe",
      "metadata": {
        "id": "T-5wi_QoKeBe"
      },
      "source": [
        "This section displays feature pairs involving at least one metadata-derived column (such as `*_avg_popularity` or `*_song_count`), excluding any combinations that are purely inter-metadata (e.g. artist metadata <-> genre metadata). Popularity-based pairs have also been filtered out to avoid duplication with the previous section.\n",
        "\n",
        "Positive correlations show expected relationships like higher `genre_avg_popularity` aligning with more `danceable` or `valence`-rich tracks, while `acousticness` and `genre_song_count` also trend together, suggesting genre-scale acoustic trends. On the negative side, instrumentals and longer tracks tend to be associated with lower average popularity across both artist and genre groupings. These relationships help identify softer trends that sit between raw audio features and broader artist/genre effects.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UX8677aUkzTA",
      "metadata": {
        "id": "UX8677aUkzTA"
      },
      "outputs": [],
      "source": [
        "metadata_cols = ['artist_avg_popularity', 'genre_avg_popularity', 'year_avg_popularity', 'artist_song_count', 'genre_song_count', 'year_song_count']\n",
        "metadata_pairs = remove_mutually_exclusive(correlate_categorise(corr_matrix), included_columns=metadata_cols, inter=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-0qyLbQtRD1E",
      "metadata": {
        "id": "-0qyLbQtRD1E"
      },
      "outputs": [],
      "source": [
        "#excluding popularity pair features we have already seen\n",
        "metadata_pairs = exclude_pairs_from_table(metadata_pairs, popularity_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "C17bGhlJQCyq",
      "metadata": {
        "id": "C17bGhlJQCyq"
      },
      "outputs": [],
      "source": [
        "display_pairs(metadata_pairs, display_pair_name='Metadata')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6yS_tfDBQDXP",
      "metadata": {
        "id": "6yS_tfDBQDXP"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "thMk3Nk4VT77",
      "metadata": {
        "id": "thMk3Nk4VT77"
      },
      "source": [
        "####**Inter-Metadata Correlation Pairs**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cvfytDIJMOIS",
      "metadata": {
        "id": "cvfytDIJMOIS"
      },
      "source": [
        "This section isolates correlations between metadata-derived features only - specifically those combining different types of popularity and song count statistics (e.g. artist metadata <-> year metadata). Popularity-based pairs already explored earlier are excluded to avoid redundancy.\n",
        "\n",
        "Strong correlations include `artist_avg_popularity` to `genre_avg_popularity`, and `year_avg_popularity` to `year_song_count`, reflecting structural overlap between these aggregated metrics. Weaker negative correlations suggest that more prolific artists and genres tend to be associated with lower average popularity, which may point to long-tail behaviour or oversaturation effects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Wsrm2RN8lV7_",
      "metadata": {
        "id": "Wsrm2RN8lV7_"
      },
      "outputs": [],
      "source": [
        "metadata_cols = ['artist_avg_popularity', 'genre_avg_popularity', 'year_avg_popularity', 'artist_song_count', 'genre_song_count', 'year_song_count']\n",
        "intermetadata_pairs = remove_mutually_exclusive(correlate_categorise(corr_matrix), included_columns=metadata_cols, inter=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "g0_dErf2ZBMt",
      "metadata": {
        "id": "g0_dErf2ZBMt"
      },
      "outputs": [],
      "source": [
        "#excluding popularity pair features we have already seen\n",
        "intermetadata_pairs = exclude_pairs_from_table(exclude_pairs_from_table(intermetadata_pairs, popularity_pairs), metadata_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_zdGoCRaZq6J",
      "metadata": {
        "id": "_zdGoCRaZq6J"
      },
      "outputs": [],
      "source": [
        "display_pairs(intermetadata_pairs, display_pair_name='Inter-Metadata')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lrT42LqEVW9P",
      "metadata": {
        "id": "lrT42LqEVW9P"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "g13cDV4wgBlr",
      "metadata": {
        "id": "g13cDV4wgBlr"
      },
      "source": [
        "####**Feature Correlation Pairs**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_sBQY7smMwyO",
      "metadata": {
        "id": "_sBQY7smMwyO"
      },
      "source": [
        "This section isolates correlations between non-metadata audio features (e.g. `energy`, `valence`, `loudness`, etc.), excluding any pairs already shown in previous categories. The strongest relationship is between `energy` and `loudness` (r = 0.79), which reflects their shared role in musical intensity.\n",
        "\n",
        "Other notable positive correlations include `danceability` with `valence`, and `instrumentalness` with `duration_mins`, suggesting that happier tracks tend to be more danceable, and instrumentals are often longer. Strong negative correlations appear between `acousticness` and both `energy` and `loudness`, highlighting a fundamental contrast between high-energy productions and more stripped-back acoustic recordings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "v1EZWWY_ei8o",
      "metadata": {
        "id": "v1EZWWY_ei8o"
      },
      "outputs": [],
      "source": [
        "feat_pairs = correlate_categorise(corr_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "T0kd9gNDKojC",
      "metadata": {
        "id": "T0kd9gNDKojC"
      },
      "outputs": [],
      "source": [
        "#excluding popularity pair features we have already seen\n",
        "feat_pairs = exclude_pairs_from_table(exclude_pairs_from_table(exclude_pairs_from_table(\n",
        "                feat_pairs, popularity_pairs), metadata_pairs), intermetadata_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lHceRUmSKei2",
      "metadata": {
        "id": "lHceRUmSKei2"
      },
      "outputs": [],
      "source": [
        "display_pairs(feat_pairs, display_pair_name='Feature')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8vxKh-kHKfpy",
      "metadata": {
        "id": "8vxKh-kHKfpy"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ygifQpLhbsQC",
      "metadata": {
        "id": "ygifQpLhbsQC"
      },
      "source": [
        "####**Categorical Correlation Pairs**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4drLztXkOIXY",
      "metadata": {
        "id": "4drLztXkOIXY"
      },
      "source": [
        "This section explores meaningful correlations involving categorical one-hot encoded features such as `genre_*`, `year_*`, and musical metadata indicators like `keyOf*`. Columns representing metadata values (e.g. `genre_avg_popularity`, `year_song_count`) are specified to be excluded as they have a similar name to the one-hot columns, e.g. `genre_x`.\n",
        "\n",
        "High positive correlations include strong genre-popularity links such as `genre_hip-hop`, `genre_dance`, and `genre_pop`, as well as recent years (`year_2022`, `year_2021`) showing elevated average popularity - consistent with the dataset's known recency bias. Negative correlations identify older years and certain niche genres (e.g. `trip-hop`, `piano`) that are underrepresented or consistently less popular.\n",
        "\n",
        "These correlations reflect broader trends in musical culture and industry shifts over time, and can be useful for genre-specific modelling or for understanding systemic popularity advantages across categories.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0iywYlxZmQNL",
      "metadata": {
        "id": "0iywYlxZmQNL"
      },
      "outputs": [],
      "source": [
        "excluded_cols = ['genre_avg_popularity','genre_song_count','year_avg_popularity','year_song_count'] # Note that these columns are excluded from the groupings, not from the pairs entirely -\n",
        "included_cols = []                                                                                  # it means that these columns are forced to identify as NOT a part of included_cols.\n",
        "keywords = ['keyOf', 'genre_', 'year_']\n",
        "\n",
        "#appending a list of all one-hot columns into included_cols[]\n",
        "for column in train_df:\n",
        "  for keyword in keywords:\n",
        "    if keyword in column and column not in excluded_cols:\n",
        "      included_cols.append(column)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PzSvwhu-pNlO",
      "metadata": {
        "id": "PzSvwhu-pNlO"
      },
      "outputs": [],
      "source": [
        "one_hot_pairs = remove_mutually_exclusive(correlate_categorise(one_hot_corr_matrix, high_bound=0.3, low_bound=0.2),\n",
        "                                          included_columns=included_cols, excluded_columns=excluded_cols, inter=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mXfyW5kmgr8m",
      "metadata": {
        "id": "mXfyW5kmgr8m"
      },
      "outputs": [],
      "source": [
        "display_pairs(one_hot_pairs, display_pair_name='Categorical')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wKTQwu_Jo0rq",
      "metadata": {
        "id": "wKTQwu_Jo0rq"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eFYZ80sUcSP4",
      "metadata": {
        "id": "eFYZ80sUcSP4"
      },
      "source": [
        "####**Temporally Respective Correlation Pairs**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZRv11SnUPoa3",
      "metadata": {
        "id": "ZRv11SnUPoa3"
      },
      "source": [
        "This section displays correlations between engineered temporal features derived from `temporal_df`, including `year_decimal`, `running_avg_popularity`, and `running_song_count`. These variables are designed to capture the influence of release timing on popularity in a way that avoids data leakage.\n",
        "\n",
        "The strongest correlations are between the temporal tracking features themselves, which confirms that the temporal pipeline functions as intended. Additional correlations (e.g. `popularity` <-> `running_avg_popularity`, or `artist_avg_popularity` <-> `year_decimal`) show clear trends over time and reinforce the idea that newer releases and rising popularity are closely linked.\n",
        "\n",
        "Negative correlations with `duration_mins` and `valence` suggest that emotional tone and track length may shift subtly across time, potentially due to changing production standards or listener preferences.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "g7b53EBstzdb",
      "metadata": {
        "id": "g7b53EBstzdb"
      },
      "outputs": [],
      "source": [
        "temporal_cols = ['year', 'year_decimal', 'artist_avg_popularity', 'genre_avg_popularity', 'year_avg_popularity', 'artist_song_count', 'genre_song_count', 'year_song_count', 'running_avg_popularity', 'running_song_count']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AwrNBtcf61Hs",
      "metadata": {
        "id": "AwrNBtcf61Hs"
      },
      "outputs": [],
      "source": [
        "temporal_pairs = remove_mutually_exclusive(correlate_categorise(temporal_corr_matrix),included_columns=temporal_cols, inter=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "woXzFtNj8bOr",
      "metadata": {
        "id": "woXzFtNj8bOr"
      },
      "outputs": [],
      "source": [
        "#excluding popularity pair features we have already seen\n",
        "temporal_pairs = exclude_pairs_from_table(exclude_pairs_from_table(exclude_pairs_from_table(\n",
        "                  temporal_pairs, popularity_pairs), metadata_pairs), intermetadata_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "s34J9O7661vR",
      "metadata": {
        "id": "s34J9O7661vR"
      },
      "outputs": [],
      "source": [
        "display_pairs(temporal_pairs, display_pair_name='Temporal')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ofG4aZ5wD-N",
      "metadata": {
        "id": "9ofG4aZ5wD-N"
      },
      "source": [
        "**[END]**\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qlKDw1KK60Sp",
      "metadata": {
        "id": "qlKDw1KK60Sp"
      },
      "source": [
        "##**2.3** Visualising Correlated Multivariate Relationships"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LQsClN9d63Hk",
      "metadata": {
        "id": "LQsClN9d63Hk"
      },
      "source": [
        "This section presents a focused set of bivariate scatter plots coloured by popularity to visually explore the relationships between key continuous features. Each scatter plot being coloured by `popularity` offers insight into how certain feature combinations might influence a track's success. All plots are shown in both their raw and `pruned_df` forms - the latter helping to reveal more distinct patterns by removing predictably biased data.\n",
        "\n",
        "Clear non-linear relationships appear in plots like `energy` vs `loudness`, or `valence` vs `speechiness`, with visibly skewed density and curved boundaries. The pruned version generally shows tighter and more meaningful clustering, suggesting that filtering increases interpretability in a way that aligns with downstream modelling objectives.\n",
        "\n",
        "Overall, this section acts as a visual precursor to correlation analysis, helping surface feature pairs worth investigating."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HbkWebif640J",
      "metadata": {
        "id": "HbkWebif640J"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ObfLXlaMr_SR",
      "metadata": {
        "id": "ObfLXlaMr_SR"
      },
      "source": [
        "###***2.3.1*** - Audio Feature and Popularity Multivariate Scatterplots"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IILPWoj4sE38",
      "metadata": {
        "id": "IILPWoj4sE38"
      },
      "source": [
        "These scatter plots includes all of the relevant feature relationships determined in a previous section (**2.2.4**), coloured by popularity for both *df* and *pruned_df* - highlighting the effectiveness of *pruned_df* in revealing patterns amongst the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mkR0WUJGsFFQ",
      "metadata": {
        "id": "mkR0WUJGsFFQ"
      },
      "outputs": [],
      "source": [
        "scatter_pairs = [\n",
        "    ('energy', 'loudness'), # -----------------VERY POSITIVE\n",
        "\n",
        "    ('danceability', 'valence'), # ------------POSITIVE\n",
        "    ('instrumentalness', 'duration_mins'),\n",
        "    ('energy', 'tempo'),\n",
        "    ('energy', 'liveness'),\n",
        "    ('energy', 'speechiness'),\n",
        "    ('energy', 'valence'),\n",
        "    ('loudness', 'tempo'),\n",
        "    ('loudness', 'valence'),\n",
        "    ('loudness', 'speechiness'),\n",
        "    ('loudness', 'liveness'),\n",
        "    ('speechiness', 'liveness'),\n",
        "    ('acousticness', 'instrumentalness'),\n",
        "\n",
        "    ('loudness', 'instrumentalness'), # -------NEGATIVE\n",
        "    ('instrumentalness', 'valence'),\n",
        "    ('acousticness', 'tempo'),\n",
        "    ('danceability', 'liveness'),\n",
        "    ('valence', 'duration_mins'),\n",
        "    ('speechiness', 'instrumentalness'),\n",
        "    ('energy', 'instrumentalness'),\n",
        "    ('speechiness', 'acousticness'),\n",
        "    ('danceability', 'tempo'),\n",
        "    ('acousticness', 'liveness'),\n",
        "    ('acousticness', 'duration_mins'),\n",
        "    ('speechiness', 'duration_mins'),\n",
        "\n",
        "    ('energy', 'acousticness'), # -------------VERY NEGATIVE\n",
        "    ('loudness', 'acousticness')\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ftm7MNyFsL4n",
      "metadata": {
        "id": "Ftm7MNyFsL4n"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_W4Gmf3IsMvh",
      "metadata": {
        "id": "_W4Gmf3IsMvh"
      },
      "outputs": [],
      "source": [
        "# Plot using df\n",
        "plot_scatter_grid(df, scatter_pairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wllWKuUjsOSh",
      "metadata": {
        "id": "wllWKuUjsOSh"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6jN3j7prsO8Y",
      "metadata": {
        "id": "6jN3j7prsO8Y"
      },
      "outputs": [],
      "source": [
        "# Plot using pruned_df\n",
        "plot_scatter_grid(pruned_df, scatter_pairs, title_prefix='(pruned)')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CH5snh0P7Izo",
      "metadata": {
        "id": "CH5snh0P7Izo"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_d4iBDC-4Zyo",
      "metadata": {
        "id": "_d4iBDC-4Zyo"
      },
      "source": [
        "###***2.3.2*** - Popularity, Loudness and Energy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rD-8oAMG4hBg",
      "metadata": {
        "id": "rD-8oAMG4hBg"
      },
      "source": [
        "This scatter plot visualises the relationship between `loudness` and `energy`, with colour representing `popularity`. The pruned plot (`scale=1.65`) removes points where popularity is heavily explained by artist reputation, revealing a sharp clustering in the upper-right. This aligns with modern production trends where high energy and loud mastering dominate among highly popular songs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e2IktbwoOrj",
      "metadata": {
        "id": "0e2IktbwoOrj"
      },
      "outputs": [],
      "source": [
        "pruned_df = Prune(df,\n",
        "                       df['popularity'],\n",
        "                       df['artist_avg_popularity'],\n",
        "                       scale=1.65)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q1AcpFLg5J4h",
      "metadata": {
        "id": "Q1AcpFLg5J4h"
      },
      "outputs": [],
      "source": [
        "ax1 = df.plot.scatter(x='loudness', y='energy', c='popularity', cmap='viridis')\n",
        "ax2 = pruned_df.plot.scatter(x='loudness', y='energy', c='popularity', cmap='viridis')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ODq1k8zIfGRK",
      "metadata": {
        "id": "ODq1k8zIfGRK"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9xT4cHmX__M",
      "metadata": {
        "id": "c9xT4cHmX__M"
      },
      "source": [
        "These plots highlight that the most popular and well-performing songs cluster tightly within a specific loudness and energy range - typically loud (around -5 dB) and highly energetic (~0.7). This suggests that modern production trends favour a consistent dynamic profile in hit tracks, where outliers in either direction (too quiet or low-energy) are rarely among the most successful.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iv9k9UZ-fHHb",
      "metadata": {
        "id": "iv9k9UZ-fHHb"
      },
      "outputs": [],
      "source": [
        "ax1 = pruned_df[pruned_df['popularity'] > 75].plot.scatter(\n",
        "                                                  x='loudness',\n",
        "                                                  y='energy',\n",
        "                                                  c='popularity',\n",
        "                                                  colormap='viridis')\n",
        "\n",
        "ax2 = df[df['artist_avg_popularity'] > 60].plot.scatter(\n",
        "                                                  x='loudness',\n",
        "                                                  y='energy',\n",
        "                                                  c='popularity',\n",
        "                                                  colormap='viridis')\n",
        "\n",
        "ax3 = df[df['artist_avg_popularity'] > 30].plot.scatter(\n",
        "                                                  x='loudness',\n",
        "                                                  y='energy',\n",
        "                                                  c='popularity',\n",
        "                                                  colormap='viridis')\n",
        "\n",
        "ax4 = df[df['artist_avg_popularity'] < 10].plot.scatter(\n",
        "                                                  x='loudness',\n",
        "                                                  y='energy',\n",
        "                                                  c='popularity',\n",
        "                                                  colormap='viridis')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "L2OvhFa7fHn5",
      "metadata": {
        "id": "L2OvhFa7fHn5"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RNrRHqBn4nEh",
      "metadata": {
        "id": "RNrRHqBn4nEh"
      },
      "source": [
        "###***2.3.3*** - Popularity, Danceability and Valence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57rocw1i4uo6",
      "metadata": {
        "id": "57rocw1i4uo6"
      },
      "source": [
        "This scatter plot explores the relationship between `danceability` and `valence`, with colour indicating `popularity`. After applying the pruning filter (`scale=1.55`) to remove songs whose popularity closely mirrors their artist average, we observe a clearer clustering of higher popularity in the central and upper-right region of the distribution. This suggests that songs which are both happy (valence) and danceable tend to resonate more with listeners (where danceability is the dominant feature), supporting the idea that emotionally upbeat tracks hold broad commercial appeal.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "n-CNaJO6oMh2",
      "metadata": {
        "id": "n-CNaJO6oMh2"
      },
      "outputs": [],
      "source": [
        "pruned_df = Prune(df,\n",
        "                       df['popularity'],\n",
        "                       df['artist_avg_popularity'],\n",
        "                       scale=1.55)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dE9trn5V4uy_",
      "metadata": {
        "id": "dE9trn5V4uy_"
      },
      "outputs": [],
      "source": [
        "ax1 = df.plot.scatter(x='danceability',\n",
        "                      y='valence',\n",
        "                      c='popularity',\n",
        "                      cmap='viridis')\n",
        "\n",
        "ax2 = pruned_df.plot.scatter(x='danceability',\n",
        "                                y='valence',\n",
        "                                c='popularity',\n",
        "                                cmap='viridis')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_cnpJJcrnsJL",
      "metadata": {
        "id": "_cnpJJcrnsJL"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iyMn2cUvnuij",
      "metadata": {
        "id": "iyMn2cUvnuij"
      },
      "source": [
        "We further investigate the same relationship by filtering the data across different popularity and artist popularity bands. Notably, high popularity songs (`popularity > 60`) continue to cluster in the top-right region, while filtering for artists with high average popularity reveals a similar emotional and danceable signature. Conversely, songs from low-popularity artists exhibit no such concentration, supporting the idea that artist influence plays a major role in driving the emotional accessibility of popular music.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ArPlZ4TiOe5",
      "metadata": {
        "id": "5ArPlZ4TiOe5"
      },
      "outputs": [],
      "source": [
        "ax1 = pruned_df[pruned_df['popularity'] > 60].plot.scatter(\n",
        "                                                  x='danceability',\n",
        "                                                  y='valence',\n",
        "                                                  c='popularity',\n",
        "                                                  colormap='viridis')\n",
        "\n",
        "ax2 = df[df['artist_avg_popularity'] > 60].plot.scatter(\n",
        "                                                  x='danceability',\n",
        "                                                  y='valence',\n",
        "                                                  c='popularity',\n",
        "                                                  colormap='viridis')\n",
        "\n",
        "ax3 = df[df['artist_avg_popularity'] > 30].plot.scatter(\n",
        "                                                  x='danceability',\n",
        "                                                  y='valence',\n",
        "                                                  c='popularity',\n",
        "                                                  colormap='viridis')\n",
        "\n",
        "ax4 = df[df['artist_avg_popularity'] < 10].plot.scatter(\n",
        "                                                  x='danceability',\n",
        "                                                  y='valence',\n",
        "                                                  c='popularity',\n",
        "                                                  colormap='viridis')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zOmTEB7yfexc",
      "metadata": {
        "id": "zOmTEB7yfexc"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kJiQf7lb4vLK",
      "metadata": {
        "id": "kJiQf7lb4vLK"
      },
      "source": [
        "###***2.3.4*** - Popularity, Tempo and Danceability"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KCBE5SP542XP",
      "metadata": {
        "id": "KCBE5SP542XP"
      },
      "source": [
        "This scatter plot explores the relationship between `tempo` and `danceability`, with colour indicating `popularity`. Using the pruning filter (`scale=1.55`), we remove popularity values that closely align with the artist averages, revealing a clearer structure. The filtered data shows that highly danceable songs tend to cluster around mid-tempo ranges (roughly 90-140 BPM), suggesting that extreme tempos may hinder danceability and mainstream appeal.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "im3qze_Ng5IZ",
      "metadata": {
        "id": "im3qze_Ng5IZ"
      },
      "outputs": [],
      "source": [
        "pruned_df = Prune(df,\n",
        "                       df['popularity'],\n",
        "                       df['artist_avg_popularity'],\n",
        "                       scale=1.55)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf2FCoAg42hO",
      "metadata": {
        "id": "cf2FCoAg42hO"
      },
      "outputs": [],
      "source": [
        "ax1 = df.plot.scatter(x='tempo', y='danceability', c='popularity', cmap='viridis')\n",
        "ax2 = pruned_df.plot.scatter(x='tempo', y='danceability', c='popularity', cmap='viridis')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mPwrLrdYpCkd",
      "metadata": {
        "id": "mPwrLrdYpCkd"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yWJ9ScVVpyDh",
      "metadata": {
        "id": "yWJ9ScVVpyDh"
      },
      "source": [
        "Interestingly, when filtering by artist average popularity, we find that the pattern seen in the pruned dataset is broadly replicated-particularly among artists with moderate-to-high popularity. These clusters reinforce the idea that high-performing tracks typically favour a mid-tempo, high-danceability profile. Conversely, artists with low average popularity produce far more scattered results, suggesting less consistent alignment with popular sonic traits.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DpvEGf5HgotR",
      "metadata": {
        "id": "DpvEGf5HgotR"
      },
      "outputs": [],
      "source": [
        "ax1 = pruned_df[pruned_df['popularity'] > 70].plot.scatter(\n",
        "                                                  x='tempo',\n",
        "                                                  y='danceability',\n",
        "                                                  c='popularity',\n",
        "                                                  colormap='viridis')\n",
        "\n",
        "ax2 = df[df['artist_avg_popularity'] > 60].plot.scatter(\n",
        "                                                  x='tempo',\n",
        "                                                  y='danceability',\n",
        "                                                  c='popularity',\n",
        "                                                  colormap='viridis')\n",
        "\n",
        "ax3 = df[df['artist_avg_popularity'] > 30].plot.scatter(\n",
        "                                                  x='tempo',\n",
        "                                                  y='danceability',\n",
        "                                                  c='popularity',\n",
        "                                                  colormap='viridis')\n",
        "\n",
        "ax4 = df[df['artist_avg_popularity'] < 10].plot.scatter(\n",
        "                                                  x='tempo',\n",
        "                                                  y='danceability',\n",
        "                                                  c='popularity',\n",
        "                                                  colormap='viridis')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5U4dcwIOkU8P",
      "metadata": {
        "id": "5U4dcwIOkU8P"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ae9WM3fp42oq",
      "metadata": {
        "id": "Ae9WM3fp42oq"
      },
      "source": [
        "###***2.3.5*** - Popularity, Speechiness and Duration(mins)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_mF1_8gt5ARQ",
      "metadata": {
        "id": "_mF1_8gt5ARQ"
      },
      "source": [
        "This scatter plot explores the relationship between `speechiness` and `duration_mins`, coloured by `popularity`. Applying the pruning filter (`scale=1.75`) reveals a clearer trend: higher popularity tracks tend to avoid extremes - neither overly short nor long, and with moderate levels of speechiness. The filter helps isolate this more stable core, suggesting that moderately structured vocal content and song length may contribute to broader appeal.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "n6KjljxpirYn",
      "metadata": {
        "id": "n6KjljxpirYn"
      },
      "outputs": [],
      "source": [
        "pruned_df = Prune(df,\n",
        "                       df['popularity'],\n",
        "                       df['artist_avg_popularity'],\n",
        "                       scale=1.75)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q8LENiS-5AZB",
      "metadata": {
        "id": "Q8LENiS-5AZB"
      },
      "outputs": [],
      "source": [
        "ax1 = df.plot.scatter(x='speechiness', y='duration_mins', c='popularity', cmap='viridis')\n",
        "ax2 = pruned_df.plot.scatter(x='speechiness', y='duration_mins', c='popularity', cmap='viridis')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hc2A1mg6rXDm",
      "metadata": {
        "id": "hc2A1mg6rXDm"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jrdLdbGTrX06",
      "metadata": {
        "id": "jrdLdbGTrX06"
      },
      "source": [
        "When segmenting by artist popularity, we observe that higher-quality, more popular songs (top) concentrate in a narrower range of moderate speechiness and durations between 2-5 minutes. This aligns with mainstream expectations for pop structures. In contrast, less popular artists (bottom) show wide variance, especially towards extremes-longer, speech-heavy tracks that likely deviate from norms. These plots further support the idea that predictability and structure play a role in achieving mass appeal.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NESu9fPNiyFQ",
      "metadata": {
        "id": "NESu9fPNiyFQ"
      },
      "outputs": [],
      "source": [
        "ax1 = pruned_df[pruned_df['popularity'] > 60].plot.scatter(\n",
        "                                                  x='speechiness',\n",
        "                                                  y='duration_mins',\n",
        "                                                  c='popularity',\n",
        "                                                  colormap='viridis')\n",
        "\n",
        "ax2 = df[df['artist_avg_popularity'] > 60].plot.scatter(\n",
        "                                                  x='speechiness',\n",
        "                                                  y='duration_mins',\n",
        "                                                  c='popularity',\n",
        "                                                  colormap='viridis')\n",
        "\n",
        "ax3 = df[df['artist_avg_popularity'] > 30].plot.scatter(\n",
        "                                                  x='speechiness',\n",
        "                                                  y='duration_mins',\n",
        "                                                  c='popularity',\n",
        "                                                  colormap='viridis')\n",
        "\n",
        "ax4 = df[df['artist_avg_popularity'] < 10].plot.scatter(\n",
        "                                                  x='speechiness',\n",
        "                                                  y='duration_mins',\n",
        "                                                  c='popularity',\n",
        "                                                  colormap='viridis')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AZ6ikSr1izG7",
      "metadata": {
        "id": "AZ6ikSr1izG7"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "U01xco-_5Aj2",
      "metadata": {
        "id": "U01xco-_5Aj2"
      },
      "source": [
        "###***2.3.6*** - Popularity, Energy and Acousticness"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YGSGfhgG5HYH",
      "metadata": {
        "id": "YGSGfhgG5HYH"
      },
      "source": [
        "This scatter plot explores the relationship between `energy` and `acousticness`, coloured by `popularity`. After applying the pruning filter (`scale=1.75`), a clear inverse pattern emerges - tracks with high energy tend to be less acoustic. The filtered data shows that popular tracks cluster in this low-acousticness/high-energy zone, reinforcing the idea that energetic, studio-produced music typically outperforms acoustic recordings in the mainstream.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pruned_df = Prune(df,\n",
        "                       df['popularity'],\n",
        "                       df['artist_avg_popularity'],\n",
        "                       scale=1.75)"
      ],
      "metadata": {
        "id": "PZw8iJx21k7z"
      },
      "id": "PZw8iJx21k7z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nNmURGZ95Hdi",
      "metadata": {
        "id": "nNmURGZ95Hdi"
      },
      "outputs": [],
      "source": [
        "ax1 = df.plot.scatter(x='energy',\n",
        "                      y='acousticness',\n",
        "                      c='popularity',\n",
        "                      cmap='viridis')\n",
        "\n",
        "ax2 = pruned_df.plot.scatter(x='energy',\n",
        "                                y='acousticness',\n",
        "                                c='popularity',\n",
        "                                cmap='viridis')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Msm_CIPssVCw",
      "metadata": {
        "id": "Msm_CIPssVCw"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "I1y-ZhRKsV2v",
      "metadata": {
        "id": "I1y-ZhRKsV2v"
      },
      "source": [
        "The filtered scatter plot (popularity > 70) highlights a strong inverse relationship between `energy` and `acousticness`, with most high-performing tracks concentrated in the low-acousticness, high-energy quadrant. Compared to artists with varying average popularity, this cluster is especially pronounced among top performers. These results support the view that high-energy, non-acoustic (i.e., more electronically produced) tracks dominate popular music, while lower-tier artists tend to show greater acoustic variation and less energetic content.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S6zVN-zGluk4",
      "metadata": {
        "id": "S6zVN-zGluk4"
      },
      "outputs": [],
      "source": [
        "ax1 = pruned_df[pruned_df['popularity'] > 70].plot.scatter(\n",
        "                                                  x='energy', y='acousticness', c='popularity',\n",
        "                                                  colormap='viridis')\n",
        "\n",
        "ax2 = df[df['artist_avg_popularity'] > 60].plot.scatter(\n",
        "                                                  x='energy', y='acousticness', c='popularity',\n",
        "                                                  colormap='viridis')\n",
        "\n",
        "ax3 = df[df['artist_avg_popularity'] > 30].plot.scatter(\n",
        "                                                  x='energy', y='acousticness', c='popularity',\n",
        "                                                  colormap='viridis')\n",
        "\n",
        "ax4 = df[df['artist_avg_popularity'] < 10].plot.scatter(\n",
        "                                                  x='energy', y='acousticness', c='popularity',\n",
        "                                                  colormap='viridis')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "C2FdsOXHluyA",
      "metadata": {
        "id": "C2FdsOXHluyA"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZkJrP2DA6kQd",
      "metadata": {
        "id": "ZkJrP2DA6kQd"
      },
      "source": [
        "###***2.3.7*** - Popularity, Instrumentalness and Loudness"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wvvuzzDa6nkz",
      "metadata": {
        "id": "wvvuzzDa6nkz"
      },
      "source": [
        "This scatter plot explores the relationship between `instrumentalness` and `loudness`, with colour indicating `popularity`. After applying the pruning filter (`scale=1.75`), the inverse association becomes more visible: highly instrumental tracks are typically quieter and less popular. The filtered data helps surface this pattern more clearly, suggesting that louder, more vocal-forward songs are favoured in mainstream music.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pruned_df = Prune(df,\n",
        "                       df['popularity'],\n",
        "                       df['artist_avg_popularity'],\n",
        "                       scale=1.75)"
      ],
      "metadata": {
        "id": "lfM5g-rE1ply"
      },
      "id": "lfM5g-rE1ply",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ErbnY33Y6nqM",
      "metadata": {
        "id": "ErbnY33Y6nqM"
      },
      "outputs": [],
      "source": [
        "ax1 = df.plot.scatter(x='instrumentalness',\n",
        "                      y='loudness',\n",
        "                      c='popularity',\n",
        "                      colormap='viridis')\n",
        "\n",
        "ax2 = pruned_df.plot.scatter(x='instrumentalness',\n",
        "                      y='loudness',\n",
        "                      c='popularity',\n",
        "                      colormap='viridis')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_IpvNFg8ubBY",
      "metadata": {
        "id": "_IpvNFg8ubBY"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a67DQ10fugtF",
      "metadata": {
        "id": "a67DQ10fugtF"
      },
      "source": [
        "Compared across filtered subsets, the relationship between `instrumentalness` and `loudness` becomes clearer. Tracks by low-popularity artists are consistently quieter and more instrumental, while popular tracks cluster toward low `instrumentalness` and high `loudness`. These filtered views reinforce the notion that instrumental music tends to underperform commercially compared to louder, vocal-forward recordings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "al9-x2pY61Ci",
      "metadata": {
        "id": "al9-x2pY61Ci"
      },
      "outputs": [],
      "source": [
        "ax1 = pruned_df[pruned_df['popularity'] > 70].plot.scatter(\n",
        "                                                  x='instrumentalness',\n",
        "                                                  y='loudness',\n",
        "                                                  c='popularity',\n",
        "                                                  colormap='viridis')\n",
        "\n",
        "ax2 = df[df['artist_avg_popularity'] > 60].plot.scatter(\n",
        "                                                  x='instrumentalness',\n",
        "                                                  y='loudness',\n",
        "                                                  c='popularity',\n",
        "                                                  colormap='viridis')\n",
        "\n",
        "ax3 = df[df['artist_avg_popularity'] > 30].plot.scatter(\n",
        "                                                  x='instrumentalness',\n",
        "                                                  y='loudness',\n",
        "                                                  c='popularity',\n",
        "                                                  colormap='viridis')\n",
        "\n",
        "ax4 = df[df['artist_avg_popularity'] < 10].plot.scatter(\n",
        "                                                  x='instrumentalness',\n",
        "                                                  y='loudness',\n",
        "                                                  c='popularity',\n",
        "                                                  colormap='viridis')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5TtfhncxA1ha",
      "metadata": {
        "id": "5TtfhncxA1ha"
      },
      "source": [
        "**[END]**\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Dk5mHWp3mDKN",
      "metadata": {
        "id": "Dk5mHWp3mDKN"
      },
      "source": [
        "##**2.4** Time-Respective Temporal Patterns and Trends"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IKXSQfMGotwJ",
      "metadata": {
        "id": "IKXSQfMGotwJ"
      },
      "source": [
        "This section explores how key popularity signals and metadata features evolve over time using the enriched `temporal_df`, which captures time-aware statistics such as `artist_avg_popularity`, `genre_avg_popularity`, and `year_avg_popularity`. A `year_decimal` column was introduced to emulate continuous release patterns, enabling smoother and more realistic trend analysis.\n",
        "\n",
        "The plots in this section illustrate long-term shifts in popularity, artist influence, and output volume, using only information available up to each song's release. This allows us to observe meaningful patterns while preserving causal integrity — forming a basis for cleaner temporal modelling and interpretation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Lc45BHZbpoHv",
      "metadata": {
        "id": "Lc45BHZbpoHv"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***2.4.1*** - Running Average Popularity Over Time"
      ],
      "metadata": {
        "id": "lG9oVj5fJqmb"
      },
      "id": "lG9oVj5fJqmb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This plot displays the global running average of song popularity over time, smoothed using a 500-entry rolling window. The clear upward trajectory indicates a general rise in average popularity values across the dataset, potentially reflecting increased catalogue optimisation, algorithmic promotion, or shifts in streaming-era user behaviour."
      ],
      "metadata": {
        "id": "ZKp0YjhtJrpV"
      },
      "id": "ZKp0YjhtJrpV"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(data=temporal_df.sort_values(by='year_decimal').rolling(window=500).mean(), x='year_decimal', y='running_avg_popularity')\n",
        "plt.title('Running Average Popularity Over Time (Global)')\n",
        "plt.xlabel('Year (Continuous)')\n",
        "plt.ylabel('Running Average Popularity')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WjK9zQ8xJryB"
      },
      "id": "WjK9zQ8xJryB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ],
      "metadata": {
        "id": "krzxX1irJvsD"
      },
      "id": "krzxX1irJvsD"
    },
    {
      "cell_type": "markdown",
      "id": "fr5d2f_Npid2",
      "metadata": {
        "id": "fr5d2f_Npid2"
      },
      "source": [
        "###***2.4.2*** - Average Artist Popularity Over Time"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XIFd5j-5prVo",
      "metadata": {
        "id": "XIFd5j-5prVo"
      },
      "source": [
        "This plot shows the running average of `artist_avg_popularity` over time, using a 500-entry rolling mean to smooth fluctuations. The steady upward trend highlights how artist momentum has increased across the streaming era, suggesting that newer releases benefit more consistently from prior success than in earlier years.\n",
        "\n",
        "Additionally, dips appear at the start of each year - suggesting that some sort of system reset occurs each year that Spotify may utilise to regulate the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_8sAi_hzpmBo",
      "metadata": {
        "id": "_8sAi_hzpmBo"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(data=temporal_df.sort_values(by='year_decimal').rolling(window=500).mean(), x='year_decimal', y='artist_avg_popularity')\n",
        "plt.title('Running Artist Average Popularity Over Time')\n",
        "plt.xlabel('Year (continuous)')\n",
        "plt.ylabel('Artist Average Popularity (at time of release)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nHWgH307pqCE",
      "metadata": {
        "id": "nHWgH307pqCE"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***2.4.3*** -Average Genre Popularity Over Time"
      ],
      "metadata": {
        "id": "A_MT-p0IIhLs"
      },
      "id": "A_MT-p0IIhLs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This plot tracks the rolling average of `genre_avg_popularity` over time, based on each song's genre context at release. The long-term upward trend suggests that genre affiliation is increasingly predictive of popularity — potentially reflecting the platform-driven rise of trend-dominant genres and algorithmic reinforcement of high-performing categories."
      ],
      "metadata": {
        "id": "4OCH65NMIhmn"
      },
      "id": "4OCH65NMIhmn"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(data=temporal_df.sort_values(by='year_decimal').rolling(window=500).mean(), x='year_decimal', y='genre_avg_popularity')\n",
        "plt.title('Running Genre Average Popularity Over Time')\n",
        "plt.xlabel('Year (continuous)')\n",
        "plt.ylabel('Genre Average Popularity (at time of release)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4szPAgN0IiPh"
      },
      "id": "4szPAgN0IiPh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ],
      "metadata": {
        "id": "6KjBKpyNI3yd"
      },
      "id": "6KjBKpyNI3yd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***2.4.4*** -Average Year Popularity Over Time"
      ],
      "metadata": {
        "id": "MXJ9AO58I-4H"
      },
      "id": "MXJ9AO58I-4H"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This graph visualises the time-respective `year_avg_popularity`, capturing how the average popularity within each release year has evolved over time. The stepped pattern reflects our yearly aggregation logic, with sharp dips and peaks where the mean is unstable at the start of each year, while the overall upward trend suggests that more recent years have consistently higher baseline popularity — potentially driven by streaming platform bias, catalogue expansion, and changing user engagement metrics."
      ],
      "metadata": {
        "id": "E8rN7FE8JCSy"
      },
      "id": "E8rN7FE8JCSy"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(data=temporal_df.sort_values(by='year_decimal').rolling(window=500).mean(), x='year_decimal', y='year_avg_popularity')\n",
        "plt.title('Running Year Average Popularity Over Time')\n",
        "plt.xlabel('Year (continuous)')\n",
        "plt.ylabel('Year Average Popularity (at time of release)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kND0PrrjJCcV"
      },
      "id": "kND0PrrjJCcV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ],
      "metadata": {
        "id": "VCmbdqIvJPJb"
      },
      "id": "VCmbdqIvJPJb"
    },
    {
      "cell_type": "markdown",
      "id": "JHakntTIpxkB",
      "metadata": {
        "id": "JHakntTIpxkB"
      },
      "source": [
        "###***2.4.5*** - Distribution of Song Count Over Time"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Rh3amxXSp0wn",
      "metadata": {
        "id": "Rh3amxXSp0wn"
      },
      "source": [
        "This histogram illustrates the yearly distribution of songs within the dataset from 2000 to 2022. The number of releases increases steadily over time, reflecting platform growth and digital accessibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LzX8xecVxvHb",
      "metadata": {
        "id": "LzX8xecVxvHb"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(temporal_df['year'], bins=23)\n",
        "plt.title('Distribution of Songs Released per Year')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Number of Songs')\n",
        "plt.xticks(range(2000, 2023), rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2CvTb40WzSG1",
      "metadata": {
        "id": "2CvTb40WzSG1"
      },
      "source": [
        "**[END]**\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbjoatMDXpo2",
      "metadata": {
        "id": "dbjoatMDXpo2"
      },
      "source": [
        "#**Section 3** - Data Analysis using Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9tN00f6TzGv",
      "metadata": {
        "id": "e9tN00f6TzGv"
      },
      "source": [
        "This section begins the core modelling pipeline. Using the fully preprocessed and normalised dataset `train_df`, we now attempt to predict song `popularity` using a range of regression-based approaches. The goal is to determine how well various models can capture the complex, nonlinear relationships embedded in the music features - from structural audio elements like `loudness` and `danceability`, to engineered metadata like `artist_avg_popularity`.\n",
        "\n",
        "Rather than focusing on theoretical benchmarking, this section prioritises interpretability, simplicity, and a realistic appraisal of model behaviour in the context of real-world music data. We begin with linear regression as a baseline, then expand into ensemble and deep learning methods later in **Section 4**. Throughout, we maintain consistent evaluation metrics for fair comparison, with additional attention given to feature importance and residual behaviour."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fL2D52i5_uPT",
      "metadata": {
        "id": "fL2D52i5_uPT"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BOor6n932Tfo",
      "metadata": {
        "id": "BOor6n932Tfo"
      },
      "source": [
        "##**3.1** Instantiation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4uC8p630N6aj",
      "metadata": {
        "id": "4uC8p630N6aj"
      },
      "source": [
        "###***3.1.1*** - Instantiating new DataFrame *reg_df*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "g_ddmDPAUCjl",
      "metadata": {
        "id": "g_ddmDPAUCjl"
      },
      "outputs": [],
      "source": [
        "reg_df = train_df.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "u0n1Fg1OfdG2",
      "metadata": {
        "id": "u0n1Fg1OfdG2"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sqX_nctL2bpj",
      "metadata": {
        "id": "sqX_nctL2bpj"
      },
      "source": [
        "##**3.2** Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2Vj9eXwKYABS",
      "metadata": {
        "id": "2Vj9eXwKYABS"
      },
      "source": [
        "This section introduces the first baseline model used in the project: standard linear regression. Linear regression provides a transparent and interpretable starting point for understanding how the selected features relate to song popularity.\n",
        "\n",
        "The model is trained on the normalised `reg_df` dataset, where `popularity` is the target and all other features serve as inputs. After performing an 80/20 train-test split, the `LinearRegression` model is fitted and evaluated using both the R² score and a custom average prediction accuracy metric.\n",
        "\n",
        "The resulting R² score of ~**76.2%** suggests a moderately strong linear relationship between features and popularity. Coefficient analysis reveals the weight (β) assigned to each feature, helping to interpret which variables have the most influence. However the custom accuracy measure (based on proportional error) yields ~**63.02%**, displaying very poor actual predictive power.\n",
        "\n",
        "Although limited by its linear assumptions, this model establishes a performance baseline. It offers a foundation for comparison with more advanced ensemble and deep learning models explored in later sections."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ni1fg1WAYAMh",
      "metadata": {
        "id": "Ni1fg1WAYAMh"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EPKfVQIIEN3-",
      "metadata": {
        "id": "EPKfVQIIEN3-"
      },
      "source": [
        "###***3.2.1*** - Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21PcbKiY9fyD",
      "metadata": {
        "id": "21PcbKiY9fyD"
      },
      "source": [
        "This sets up a basic linear regression model using the normalised `reg_df` dataset. The `popularity` column is assigned as the target variable, and all other features are treated as inputs. The data is split into the training and testing sets (80/20 split), and a `LinearRegression` model is fitted to the training data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RxZ0Qu7zEPsW",
      "metadata": {
        "id": "RxZ0Qu7zEPsW"
      },
      "outputs": [],
      "source": [
        "#assigning target and independant variables\n",
        "target = 'popularity'\n",
        "\n",
        "\n",
        "y = reg_df.loc[:, 'popularity']\n",
        "X = reg_df.drop(labels='popularity', axis=1)\n",
        "\n",
        "# Splitting to training and testing data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "#Initiate a regression\n",
        "linearRegressor = LinearRegression()\n",
        "#trains the regressor\n",
        "linearRegressor.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "p7PJBAnB-kfB",
      "metadata": {
        "id": "p7PJBAnB-kfB"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ckv6fYRbGOtJ",
      "metadata": {
        "id": "ckv6fYRbGOtJ"
      },
      "source": [
        "###***3.2.2*** - Coefficient Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DFayBTYyq3hK",
      "metadata": {
        "id": "DFayBTYyq3hK"
      },
      "source": [
        "This step outputs the learned linear regression coefficients for each input feature. Each value represents the weight (β) assigned to a feature, indicating its influence on the predicted popularity. Higher absolute values suggest stronger impact, with positive or negative signs showing direction of effect."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NxwR8tlKGLuW",
      "metadata": {
        "id": "NxwR8tlKGLuW"
      },
      "outputs": [],
      "source": [
        "#outputs the coefficients against each independant variable\n",
        "display(pd.DataFrame(linearRegressor.coef_, X.columns, columns = ['Coefficient(β)']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FT3YMzlO-ibz",
      "metadata": {
        "id": "FT3YMzlO-ibz"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WMQaGirZGfSy",
      "metadata": {
        "id": "WMQaGirZGfSy"
      },
      "source": [
        "###***3.2.3*** - r² Prediction Score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bT3vPUSU96iy",
      "metadata": {
        "id": "bT3vPUSU96iy"
      },
      "source": [
        "The R² score is calculated on the test set to evaluate how well the linear regression model explains variance in popularity. The resulting prediction accuracy is ~**76.2%**, indicating a moderately strong linear fit between the features and target variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PhOeZ_ILWXM4",
      "metadata": {
        "id": "PhOeZ_ILWXM4"
      },
      "outputs": [],
      "source": [
        "r2_score = linearRegressor.score(X_test,y_test)\n",
        "print('\\n\\nr2 predicted accuracy: ',round(r2_score*100,2),'%\\n\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LMcCOMtB-f5A",
      "metadata": {
        "id": "LMcCOMtB-f5A"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1I9NIJZlueud",
      "metadata": {
        "id": "1I9NIJZlueud"
      },
      "source": [
        "###***3.2.4*** - Predictive Score (Calculated from the Dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5gu_grap-a1b",
      "metadata": {
        "id": "5gu_grap-a1b"
      },
      "source": [
        "To further assess the model's predictive accuracy, each test prediction is compared to its true value. Linear Regression has a relatively poor actual performance of ~**63.02%** accuracy. Shown is a scatter plot of predictions vs. actual values, with the diagonal line `x = y` representing perfect prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IJeszPI9yjlw",
      "metadata": {
        "id": "IJeszPI9yjlw"
      },
      "outputs": [],
      "source": [
        "#Calculating predictions from test and training\n",
        "predictions = linearRegressor.predict(X_test)\n",
        "\n",
        "#Adding an array of the Actual Values\n",
        "observations = y_test.to_numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aO1a1FTWpaJV",
      "metadata": {
        "id": "aO1a1FTWpaJV"
      },
      "outputs": [],
      "source": [
        "for x in range(len(predictions)):\n",
        "  predictions[x] = predictions[x] * 100\n",
        "\n",
        "for x in range(len(observations)):\n",
        "  observations[x] = observations[x] * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7baFCOItQfTi",
      "metadata": {
        "id": "7baFCOItQfTi"
      },
      "outputs": [],
      "source": [
        "pred_average = float()\n",
        "sum_preds = 0\n",
        "\n",
        "for x in range(len(observations)):\n",
        "\n",
        "  if predictions[x] < 0:\n",
        "    predictions[x] = 0\n",
        "  if predictions[x] > 100:\n",
        "    prediction[x] = 100\n",
        "\n",
        "\n",
        "  prediction = predictions[x]\n",
        "  observation = observations[x]\n",
        "\n",
        "\n",
        "  if prediction == 0 and observation == 0:\n",
        "    pred_average = 0\n",
        "\n",
        "  elif prediction == 0:\n",
        "    pred_average = (float((prediction/observation)*100))\n",
        "\n",
        "  elif observation == 0:\n",
        "    pred_average = (float((observation/prediction)*100))\n",
        "\n",
        "  elif prediction <= observation:\n",
        "    pred_average = (float((prediction/observation)*100))\n",
        "\n",
        "  elif prediction > observation:\n",
        "    pred_average = (float((observation/prediction)*100))\n",
        "\n",
        "  sum_preds += pred_average\n",
        "\n",
        "predAvg = sum_preds/(len(observations))\n",
        "\n",
        "print(\"\\n\\n\\nAverage prediction accuracy: \"+str(round(predAvg,2))+\"%\\n\\n\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#plot actual values against predicted values\n",
        "plt.plot(observations, predictions, '+')\n",
        "plt.xlabel('Observation (o)')\n",
        "plt.ylabel('Prediction (p)')\n",
        "\n",
        "# plot a line x = y between 0 and 100\n",
        "x = np.linspace(0, 100, 100)\n",
        "y = x\n",
        "plt.plot(x, y)\n",
        "\n",
        "# --- references ---\n",
        "#https://seaborn.pydata.org/generated/seaborn.scatterplot.html"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xs9E-Ti70eUl",
      "metadata": {
        "id": "xs9E-Ti70eUl"
      },
      "source": [
        "**[END]**\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jvLzP3fqpyVz",
      "metadata": {
        "id": "jvLzP3fqpyVz"
      },
      "source": [
        "##**3.3** XGBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KtqdwFtQnXmn",
      "metadata": {
        "id": "KtqdwFtQnXmn"
      },
      "source": [
        "This section introduces the first ensemble model used in the project - XGBoost. XGBoost is a popular implementation of gradient-boosted decision trees that is well-suited to handling structured data like this. It incrementally builds a strong predictor by combining the outputs of many shallow trees, each correcting the errors of the one before.\n",
        "\n",
        "The parameters used (n_estimators=300, max_depth=6, learning_rate=0.05, etc.) are chosen to strike a balance between accuracy and overfitting. The model is trained using the same preprocessed dataset (reg_df) as before, and evaluated using R² and a custom prediction accuracy score.\n",
        "\n",
        "Despite its complexity, the model remains interpretable through feature importance and residual evaluation. This model sets a performance benchmark that can be compared against both simpler linear methods and more flexible deep learning approaches."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gi5cstlg_sMw",
      "metadata": {
        "id": "gi5cstlg_sMw"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bquxpr8qqGqW",
      "metadata": {
        "id": "bquxpr8qqGqW"
      },
      "source": [
        "###***3.3.1*** - Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6wMmGaya_mRw",
      "metadata": {
        "id": "6wMmGaya_mRw"
      },
      "source": [
        "This sets up an XGBoost regression model using the normalised `reg_df` dataset. `popularity` is assigned as the target variable, with all other features used as inputs. After splitting into training and testing sets (80/20 split), the `XGBRegressor` is configured with 300 trees, controlled depth, and subsampling to avoid overfitting, then trained on the training data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wFP6VT45qE2A",
      "metadata": {
        "id": "wFP6VT45qE2A"
      },
      "outputs": [],
      "source": [
        "#assigning target and independant variables\n",
        "target = 'popularity'\n",
        "\n",
        "y = reg_df.loc[:, target]\n",
        "X = reg_df.drop(labels=target, axis=1)\n",
        "\n",
        "# Splitting to training and testing data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "#Initiate an XGBoost regressor\n",
        "xgbRegressor = xgb.XGBRegressor(\n",
        "      objective='reg:squarederror',\n",
        "      n_estimators=300,         # more trees = more capacity\n",
        "      max_depth=6,              # controls model complexity\n",
        "      learning_rate=0.05,       # lower = slower but more accurate learning\n",
        "      subsample=0.8,            # prevents overfitting\n",
        "      colsample_bytree=0.8,     # use only subset of features per tree\n",
        "      n_jobs=-1                 # use all CPU cores\n",
        ")\n",
        "\n",
        "#trains the regressor\n",
        "xgbRegressor.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "663i5uWP_p7Q",
      "metadata": {
        "id": "663i5uWP_p7Q"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nXL5MAuz1UAy",
      "metadata": {
        "id": "nXL5MAuz1UAy"
      },
      "source": [
        "###***3.3.2*** - Feature Importance Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e88wGLVm1YFB",
      "metadata": {
        "id": "e88wGLVm1YFB"
      },
      "source": [
        "This step outputs the feature importances from the trained XGBoost model. Unlike linear regression coefficients, these values reflect how often and how effectively each feature is used in the decision trees to reduce prediction error. Higher values indicate stronger influence on the predicted popularity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OQZAvrm0xGBz",
      "metadata": {
        "id": "OQZAvrm0xGBz"
      },
      "outputs": [],
      "source": [
        "# Get feature importances\n",
        "importances = xgbRegressor.feature_importances_\n",
        "\n",
        "# Create a DataFrame to display feature importances\n",
        "display(pd.DataFrame(importances, X.columns, columns=['Feature Importance']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4YVqKzjP1SNo",
      "metadata": {
        "id": "4YVqKzjP1SNo"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oQhk2yB1qwM9",
      "metadata": {
        "id": "oQhk2yB1qwM9"
      },
      "source": [
        "###***3.3.3*** - r² Prediction Score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kVrZ3UkGAM8P",
      "metadata": {
        "id": "kVrZ3UkGAM8P"
      },
      "source": [
        "The R² score is calculated on the test set to evaluate how well the XGBoost model explains variance in popularity. The resulting prediction accuracy is ~**79.76%**, indicating a much stronger fit and improved performance over linear regression.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "k7GEzSseqwSs",
      "metadata": {
        "id": "k7GEzSseqwSs"
      },
      "outputs": [],
      "source": [
        "r2_score = xgbRegressor.score(X_test,y_test)\n",
        "print('\\n\\nr2 predicted accuracy: ',round(r2_score*100,2),'%\\n\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FAiDeu-4ANqQ",
      "metadata": {
        "id": "FAiDeu-4ANqQ"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zTh26CcaJIg7",
      "metadata": {
        "id": "zTh26CcaJIg7"
      },
      "source": [
        "###***3.3.4*** - Predictive Score (Calculated from the Dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "poz1yAmNAnQq",
      "metadata": {
        "id": "poz1yAmNAnQq"
      },
      "source": [
        "To further assess the model's predictive accuracy, each test prediction is compared to its true value. XGBoost has a moderate actual performance of ~**64.86%** accuracy. Shown is a scatter plot of predictions vs. actual values, with the diagonal line `x = y` representing perfect prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KtiiCAbFq1YE",
      "metadata": {
        "id": "KtiiCAbFq1YE"
      },
      "outputs": [],
      "source": [
        "#Calculating predictions from test and training\n",
        "predictions = xgbRegressor.predict(X_test)\n",
        "\n",
        "#Adding an array of the Actual Values\n",
        "observations = y_test.to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8r3WzQfcrAah",
      "metadata": {
        "id": "8r3WzQfcrAah"
      },
      "outputs": [],
      "source": [
        "for x in range(len(predictions)):\n",
        "  predictions[x] = predictions[x] * 100\n",
        "\n",
        "for x in range(len(observations)):\n",
        "  observations[x] = observations[x] * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2LSoCxoLrEmE",
      "metadata": {
        "id": "2LSoCxoLrEmE"
      },
      "outputs": [],
      "source": [
        "pred_average = float()\n",
        "sum_preds = 0\n",
        "\n",
        "for x in range(len(observations)):\n",
        "\n",
        "  if predictions[x] < 0:\n",
        "    predictions[x] = 0\n",
        "  if predictions[x] > 100:\n",
        "    prediction[x] = 100\n",
        "\n",
        "\n",
        "  prediction = predictions[x]\n",
        "  observation = observations[x]\n",
        "\n",
        "\n",
        "  if prediction == 0 and observation == 0:\n",
        "    pred_average = 0\n",
        "\n",
        "  elif prediction == 0:\n",
        "    pred_average = (float((prediction/observation)*100))\n",
        "\n",
        "  elif observation == 0:\n",
        "    pred_average = (float((observation/prediction)*100))\n",
        "\n",
        "  elif prediction <= observation:\n",
        "    pred_average = (float((prediction/observation)*100))\n",
        "\n",
        "  elif prediction > observation:\n",
        "    pred_average = (float((observation/prediction)*100))\n",
        "\n",
        "  sum_preds += pred_average\n",
        "\n",
        "predAvg = sum_preds/(len(observations))\n",
        "\n",
        "print(\"\\n\\n\\nAverage prediction accuracy: \"+str(round(predAvg,2))+\"%\\n\\n\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#plot actual values against predicted values\n",
        "plt.plot(observations, predictions, '+')\n",
        "plt.xlabel('Observation (o)')\n",
        "plt.ylabel('Prediction (p)')\n",
        "\n",
        "# plot a line x = y between 0 and 100\n",
        "x = np.linspace(0, 100, 100)\n",
        "y = x\n",
        "plt.plot(x, y)\n",
        "\n",
        "# --- references ---\n",
        "#https://seaborn.pydata.org/generated/seaborn.scatterplot.html"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fGUeMgFdAt7o",
      "metadata": {
        "id": "fGUeMgFdAt7o"
      },
      "source": [
        "**[END]**\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0usxYpmTaTlO",
      "metadata": {
        "id": "0usxYpmTaTlO"
      },
      "source": [
        "##**3.4** Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aCQgm-gvA9OA",
      "metadata": {
        "id": "aCQgm-gvA9OA"
      },
      "source": [
        "This section applies a Random Forest Regressor, the first bagging-based ensemble model in the project. Random Forests combine the predictions of multiple decision trees trained on random subsets of both the data and features. This ensemble approach helps reduce variance and avoid overfitting, making it well-suited for noisy, structured datasets like this one.\n",
        "\n",
        "The model is trained on the normalised `reg_df` dataset, with `popularity` as the target variable. Using 60 estimators and full CPU parallelism, the regressor captures complex feature interactions while maintaining strong generalisation. Performance is evaluated using both the R² score and the custom prediction accuracy metric.\n",
        "\n",
        "With an R² score of ~**81.91%** and an average proportional accuracy of ~**66.21%**, this model currently offers the strongest performance so far. Feature importances are extracted to understand which variables were most influential in predicting song popularity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TizFv4-aA9WV",
      "metadata": {
        "id": "TizFv4-aA9WV"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cuLGRbXTalV5",
      "metadata": {
        "id": "cuLGRbXTalV5"
      },
      "source": [
        "###***3.4.1*** - Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "J2MrPRgLBNMj",
      "metadata": {
        "id": "J2MrPRgLBNMj"
      },
      "source": [
        "This sets up a Random Forest regression model using the normalised `reg_df` dataset. The target variable is `popularity`, with all remaining features treated as inputs. After an 80/20 train-test split, the `RandomForestRegressor` is configured with 60 trees and parallel processing enabled. The model is then trained on the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "i9jJ9q1lalbx",
      "metadata": {
        "id": "i9jJ9q1lalbx"
      },
      "outputs": [],
      "source": [
        "#assigning target and independant variables\n",
        "target = 'popularity'\n",
        "\n",
        "y = reg_df.loc[:, target]\n",
        "X = reg_df.drop(labels=target, axis=1)\n",
        "\n",
        "# Splitting to training and testing data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "#Initiate a Random Forest Regressor\n",
        "rfRegressor = RandomForestRegressor(n_estimators=60, random_state=0, n_jobs=-1)\n",
        "\n",
        "#train the regressor\n",
        "rfRegressor.fit(X_train, y_train)\n",
        "\n",
        "# about +5m 20s compute time per 30 estimators"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NYcEClG8A7Ph",
      "metadata": {
        "id": "NYcEClG8A7Ph"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Af1lZ02ahknv",
      "metadata": {
        "id": "Af1lZ02ahknv"
      },
      "source": [
        "###***3.4.2*** - Feature Importance Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DCrlMvL7hksv",
      "metadata": {
        "id": "DCrlMvL7hksv"
      },
      "source": [
        "This step outputs the feature importances from the trained Random Forest model. Unlike linear regression coefficients, these values reflect how often and how effectively each feature is used in the decision trees to reduce prediction error. Higher values indicate stronger influence on the predicted popularity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q5mhImQChcv3",
      "metadata": {
        "id": "Q5mhImQChcv3"
      },
      "outputs": [],
      "source": [
        "# Get feature importances instead of coefficients\n",
        "importances = rfRegressor.feature_importances_\n",
        "\n",
        "# Create a DataFrame to display feature importances\n",
        "display(pd.DataFrame(importances, X.columns, columns=['Feature Importance']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rARqQ9A0BbSS",
      "metadata": {
        "id": "rARqQ9A0BbSS"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4kmHdDZkh412",
      "metadata": {
        "id": "4kmHdDZkh412"
      },
      "source": [
        "###***3.4.3*** - r² Prediction Score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-VlZ3SjUCC-L",
      "metadata": {
        "id": "-VlZ3SjUCC-L"
      },
      "source": [
        "The R² score is calculated on the test set to evaluate how well the Random Forest model explains variance in popularity. The resulting prediction accuracy is ~**81.91%**, indicating a strong fit and the highest performance so far, surpassing both the linear and XGBoost regressors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u5fY9o__h4_N",
      "metadata": {
        "id": "u5fY9o__h4_N"
      },
      "outputs": [],
      "source": [
        "r2_score = rfRegressor.score(X_test,y_test)\n",
        "print('\\n\\nr2 predicted accuracy: ',round(r2_score*100,2),'%\\n\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "w-s-bRPyCgG7",
      "metadata": {
        "id": "w-s-bRPyCgG7"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZoxUOt_YiXSN",
      "metadata": {
        "id": "ZoxUOt_YiXSN"
      },
      "source": [
        "###***3.4.4*** - Predictive Score (Calculated from the Dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "T5qcuX0JCmjF",
      "metadata": {
        "id": "T5qcuX0JCmjF"
      },
      "source": [
        "To further assess the model's predictive accuracy, each test prediction is compared to its true value. Random Forest has a moderate actual performance of ~**66.21%** accuracy. Shown is a scatter plot of predictions vs. actual values, with the diagonal line `x = y` representing perfect prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GtwPvskucy1J",
      "metadata": {
        "id": "GtwPvskucy1J"
      },
      "outputs": [],
      "source": [
        "#Calculating predictions from test and training\n",
        "predictions = rfRegressor.predict(X_test)\n",
        "\n",
        "#Adding an array of the Actual Values\n",
        "observations = y_test.to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2FFuwFebagoD",
      "metadata": {
        "id": "2FFuwFebagoD"
      },
      "outputs": [],
      "source": [
        "for x in range(len(predictions)):\n",
        "  predictions[x] = predictions[x] * 100\n",
        "\n",
        "for x in range(len(observations)):\n",
        "  observations[x] = observations[x] * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-WepdpY8dDhB",
      "metadata": {
        "id": "-WepdpY8dDhB"
      },
      "outputs": [],
      "source": [
        "pred_average = float()\n",
        "sum_preds = 0\n",
        "\n",
        "for x in range(len(observations)):\n",
        "\n",
        "  if predictions[x] < 0:\n",
        "    predictions[x] = 0\n",
        "  if predictions[x] > 100:\n",
        "    prediction[x] = 100\n",
        "\n",
        "\n",
        "  prediction = predictions[x]\n",
        "  observation = observations[x]\n",
        "\n",
        "\n",
        "  if prediction == 0 and observation == 0:\n",
        "    pred_average = 0\n",
        "\n",
        "  elif prediction == 0:\n",
        "    pred_average = (float((prediction/observation)*100))\n",
        "\n",
        "  elif observation == 0:\n",
        "    pred_average = (float((observation/prediction)*100))\n",
        "\n",
        "  elif prediction <= observation:\n",
        "    pred_average = (float((prediction/observation)*100))\n",
        "\n",
        "  elif prediction > observation:\n",
        "    pred_average = (float((observation/prediction)*100))\n",
        "\n",
        "  sum_preds += pred_average\n",
        "\n",
        "predAvg = sum_preds/(len(observations))\n",
        "\n",
        "print(\"\\n\\n\\nAverage prediction accuracy: \"+str(round(predAvg,2))+\"%\\n\\n\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#plot actual values against predicted values\n",
        "plt.plot(observations, predictions, '+')\n",
        "plt.xlabel('Observation (o)')\n",
        "plt.ylabel('Prediction (p)')\n",
        "\n",
        "# plot a line x = y between 0 and 100\n",
        "x = np.linspace(0, 100, 100)\n",
        "y = x\n",
        "plt.plot(x, y)\n",
        "\n",
        "# --- references ---\n",
        "#https://seaborn.pydata.org/generated/seaborn.scatterplot.html"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5DKTOdbxCj1x",
      "metadata": {
        "id": "5DKTOdbxCj1x"
      },
      "source": [
        "**[END]**\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Fq8uzJdpXVUw",
      "metadata": {
        "id": "Fq8uzJdpXVUw"
      },
      "source": [
        "#**Section 4** - Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "l79De0hYGzGz",
      "metadata": {
        "id": "l79De0hYGzGz"
      },
      "source": [
        "This section introduces the deep learning phase of the project. Building on the fully preprocessed and normalised `train_df`, we construct, train, and evaluate a neural network for predicting song popularity. This model represents the most flexible and expressive approach used so far, designed to uncover non-linear and higher-order interactions among features that may not be captured by simpler regression or ensemble methods.\n",
        "\n",
        "The architecture is built using TensorFlow's `Sequential` API. It includes three double hidden dense layers (256, 128, and 64 units), each using ReLU activation and dropout (rate = 0.2) between each pair to prevent overfitting. A single linear output node is used to predict the final popularity score, allowing for unconstrained regression output. The model is compiled using the RMSprop optimiser and trained using MSE loss.\n",
        "\n",
        "This neural model establishes the most advanced benchmark in the project. While less interpretable than linear or tree-based models, it reveals more subtle patterns in the data and offers the strongest potential for future prediction accuracy improvements in music analytics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8YPNPUrXwfKm",
      "metadata": {
        "id": "8YPNPUrXwfKm"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Concatenate, Activation, Lambda"
      ],
      "metadata": {
        "id": "oQE0mFQ1z4FG"
      },
      "id": "oQE0mFQ1z4FG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ],
      "metadata": {
        "id": "TCUMNACmz5Hi"
      },
      "id": "TCUMNACmz5Hi"
    },
    {
      "cell_type": "markdown",
      "id": "VZePH2Cvjgmk",
      "metadata": {
        "id": "VZePH2Cvjgmk"
      },
      "source": [
        "##**4.1**  Training *model*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f-YyjI8pG_uy",
      "metadata": {
        "id": "f-YyjI8pG_uy"
      },
      "source": [
        "###***4.1.1*** - Fabricating the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ewEj5eKvIBha",
      "metadata": {
        "id": "ewEj5eKvIBha"
      },
      "source": [
        "This subsection defines a deep neural network using TensorFlow's `Sequential` API. The input layer expects 114 features from the one-hot encoded and normalised `train_df`. The model includes 6 hidden layers (with 2x 256, 2x 128, and 2x 64 neurons respectively), each using ReLU activation and dropout regularisation (rate = 0.2) to prevent overfitting. The final output layer uses a linear activation to predict the continuous popularity score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7645a23-7224-40be-b30c-b9110cb5f36a",
      "metadata": {
        "id": "a7645a23-7224-40be-b30c-b9110cb5f36a"
      },
      "outputs": [],
      "source": [
        "X = train_df.drop(labels='popularity', axis=1)\n",
        "\n",
        "y = train_df['popularity']\n",
        "\n",
        "\n",
        "#Train and Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, shuffle=True, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ],
      "metadata": {
        "id": "7u2kB47B0BDz"
      },
      "id": "7u2kB47B0BDz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining Model Architecture:"
      ],
      "metadata": {
        "id": "lhEntERf0Ehg"
      },
      "id": "lhEntERf0Ehg"
    },
    {
      "cell_type": "code",
      "source": [
        "excluded_cols = ['artist_avg_popularity', 'genre_avg_popularity', 'year_avg_popularity',\n",
        "                 'artist_song_count', 'genre_song_count', 'year_song_count']\n",
        "\n",
        "# Define column groups\n",
        "one_hot_cols = [col for col in train_df.columns if col.startswith(('genre_', 'year_', 'major', 'minor', 'keyOf', 'time_signature'))\n",
        "                and col not in excluded_cols]\n",
        "\n",
        "continuous_cols = [col for col in train_df.columns if col not in one_hot_cols + ['popularity']]"
      ],
      "metadata": {
        "id": "CE6z9qNUzjMe"
      },
      "id": "CE6z9qNUzjMe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find slice bounds\n",
        "one_hot_start = train_df.columns.get_loc(one_hot_cols[0])\n",
        "one_hot_end = train_df.columns.get_loc(one_hot_cols[-1]) + 1\n",
        "\n",
        "cont_start = train_df.columns.get_loc(continuous_cols[0])\n",
        "cont_end = train_df.columns.get_loc(continuous_cols[-1]) + 1"
      ],
      "metadata": {
        "id": "L-9tZaC5zjUK"
      },
      "id": "L-9tZaC5zjUK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL ARCHITECTURE\n",
        "\n",
        "# Input layer (all features)\n",
        "input_layer = Input(shape=(114,))\n",
        "\n",
        "# Lambda slices for one-hot and continuous blocks\n",
        "one_hot_input = Lambda(lambda x: x[:, one_hot_start:one_hot_end])(input_layer)\n",
        "continuous_input = Lambda(lambda x: x[:, cont_start:cont_end])(input_layer)\n",
        "\n",
        "# One-hot branch\n",
        "one_hot_branch = Dense(128)(one_hot_input)\n",
        "one_hot_branch = BatchNormalization()(one_hot_branch)\n",
        "one_hot_branch = Activation('relu')(one_hot_branch)\n",
        "one_hot_branch = Dropout(0.1)(one_hot_branch)\n",
        "\n",
        "# Continuous branch\n",
        "cont_branch = Dense(256)(continuous_input)\n",
        "cont_branch = BatchNormalization()(cont_branch)\n",
        "cont_branch = Activation('relu')(cont_branch)\n",
        "cont_branch = Dropout(0.2)(cont_branch)\n",
        "\n",
        "cont_branch = Dense(128)(cont_branch)\n",
        "cont_branch = BatchNormalization()(cont_branch)\n",
        "cont_branch = Activation('relu')(cont_branch)\n",
        "cont_branch = Dropout(0.2)(cont_branch)\n",
        "\n",
        "cont_branch = Dense(64)(cont_branch)\n",
        "cont_branch = BatchNormalization()(cont_branch)\n",
        "cont_branch = Activation('relu')(cont_branch)\n",
        "cont_branch = Dropout(0.2)(cont_branch)\n",
        "\n",
        "# Merge both branches\n",
        "merged = Concatenate()([one_hot_branch, cont_branch])\n",
        "merged = Dense(64)(merged)\n",
        "merged = BatchNormalization()(merged)\n",
        "merged = Activation('relu')(merged)\n",
        "merged = Dropout(0.2)(merged)\n",
        "\n",
        "# Final output\n",
        "output = Dense(1, activation='linear')(merged)\n",
        "\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output)\n"
      ],
      "metadata": {
        "id": "zj5a2Jyfo71G"
      },
      "id": "zj5a2Jyfo71G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "BFCH3tMpHta9",
      "metadata": {
        "id": "BFCH3tMpHta9"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4-hSfPavkTfE",
      "metadata": {
        "id": "4-hSfPavkTfE"
      },
      "source": [
        "###***4.1.2*** - Compilation, Optimisation and Loss Function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lzy-LZriIZWY",
      "metadata": {
        "id": "lzy-LZriIZWY"
      },
      "source": [
        "The model is compiled using the RMSprop optimiser and mean squared error (MSE) as the loss function. RMSprop is well-suited for handling noisy or sparse data, while MSE is appropriate for continuous regression tasks like predicting popularity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NL4C9N9N8_2C",
      "metadata": {
        "id": "NL4C9N9N8_2C"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='rmsprop',loss='mean_squared_error')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ur37FER5IVoz",
      "metadata": {
        "id": "ur37FER5IVoz"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kg2XIByFkdoc",
      "metadata": {
        "id": "kg2XIByFkdoc"
      },
      "source": [
        "###***4.1.3*** - Training from the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tYFzizMWI8H0",
      "metadata": {
        "id": "tYFzizMWI8H0"
      },
      "source": [
        "The model is trained for 30 epochs with a batch size of 96, using 20% of the training data for validation. Training stabilises quickly, with validation loss converging around **0.0053**. After training, the model is evaluated on the test set, yielding a final test loss of **0.0057**, confirming consistent performance. The trained model is saved for future use as `'model.keras'`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Zs8p4k7l9CI8",
      "metadata": {
        "id": "Zs8p4k7l9CI8"
      },
      "outputs": [],
      "source": [
        "training = model.fit(X_train, y_train, batch_size=96, shuffle=True, validation_split=0.2, epochs=15)\n",
        "model.evaluate(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XX6I1VUsI2AC",
      "metadata": {
        "id": "XX6I1VUsI2AC"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hfhq87TKNEyn",
      "metadata": {
        "id": "hfhq87TKNEyn"
      },
      "outputs": [],
      "source": [
        "model.save('model.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "i9ZVLs2gI3aa",
      "metadata": {
        "id": "i9ZVLs2gI3aa"
      },
      "source": [
        "**[END]**\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JN216zbKldI-",
      "metadata": {
        "id": "JN216zbKldI-"
      },
      "source": [
        "##**4.2**  Analysis of  *model*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57TIxFNolJ5m",
      "metadata": {
        "id": "57TIxFNolJ5m"
      },
      "source": [
        "###***4.2.1*** - Model Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "E_YmtJKOJZAd",
      "metadata": {
        "id": "E_YmtJKOJZAd"
      },
      "source": [
        "This structure provides enough capacity to capture complex patterns while maintaining interpretability and regularisation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LclphtjM_z_v",
      "metadata": {
        "id": "LclphtjM_z_v"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(model, to_file='model_diagram.png', show_shapes=True, show_layer_names=True)"
      ],
      "metadata": {
        "id": "-E2lH5I7_lGf"
      },
      "id": "-E2lH5I7_lGf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "uPF81-oxJNb2",
      "metadata": {
        "id": "uPF81-oxJNb2"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OeiC5w_rKNgg",
      "metadata": {
        "id": "OeiC5w_rKNgg"
      },
      "source": [
        "###***4.2.2*** - Training Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "K4BEwQNRKP1L",
      "metadata": {
        "id": "K4BEwQNRKP1L"
      },
      "source": [
        "This plot shows the training and validation loss curves over the 15 epochs. Both curves steadily decline and closely follow one another, suggesting consistent learning without overfitting. The gap between them remains small, indicating that the model generalises well across both seen and unseen data during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rF2iJQICKP9o",
      "metadata": {
        "id": "rF2iJQICKP9o"
      },
      "outputs": [],
      "source": [
        "# Plot training & validation loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(training.history['loss'], label='Training Loss')\n",
        "plt.plot(training.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yzjpn7QBKfyO",
      "metadata": {
        "id": "yzjpn7QBKfyO"
      },
      "source": [
        "**[END]**\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cPENW6RlCWGz",
      "metadata": {
        "id": "cPENW6RlCWGz"
      },
      "source": [
        "##**4.3** Testing *model*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qvKFXHi8Cq-r",
      "metadata": {
        "id": "qvKFXHi8Cq-r"
      },
      "source": [
        "###***4.3.1*** - Generating Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5xvXn-ufCxo8",
      "metadata": {
        "id": "5xvXn-ufCxo8"
      },
      "source": [
        "This code loads observed values and generates predicted values from the trained model using the test set. Both the predictions and true observations are then descaled by multiplying by 100 to return them to their original 0-100 popularity range."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4mjOTmBaCtZH",
      "metadata": {
        "id": "4mjOTmBaCtZH"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(X_test).flatten() * 100\n",
        "observations = y_test.to_numpy() * 100"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cVikFEtqDNXW",
      "metadata": {
        "id": "cVikFEtqDNXW"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tia9-DxyKhE0",
      "metadata": {
        "id": "tia9-DxyKhE0"
      },
      "source": [
        "###***4.3.2*** - Residual Plot"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lUkXMw6OKjHy",
      "metadata": {
        "id": "lUkXMw6OKjHy"
      },
      "source": [
        "This residual plot visualises the difference between actual popularity and predicted popularity from the deep learning model. Each point represents a test sample, with the **x-axis** showing the true (actual) popularity and the **y-axis** showing the residual (i.e. `Actual - Predicted`). A residual of **0** (highlighted by the red line) indicates a perfect prediction. Points **above** the line represent **under-predictions** (model predicted too low), while points **below** the line indicate **over-predictions** (model predicted too high).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "E4xcMlsfKjRD",
      "metadata": {
        "id": "E4xcMlsfKjRD"
      },
      "outputs": [],
      "source": [
        "# Residuals\n",
        "residuals = observations - predictions\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.scatter(observations, residuals)\n",
        "plt.axhline(0, color='red')\n",
        "plt.title('Residual Plot')\n",
        "plt.xlabel('Actual Popularity')\n",
        "plt.ylabel('Residual (Observed - Predicted)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "auSJIjAXKpzv",
      "metadata": {
        "id": "auSJIjAXKpzv"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8-tOtGuZK0Kt",
      "metadata": {
        "id": "8-tOtGuZK0Kt"
      },
      "source": [
        "###***4.3.3*** - Visualising Distribution Accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8u68zs2iK3HI",
      "metadata": {
        "id": "8u68zs2iK3HI"
      },
      "source": [
        "This plot compares the distribution of predicted popularity values to the true values in the test set. While the overall shape of the predicted distribution closely follows the actual data, the model slightly overestimates in the mid-range, and slightly underestimates in the high-range. The high concentration of low-popularity tracks is preserved, suggesting that the model captures the overall skew in the dataset reasonably well.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mF9UgT05K3Mb",
      "metadata": {
        "id": "mF9UgT05K3Mb"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "sns.kdeplot(observations, label='Observations', fill=True, clip=(0, 100))\n",
        "sns.kdeplot(predictions, label='Predictions', fill=True, clip=(0, 100))\n",
        "plt.title('Distribution of Predicted vs Actual Popularity')\n",
        "plt.xlabel('Popularity')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eCGHam6DeiN4",
      "metadata": {
        "id": "eCGHam6DeiN4"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mOibNzhy1WVG",
      "metadata": {
        "id": "mOibNzhy1WVG"
      },
      "source": [
        "###***4.3.4*** - r² Prediction Score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "B3IuVWuHe-FE",
      "metadata": {
        "id": "B3IuVWuHe-FE"
      },
      "source": [
        "The R² score is calculated on the test set to evaluate how well the deep learning model explains variance in popularity. The resulting prediction accuracy is ~**79.86%**, indicating almost the weakest R² performance in the pipeline, albeit a clear improvement over the linear regressor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cwH5t8KsGDqN",
      "metadata": {
        "id": "cwH5t8KsGDqN"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "r2 = r2_score(observations, predictions)\n",
        "\n",
        "\n",
        "print(\"R-squared predicted accuracy:\", str(round((r2*100),2)) +\"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2VJ0gKJheqgA",
      "metadata": {
        "id": "2VJ0gKJheqgA"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gSicQZOIsB4Y",
      "metadata": {
        "id": "gSicQZOIsB4Y"
      },
      "source": [
        "###***4.3.5*** - *model* Prediction Accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To further assess the model's predictive accuracy, each test prediction is compared to its true value using a custom proportional closeness metric. This gives an average accuracy score of ~**64.86%**, indicating moderate real-world predictive performance.\n",
        "\n",
        "Shown is a scatter plot of predictions vs. actual values, with the diagonal line `x = y` representing perfect prediction. The tighter the clustering around this line, the more accurate the model's outputs — though here we observe some consistent underestimation, especially at higher popularity levels.\n"
      ],
      "metadata": {
        "id": "vJMdU3Q4YpUq"
      },
      "id": "vJMdU3Q4YpUq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3_ioMyDixcaO",
      "metadata": {
        "id": "3_ioMyDixcaO"
      },
      "outputs": [],
      "source": [
        "pred_average = float()\n",
        "sum_preds = 0\n",
        "\n",
        "for x in range(len(observations)):\n",
        "\n",
        "  if predictions[x] < 0:\n",
        "    predictions[x] = 0\n",
        "  if predictions[x] > 100:\n",
        "    prediction[x] = 100\n",
        "\n",
        "\n",
        "  prediction = predictions[x]\n",
        "  observation = observations[x]\n",
        "\n",
        "\n",
        "  if prediction == 0 and observation == 0:\n",
        "    pred_average = 0\n",
        "\n",
        "  elif prediction == 0:\n",
        "    pred_average = (float((prediction/observation)*100))\n",
        "\n",
        "  elif observation == 0:\n",
        "    pred_average = (float((observation/prediction)*100))\n",
        "\n",
        "  elif prediction <= observation:\n",
        "    pred_average = (float((prediction/observation)*100))\n",
        "\n",
        "  elif prediction > observation:\n",
        "    pred_average = (float((observation/prediction)*100))\n",
        "\n",
        "  sum_preds += pred_average\n",
        "\n",
        "predAvg = sum_preds/(len(observations))\n",
        "\n",
        "print(\"\\n\\n\\nAverage prediction accuracy: \"+str(round(predAvg,2))+\"%\\n\\n\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#plot actual values against predicted values\n",
        "plt.plot(observations, predictions, '+')\n",
        "plt.xlabel('Observation (o)')\n",
        "plt.ylabel('Prediction (p)')\n",
        "\n",
        "# plot a line x = y between 0 and 100\n",
        "x = np.linspace(0, 100, 100)\n",
        "y = x\n",
        "plt.plot(x, y)\n",
        "\n",
        "# --- references ---\n",
        "#https://seaborn.pydata.org/generated/seaborn.scatterplot.html"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "s_M31QUlTvNN",
      "metadata": {
        "id": "s_M31QUlTvNN"
      },
      "source": [
        "**[END]**\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "h8pVhzO3t2cP",
      "metadata": {
        "id": "h8pVhzO3t2cP"
      },
      "source": [
        "##**4.4** Prediction Baseline Performance Sweep"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PnN8H_EZuDqF",
      "metadata": {
        "id": "PnN8H_EZuDqF"
      },
      "source": [
        "###***4.4.1*** - Defining Functions and Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IcIvX-MbmDMw",
      "metadata": {
        "id": "IcIvX-MbmDMw"
      },
      "source": [
        "####**Functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2zHkQ8AMmOr2",
      "metadata": {
        "id": "2zHkQ8AMmOr2"
      },
      "source": [
        "Defining One-Hot Column Collapser Function *collapse_one_hot_columns()*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VJnMqLNgmPQP",
      "metadata": {
        "id": "VJnMqLNgmPQP"
      },
      "outputs": [],
      "source": [
        "def collapse_one_hot_columns(df, prefix, new_col_name):\n",
        "    one_hot_cols = []\n",
        "    excluded_cols = ['artist_avg_popularity', 'genre_avg_popularity', 'year_avg_popularity',\n",
        "                     'artist_song_count', 'genre_song_count', 'year_song_count']\n",
        "\n",
        "    col_list = list((df.drop(columns=excluded_cols)).columns)\n",
        "\n",
        "    for col in col_list:\n",
        "        if col.startswith(prefix):\n",
        "            one_hot_cols.append(col)\n",
        "\n",
        "    if len(one_hot_cols) == 0:\n",
        "        return df  # nothing to collapse\n",
        "\n",
        "    max_vals = df[one_hot_cols].idxmax(axis=1)\n",
        "\n",
        "    cleaned_vals = []\n",
        "    for val in max_vals:\n",
        "        cleaned_vals.append(val.replace(prefix, ''))\n",
        "\n",
        "    df[new_col_name] = cleaned_vals\n",
        "\n",
        "    df = df.drop(columns=one_hot_cols)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-TkPELCQmRB9",
      "metadata": {
        "id": "-TkPELCQmRB9"
      },
      "source": [
        "Defining Scaler Functions *scale_inputs()* and *unscale_inputs()*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "csHJaiHdmcUX",
      "metadata": {
        "id": "csHJaiHdmcUX"
      },
      "outputs": [],
      "source": [
        "def scale_inputs(inputs):\n",
        "    scaled = inputs.copy()\n",
        "\n",
        "    # Manual scaling\n",
        "    if 'loudness' in scaled:\n",
        "        scaled['loudness'] = (inputs['loudness'] + 60) / 60\n",
        "\n",
        "    if 'tempo' in scaled:\n",
        "        scaled['tempo'] = (inputs['tempo'] - 50) / (220 - 50)\n",
        "\n",
        "    if 'duration_mins' in scaled:\n",
        "        scaled['duration_mins'] = inputs['duration_mins'] / 20\n",
        "\n",
        "    if 'artist_avg_popularity' in scaled:\n",
        "        scaled['artist_avg_popularity'] = inputs['artist_avg_popularity'] / 100\n",
        "\n",
        "    if 'genre_avg_popularity' in scaled:\n",
        "        scaled['genre_avg_popularity'] = inputs['genre_avg_popularity'] / 100\n",
        "\n",
        "    if 'year_avg_popularity' in scaled:\n",
        "        scaled['year_avg_popularity'] = inputs['year_avg_popularity'] / 100\n",
        "\n",
        "    if 'artist_song_count' in scaled:\n",
        "        scaled['artist_song_count'] = inputs['artist_song_count'] / 3000\n",
        "\n",
        "    if 'genre_song_count' in scaled:\n",
        "        scaled['genre_song_count'] = inputs['genre_song_count'] / 30000\n",
        "\n",
        "    if 'year_song_count' in scaled:\n",
        "        scaled['year_song_count'] = inputs['year_song_count'] / 50000\n",
        "\n",
        "    return scaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DaIKXeqrpuWX",
      "metadata": {
        "id": "DaIKXeqrpuWX"
      },
      "outputs": [],
      "source": [
        "def unscale_inputs(inputs, column_names):\n",
        "    inputs = inputs.copy()\n",
        "\n",
        "    for i in range(len(column_names)):\n",
        "        name = column_names[i]\n",
        "\n",
        "        # Manual unscaling\n",
        "        if name == 'loudness':\n",
        "            inputs[i] = (inputs[i] * 60) - 60\n",
        "\n",
        "        elif name == 'tempo':\n",
        "            inputs[i] = (inputs[i] * (220-50)) + 50\n",
        "\n",
        "        elif name == 'duration_mins':\n",
        "            inputs[i] = inputs[i] * 20\n",
        "\n",
        "        elif name == 'artist_avg_popularity':\n",
        "            inputs[i] = inputs[i] * 100\n",
        "\n",
        "        elif name == 'genre_avg_popularity':\n",
        "            inputs[i] = inputs[i] * 100\n",
        "\n",
        "        elif name == 'year_avg_popularity':\n",
        "            inputs[i] = inputs[i] * 100\n",
        "\n",
        "        elif name == 'artist_song_count':\n",
        "            inputs[i] = inputs[i] * 3000\n",
        "\n",
        "        elif name == 'genre_song_count':\n",
        "            inputs[i] = inputs[i] * 30000\n",
        "\n",
        "        elif name == 'year_song_count':\n",
        "            inputs[i] = inputs[i] * 50000\n",
        "\n",
        "    return inputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fwzKduNnDz1S",
      "metadata": {
        "id": "fwzKduNnDz1S"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BYOAg7stmjej",
      "metadata": {
        "id": "BYOAg7stmjej"
      },
      "source": [
        "Defining Table Display Preparation *prepare_display_df()* Function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EFzjJyHUmj_G",
      "metadata": {
        "id": "EFzjJyHUmj_G"
      },
      "outputs": [],
      "source": [
        "def prepare_display_df(df):\n",
        "    # Make a copy so we don't touch the original\n",
        "    df_disp = df.copy()\n",
        "\n",
        "    # Columns to be dropped later\n",
        "    exclude_columns = [\n",
        "        'genre_song_count',\n",
        "        'genre_avg_popularity',\n",
        "        'year_song_count',\n",
        "        'year_avg_popularity'\n",
        "    ]\n",
        "\n",
        "    # Pull out predicted_popularity if it's there\n",
        "    pred_pop = None\n",
        "    column_list = list(df_disp.columns)\n",
        "    for column_name in column_list:\n",
        "        if column_name == 'predicted_popularity':\n",
        "            pred_pop = df_disp['predicted_popularity']\n",
        "            df_disp = df_disp.drop(columns=['predicted_popularity'])\n",
        "            break\n",
        "\n",
        "    # Collapse one-hot encoded groups into single columns\n",
        "    df_disp = collapse_one_hot_columns(df_disp, 'year_', 'year')\n",
        "    df_disp = collapse_one_hot_columns(df_disp, 'genre_', 'genre')\n",
        "    df_disp = collapse_one_hot_columns(df_disp, 'keyOf', 'key')\n",
        "    df_disp = collapse_one_hot_columns(df_disp, 'time_signature_', 'time_signature')\n",
        "\n",
        "    # Handle mode (major/minor)\n",
        "    mode_columns = list(df_disp.columns)\n",
        "    found_major = False\n",
        "    found_minor = False\n",
        "    for name in mode_columns:\n",
        "        if name == 'major':\n",
        "            found_major = True\n",
        "        elif name == 'minor':\n",
        "            found_minor = True\n",
        "    if found_major and found_minor:\n",
        "        df_disp['mode'] = df_disp[['major', 'minor']].idxmax(axis=1)\n",
        "        df_disp = df_disp.drop(columns=['major', 'minor'])\n",
        "\n",
        "    # Drop any unwanted columns that exist\n",
        "    final_columns = list(df_disp.columns)\n",
        "    for column in exclude_columns:\n",
        "        for existing_column in final_columns:\n",
        "            if column == existing_column:\n",
        "                df_disp = df_disp.drop(columns=[column])\n",
        "                break\n",
        "\n",
        "    # Put predicted_popularity back at the end if we had it earlier\n",
        "    if pred_pop is not None:\n",
        "        df_disp['predicted_popularity'] = pred_pop.round(2)\n",
        "\n",
        "    return df_disp"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ],
      "metadata": {
        "id": "XWSRXbllawb8"
      },
      "id": "XWSRXbllawb8"
    },
    {
      "cell_type": "markdown",
      "id": "C5ixzlgQlXZp",
      "metadata": {
        "id": "C5ixzlgQlXZp"
      },
      "source": [
        "####**Variables**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bo42-kilp6t",
      "metadata": {
        "id": "2bo42-kilp6t"
      },
      "source": [
        "Feature Groups:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dwosqutlh13",
      "metadata": {
        "id": "6dwosqutlh13"
      },
      "outputs": [],
      "source": [
        "cont_columns = [\n",
        "    'danceability', 'energy', 'loudness', 'speechiness', 'acousticness',\n",
        "    'instrumentalness', 'liveness', 'valence', 'tempo', 'duration_mins'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dDEK1yWlh4r",
      "metadata": {
        "id": "5dDEK1yWlh4r"
      },
      "outputs": [],
      "source": [
        "artist_col = 'artist_avg_popularity'\n",
        "year_columns = []\n",
        "genre_columns = []\n",
        "key_columns = []\n",
        "\n",
        "\n",
        "for column in X_train.columns:\n",
        "  if column.startswith('year_'):\n",
        "    year_columns.append(column)\n",
        "  elif column.startswith('genre_'):\n",
        "    genre_columns.append(column)\n",
        "  elif column.startswith('keyOf'):\n",
        "    key_columns.append(column)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KX8If2kQlrR-",
      "metadata": {
        "id": "KX8If2kQlrR-"
      },
      "source": [
        "Scaling:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "h8iz5wIHlh7J",
      "metadata": {
        "id": "h8iz5wIHlh7J"
      },
      "outputs": [],
      "source": [
        "# Scale the continuous input columns from X_train using manual scaling function\n",
        "scaled = scale_inputs(X_train[cont_columns])\n",
        "\n",
        "# Create an empty list to store the index positions of each continuous feature\n",
        "index_list = []\n",
        "\n",
        "# Loop through each column in cont_columns and get its index in X_train\n",
        "for column in cont_columns:\n",
        "    col_index = X_train.columns.get_loc(column)\n",
        "    index_list.append(col_index)\n",
        "\n",
        "#https://pandas.pydata.org/pandas-docs/version/0.25.0/reference/api/pandas.DataFrame.T.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RBuBOC6rlybd",
      "metadata": {
        "id": "RBuBOC6rlybd"
      },
      "outputs": [],
      "source": [
        "scaled.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "p3nFKAQgOQdo",
      "metadata": {
        "id": "p3nFKAQgOQdo"
      },
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TlxqOcqHHqLq",
      "metadata": {
        "id": "TlxqOcqHHqLq"
      },
      "source": [
        "###***4.4.2*** - Producing a Baseline Performance Sweep"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "E2wEI6PhuGQB",
      "metadata": {
        "id": "E2wEI6PhuGQB"
      },
      "source": [
        "This subsection generates a baseline performance sweep by varying `artist_avg_popularity` from 0 to 100 in 0.1 steps. For each value, a random song from the training data is sampled, and its `artist_avg_popularity` is overridden while keeping all other features fixed.\n",
        "\n",
        "The trained model predicts popularity for each modified input, and the result is stored alongside the input features in a new DataFrame. This allows us to observe how the model's predictions scale with increasing artist popularity in isolation.\n",
        "\n",
        "The resulting line plot shows a clear upward trend, verifying that the model strongly relies on `artist_avg_popularity` as a predictive feature. Noise and fluctuation are more visible at lower values, likely due to more unpredictable song-level deviations among less popular artists.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JqanXu9iO8qV",
      "metadata": {
        "id": "JqanXu9iO8qV"
      },
      "outputs": [],
      "source": [
        "rows = []\n",
        "\n",
        "step_size = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B0sWPVIBTVXy",
      "metadata": {
        "id": "B0sWPVIBTVXy"
      },
      "outputs": [],
      "source": [
        "# Loop to sweep through values from 0 to 100 for artist_avg_popularity using the given step size\n",
        "for original_val in tqdm(np.arange(0, 100.1, step_size), desc=\"Predicting... \"):\n",
        "\n",
        "    # Scale artist_avg_popularity to 0-1\n",
        "    scaled_val = original_val / 100.0\n",
        "\n",
        "    # Take a random row from the training set\n",
        "    x = X_train.sample(1).iloc[0].values.copy()\n",
        "\n",
        "    # Override artist_avg_popularity\n",
        "    x[X_train.columns.get_loc(artist_col)] = scaled_val\n",
        "    #print(\"\\n\\nx:\",x.round(2))\n",
        "    # Run prediction\n",
        "    y_pred = model.predict(x.reshape(1, -1), verbose=False)[0][0]\n",
        "    y_pred = np.clip(y_pred * 100, 0, 100)\n",
        "\n",
        "    # Manually unscale all features\n",
        "    x = unscale_inputs(x, X_train.columns)\n",
        "    #print(\"\\n\\nx unscaled:\",x.round(2))\n",
        "    # Save result as dictionary\n",
        "    out_row = {}\n",
        "    for i in range(len(X_train.columns)):\n",
        "        out_row[X_train.columns[i]] = x[i]\n",
        "\n",
        "    out_row[artist_col] = round(original_val, 2)\n",
        "    out_row['predicted_popularity'] = round(y_pred, 2)\n",
        "    rows.append(out_row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XbGaUe7RTaTo",
      "metadata": {
        "id": "XbGaUe7RTaTo"
      },
      "outputs": [],
      "source": [
        "# Turn results into a DataFrame\n",
        "baseline = pd.DataFrame(rows).round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8Y69bJVDXH-Y",
      "metadata": {
        "id": "8Y69bJVDXH-Y"
      },
      "source": [
        "Display:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6rfSKGkOYQpz",
      "metadata": {
        "id": "6rfSKGkOYQpz"
      },
      "outputs": [],
      "source": [
        "print(\"Random Predicted Popularity Sweep (0.1 Steps)\")\n",
        "display(prepare_display_df(baseline))\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(baseline[artist_col], baseline['predicted_popularity'], linewidth=1)\n",
        "plt.xlabel('Artist Average Popularity (0-100)')\n",
        "plt.ylabel('Predicted Popularity (0-100)')\n",
        "plt.title('Random Predicted Popularity Sweep (0.1 Steps)')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8OXDmqN7h4wK",
      "metadata": {
        "id": "8OXDmqN7h4wK"
      },
      "source": [
        "**[END]**\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfvolmb5ubCq",
      "metadata": {
        "id": "cfvolmb5ubCq"
      },
      "source": [
        "#**Section 5** - User Interface"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "A493WTIVRqWC",
      "metadata": {
        "id": "A493WTIVRqWC"
      },
      "source": [
        "A tool for **ergonomically** predicting a given songs' popularity"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_kFDhnHnC3p7",
      "metadata": {
        "id": "_kFDhnHnC3p7"
      },
      "source": [
        "##**5.1** Instantiation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rAZ7ooS57Oh4",
      "metadata": {
        "id": "rAZ7ooS57Oh4"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "from functools import partial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Z1gYhIGADsbK",
      "metadata": {
        "id": "Z1gYhIGADsbK"
      },
      "outputs": [],
      "source": [
        "new_df = df.copy()\n",
        "new_df['genre'] = categorical_df['genre']\n",
        "new_df['year'] = categorical_df['year']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[END]**\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ],
      "metadata": {
        "id": "CtBfdRI_djln"
      },
      "id": "CtBfdRI_djln"
    },
    {
      "cell_type": "markdown",
      "id": "2Hz6HLnI9vnd",
      "metadata": {
        "id": "2Hz6HLnI9vnd"
      },
      "source": [
        "##**5.2** User Interface"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zm132SUPE3uK",
      "metadata": {
        "id": "zm132SUPE3uK"
      },
      "source": [
        "###***5.1.1*** - Widget Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kLyP_KzguVbd",
      "metadata": {
        "id": "kLyP_KzguVbd"
      },
      "source": [
        "####**Functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RfbuQ1XpFW7a",
      "metadata": {
        "id": "RfbuQ1XpFW7a"
      },
      "source": [
        "**Defining function *build_feature_section()*** to create and format a feature input section using sliders for each continuous feature:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Layout sliders in two vertical columns\n",
        "def build_feature_section(sliders):\n",
        "  midpoint = len(sliders) // 2\n",
        "  slider_section = widgets.HBox([\n",
        "      widgets.VBox(sliders[:midpoint]),\n",
        "      widgets.VBox(sliders[midpoint:])])\n",
        "\n",
        "  return slider_section"
      ],
      "metadata": {
        "id": "JFtbOFRdnWvJ"
      },
      "id": "JFtbOFRdnWvJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n"
      ],
      "metadata": {
        "id": "r1uYfkMCnXfZ"
      },
      "id": "r1uYfkMCnXfZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kvdQm_JNEAAW",
      "metadata": {
        "id": "kvdQm_JNEAAW"
      },
      "outputs": [],
      "source": [
        "# function to create sliders for selected features with realistic default ranges\n",
        "def create_sliders(features):\n",
        "\n",
        "    inputs = {}     # stores sliders by feature name\n",
        "    sliders = []    # stores widgets for layout\n",
        "\n",
        "    # go through each feature we want to create a slider for\n",
        "    for feature_name in features:\n",
        "\n",
        "        # set custom ranges for loudness\n",
        "        if feature_name == 'loudness':\n",
        "            fmin = -60       # lowest expected dB\n",
        "            fmax = 0         # upper bound for dB scale\n",
        "            label = 'Loudness (dB)'\n",
        "            default = -20    # somewhere in the middle\n",
        "\n",
        "        # set custom ranges for tempo\n",
        "        elif feature_name == 'tempo':\n",
        "            fmin = 50\n",
        "            fmax = 220\n",
        "            label = 'Tempo (BPM)'\n",
        "            default = 120    # reasonable tempo baseline\n",
        "\n",
        "        # set custom ranges for duration in minutes\n",
        "        elif feature_name == 'duration_mins':\n",
        "            fmin = 0\n",
        "            fmax = 20\n",
        "            label = 'Duration (min)'\n",
        "            default = 3.5    # reasonable track length\n",
        "\n",
        "        else:\n",
        "            # default to 0–1 for all other already-normalised features\n",
        "            fmin = 0\n",
        "            fmax = 1\n",
        "            label = feature_name\n",
        "            default = 0.5\n",
        "\n",
        "        # create the FloatSlider for current feature\n",
        "        slider = widgets.FloatSlider(\n",
        "            description=label,\n",
        "            min=fmin,\n",
        "            max=fmax,\n",
        "            step=round((fmax - fmin) / 100, 3),  # make the step size 1% of the range\n",
        "            value=default,\n",
        "            style={'description_width': '180px'},\n",
        "            layout=widgets.Layout(width='400px')\n",
        "        )\n",
        "\n",
        "        inputs[feature_name] = slider   # store for later use\n",
        "        sliders.append(widgets.HBox([slider]))  # wrap in HBox for layout\n",
        "\n",
        "    return inputs, sliders\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ],
      "metadata": {
        "id": "hZqdpabXmrZr"
      },
      "id": "hZqdpabXmrZr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining function *build_artist_section*** for building the seperate input sliders for the important artist-level inputs:"
      ],
      "metadata": {
        "id": "aeLv3lpbmtZ5"
      },
      "id": "aeLv3lpbmtZ5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_OMgqhdfEIQN",
      "metadata": {
        "id": "_OMgqhdfEIQN"
      },
      "outputs": [],
      "source": [
        "def build_artist_section():\n",
        "  inputs = {}     # store sliders by name and info\n",
        "\n",
        "  # create artist input widgets\n",
        "  aap_slider = widgets.FloatSlider(\n",
        "      description='Artist Avg. Popularity',\n",
        "      min=0,\n",
        "      max=100,\n",
        "      step=0.001,\n",
        "      value=50,\n",
        "      style={'description_width': '120px'},\n",
        "      layout=widgets.Layout(width='500px')\n",
        "  )\n",
        "\n",
        "  song_count_slider = widgets.BoundedIntText(\n",
        "      description='Artist Song Count',\n",
        "      value=100,\n",
        "      min=0,\n",
        "      max=3000,\n",
        "      style={'description_width': '120px'},\n",
        "      layout=widgets.Layout(width='500px')\n",
        "  )\n",
        "\n",
        "  artist_section = widgets.HBox([aap_slider, song_count_slider])\n",
        "  inputs['artist_avg_popularity'] = aap_slider   # store for later use\n",
        "  inputs['artist_song_count'] = song_count_slider   # store for later use\n",
        "\n",
        "  return inputs, artist_section"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ],
      "metadata": {
        "id": "N4ufiU0SnDfr"
      },
      "id": "N4ufiU0SnDfr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining function *build_dropdown_section*** for building the dropdown menus for categorical inputs:"
      ],
      "metadata": {
        "id": "7s2BF39-nEVz"
      },
      "id": "7s2BF39-nEVz"
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dropdown_section():\n",
        "  inputs = {}\n",
        "\n",
        "  genre_input, genre_dropdown = create_dropdown(genre_columns, 'Genre')\n",
        "  year_input, year_dropdown = create_dropdown(year_columns, 'Year')\n",
        "  key_input, key_dropdown = create_dropdown(key_columns, 'Key')\n",
        "  mode_input, mode_dropdown = create_dropdown(mode_columns, 'Mode')\n",
        "  time_sig_input, time_sig_dropdown = create_dropdown(time_sig_columns, 'Time Signature', default=1)\n",
        "\n",
        "  dropdown_section = widgets.VBox([\n",
        "      genre_dropdown,\n",
        "      year_dropdown,\n",
        "      key_dropdown,\n",
        "      mode_dropdown,\n",
        "      time_sig_dropdown])\n",
        "\n",
        "  inputs['genre_input'] = genre_input\n",
        "  inputs['year_input'] = year_input\n",
        "  inputs['key_input'] = key_input\n",
        "  inputs['mode_input'] = mode_input\n",
        "  inputs['time_sig_input'] = time_sig_input\n",
        "\n",
        "  return inputs, dropdown_section"
      ],
      "metadata": {
        "id": "0eapy8sGvOqY"
      },
      "id": "0eapy8sGvOqY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ],
      "metadata": {
        "id": "CvxcR5IOxzo9"
      },
      "id": "CvxcR5IOxzo9"
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropdown helper function\n",
        "def clean_label(col):\n",
        "    return (col.replace('genre_', '').replace('year_', '').replace('keyOf', '')\n",
        "           ).replace('time_signature_0', '0').replace('time_signature_14','1=1').replace('time_signature_24','2/4'\n",
        "           ).replace('time_signature_34','3/4').replace('time_signature_44','4/4').replace('time_signature_54','5/4')\n"
      ],
      "metadata": {
        "id": "cnMlZQn73ZFF"
      },
      "id": "cnMlZQn73ZFF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function to format a dropdown widget from a set of one-hot encoded columns\n",
        "def format_dropdown(column_group, index):\n",
        "\n",
        "  options = []\n",
        "\n",
        "  # go through all columns in the group and\n",
        "  # clean up the label if it's not excluded\n",
        "\n",
        "  for col in column_group:\n",
        "    label = clean_label(col)\n",
        "    options.append(label)\n",
        "\n",
        "  # create a dropdown with the cleaned labels, defaulting to whatever index we pass\n",
        "  dropdown = widgets.Dropdown(options=options, value=options[index])\n",
        "\n",
        "  # building a reverse map (essentially a lookup dictionary for translating between formatted UI\n",
        "  # variable names and actual variable names) for converting cleaned labels back to original column\n",
        "  reverse_map = {}\n",
        "  for col in column_group:\n",
        "      label = clean_label(col)\n",
        "      reverse_map[label] = col\n",
        "\n",
        "  return dropdown, reverse_map"
      ],
      "metadata": {
        "id": "U3fj5ZnW3SDU"
      },
      "id": "U3fj5ZnW3SDU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ],
      "metadata": {
        "id": "AftuGvfe3aNs"
      },
      "id": "AftuGvfe3aNs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QjCDaAllEPS8",
      "metadata": {
        "id": "QjCDaAllEPS8"
      },
      "outputs": [],
      "source": [
        "#Organise dropdown layout for clarity\n",
        "def create_dropdown(columns, label, default=0):\n",
        "  dropdown, reverse_map = format_dropdown(columns, default)\n",
        "\n",
        "  return dropdown, widgets.HBox([widgets.Label(f\"{label}:\", layout=widgets.Layout(width=\"120px\")), dropdown])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ],
      "metadata": {
        "id": "BlACP677vbEE"
      },
      "id": "BlACP677vbEE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assembly**"
      ],
      "metadata": {
        "id": "ZQ1GHbYGiVyk"
      },
      "id": "ZQ1GHbYGiVyk"
    },
    {
      "cell_type": "code",
      "source": [
        "columns = [\n",
        "      'danceability', 'energy', 'loudness', 'speechiness', 'acousticness',\n",
        "      'instrumentalness', 'liveness', 'valence', 'tempo', 'duration_mins']\n",
        "\n",
        "feature_inputs, feature_sliders = create_sliders(columns)\n",
        "slider_section = build_feature_section(feature_sliders)"
      ],
      "metadata": {
        "id": "TzNbY_0EiYXr"
      },
      "id": "TzNbY_0EiYXr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "artist_inputs, artist_section = build_artist_section()"
      ],
      "metadata": {
        "id": "ODCqfA-Aicos"
      },
      "id": "ODCqfA-Aicos",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n"
      ],
      "metadata": {
        "id": "tvDP5PEOi-yN"
      },
      "id": "tvDP5PEOi-yN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collecting Categorical Columns:"
      ],
      "metadata": {
        "id": "nCN_ZJ_git3_"
      },
      "id": "nCN_ZJ_git3_"
    },
    {
      "cell_type": "code",
      "source": [
        "# loop through all column names in X_train and collect genre columns\n",
        "excluded_cols = ['artist_avg_popularity', 'genre_avg_popularity', 'year_avg_popularity',\n",
        "                 'artist_song_count', 'genre_song_count', 'year_song_count']\n",
        "\n",
        "\n",
        "genre_columns = []\n",
        "for column in X_train.columns:\n",
        "  if column.startswith('genre_') and column not in excluded_cols:\n",
        "    genre_columns.append(column)\n",
        "\n",
        "# same as above, but for year columns\n",
        "year_columns = []\n",
        "for column in X_train.columns :\n",
        "  if column.startswith('year_') and column not in excluded_cols:\n",
        "    year_columns.append(column)\n",
        "\n",
        "# extract key columns that start with 'keyOf'\n",
        "key_columns = []\n",
        "for column in X_train.columns:\n",
        "  if column.startswith('keyOf'):\n",
        "    key_columns.append(column)\n",
        "\n",
        "# mode columns are just major and minor, so we can grab those directly\n",
        "mode_columns = X_train[['major', 'minor']]\n",
        "\n",
        "# get all time signature columns\n",
        "time_sig_columns = []\n",
        "for column in X_train.columns:\n",
        "  if column.startswith('time_signature'):\n",
        "    time_sig_columns.append(column)\n"
      ],
      "metadata": {
        "id": "HrPWV8skioTQ"
      },
      "id": "HrPWV8skioTQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dropdown_inputs, dropdown_section = build_dropdown_section()"
      ],
      "metadata": {
        "id": "mXnogB_yjEBb"
      },
      "id": "mXnogB_yjEBb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        ".\n",
        "\n",
        ".\n"
      ],
      "metadata": {
        "id": "hr5_G_v_hIgp"
      },
      "id": "hr5_G_v_hIgp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating *predict_button*:"
      ],
      "metadata": {
        "id": "1F_SxCTAhKnb"
      },
      "id": "1F_SxCTAhKnb"
    },
    {
      "cell_type": "code",
      "source": [
        "# Buttons and output\n",
        "predict_button = widgets.Button(description='Predict Popularity', button_style='success')\n",
        "out = widgets.Output()"
      ],
      "metadata": {
        "id": "DfgsMqTsjcVu"
      },
      "id": "DfgsMqTsjcVu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assembling the UI:"
      ],
      "metadata": {
        "id": "gX8iNY8shPYD"
      },
      "id": "gX8iNY8shPYD"
    },
    {
      "cell_type": "code",
      "source": [
        "# Assemble full UI layout\n",
        "ui = widgets.VBox([\n",
        "    slider_section,\n",
        "    artist_section,\n",
        "    dropdown_section,\n",
        "    widgets.HBox([predict_button]),\n",
        "    out\n",
        "])"
      ],
      "metadata": {
        "id": "DEaGiw6mjcyU"
      },
      "id": "DEaGiw6mjcyU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[END]**\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ],
      "metadata": {
        "id": "IDDZ3PPcx6vX"
      },
      "id": "IDDZ3PPcx6vX"
    },
    {
      "cell_type": "markdown",
      "id": "GoE7ZVLT4dRt",
      "metadata": {
        "id": "GoE7ZVLT4dRt"
      },
      "source": [
        "##**5.3** Predict Button Logic"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "J352Utf3FPK8",
      "metadata": {
        "id": "J352Utf3FPK8"
      },
      "source": [
        "###***5.3.1*** - Defining on-button-click *on_predict()* Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B8Y_GeZZ4grE",
      "metadata": {
        "id": "B8Y_GeZZ4grE"
      },
      "outputs": [],
      "source": [
        "def on_predict(b):\n",
        "  # clear previous output in widget box\n",
        "  with out:\n",
        "    clear_output()\n",
        "\n",
        "    # run helper function to get the prediction (already normalised & ready)\n",
        "    raw_pred = get_prediction()\n",
        "\n",
        "    # display prediction results\n",
        "    #print(f\"\\nRaw model output (0-1): {raw_pred:.6f}\")\n",
        "    print(f\"Predicted Popularity: {np.clip(raw_pred * 100, 0, 100):.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "K4UYiAJorJL1",
      "metadata": {
        "id": "K4UYiAJorJL1"
      },
      "source": [
        "###***5.3.2*** - Defining *get_prediction()* Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qKoJ3PXZo9BB",
      "metadata": {
        "id": "qKoJ3PXZo9BB"
      },
      "outputs": [],
      "source": [
        "def get_prediction():\n",
        "  #print(feature_inputs)\n",
        "  #print(\"\\n\\n\",artist_inputs)\n",
        "  #print(\"\\n\\n\",dropdown_inputs)\n",
        "\n",
        "  genre_to_avg_pop = new_df.groupby('genre')['popularity'].mean().to_dict()\n",
        "  genre_to_song_count = new_df.groupby('genre')['popularity'].count().to_dict()\n",
        "  year_to_avg_pop = new_df.groupby('year')['popularity'].mean().to_dict()\n",
        "  year_to_song_count = new_df.groupby('year')['popularity'].count().to_dict()\n",
        "\n",
        "  user_vals = {}            # One-hot encoded features\n",
        "  input_vals_raw = {}       # Raw numeric and metadata inputs\n",
        "\n",
        "  # Fixed continuous inputs\n",
        "  for c, slider in feature_inputs.items():\n",
        "    input_vals_raw[c] = slider.value\n",
        "\n",
        "  # Artist-level inputs\n",
        "  aap_input = artist_inputs['artist_avg_popularity']\n",
        "  song_count_input = artist_inputs['artist_song_count']\n",
        "\n",
        "  input_vals_raw['artist_avg_popularity'] = aap_input.value\n",
        "  input_vals_raw['artist_song_count'] = song_count_input.value\n",
        "\n",
        "  # Genre\n",
        "  genre_dropdown = dropdown_inputs['genre_input']\n",
        "  genre_name = genre_dropdown.value\n",
        "  user_vals['genre_' + genre_name] = 1.0\n",
        "  input_vals_raw['genre_avg_popularity'] = genre_to_avg_pop.get(genre_name, 0.0)\n",
        "  input_vals_raw['genre_song_count'] = genre_to_song_count.get(genre_name, 0.0)\n",
        "\n",
        "  # Year\n",
        "  year_dropdown = dropdown_inputs['year_input']\n",
        "  year_val = year_dropdown.value\n",
        "  user_vals['year_' + year_val] = 1.0\n",
        "  input_vals_raw['year_avg_popularity'] = year_to_avg_pop.get(int(year_val), 0.0)\n",
        "  input_vals_raw['year_song_count'] = year_to_song_count.get(int(year_val), 0.0)\n",
        "\n",
        "\n",
        "  # Key, mode, time_sig\n",
        "  key_dropdown = dropdown_inputs['key_input']\n",
        "  mode_dropdown = dropdown_inputs['mode_input']\n",
        "  time_sig_dropdown = dropdown_inputs['time_sig_input']\n",
        "\n",
        "  user_vals['keyOf' + key_dropdown.value] = 1.0\n",
        "  user_vals[mode_dropdown.value] = 1.0\n",
        "  user_vals['time_signature_' + time_sig_dropdown.value.replace('/','')] = 1.0\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  print(f\"loudness = {input_vals_raw['loudness']}\")\n",
        "  print(f\"tempo = {input_vals_raw['tempo']}\")\n",
        "  print(f\"duration_mins = {input_vals_raw['duration_mins']}\")\n",
        "  print(f\"artist_avg_popularity = {input_vals_raw['artist_avg_popularity']}\")\n",
        "  print(f\"artist_song_count = {input_vals_raw['artist_song_count']}\")\n",
        "  print(f\"\\ngenre = {genre_dropdown.value}\")\n",
        "  print(f\"genre_avg_popularity = {input_vals_raw['genre_avg_popularity']}\")\n",
        "  print(f\"genre_song_count = {input_vals_raw['genre_song_count']}\")\n",
        "  print(f\"\\nyear = {year_dropdown.value}\")\n",
        "  print(f\"year_avg_popularity = {input_vals_raw['year_avg_popularity']}\")\n",
        "  print(f\"year_song_count = {input_vals_raw['year_song_count']}\")\n",
        "  print(f\"keyOf = {key_dropdown.value}\")\n",
        "  print(f\"mode = {mode_dropdown.value}\")\n",
        "  print(f\"time_signature = {time_sig_dropdown.value}\")\n",
        "  print(f\"user_vals = {user_vals}\")\n",
        "  \"\"\"\n",
        "\n",
        "  # Apply manual scaling from Section 1\n",
        "\n",
        "  input_vals_raw['loudness'] = np.clip(input_vals_raw['loudness'], -60, 0)\n",
        "  input_vals_raw['loudness'] = (input_vals_raw['loudness'] + 60) / 60\n",
        "\n",
        "\n",
        "  input_vals_raw['tempo'] = np.clip(input_vals_raw['tempo'], 50, 220)\n",
        "  input_vals_raw['tempo'] = (input_vals_raw['tempo'] - 50) / 170\n",
        "\n",
        "\n",
        "  input_vals_raw['duration_mins'] = np.clip(input_vals_raw['duration_mins'], 0, 20)\n",
        "  input_vals_raw['duration_mins'] = input_vals_raw['duration_mins'] / 20\n",
        "\n",
        "  input_vals_raw['artist_avg_popularity'] = np.clip(input_vals_raw['artist_avg_popularity'], 0, 100) / 100\n",
        "\n",
        "  input_vals_raw['artist_song_count'] = np.clip(input_vals_raw['artist_song_count'], 0, 3000) / 3000\n",
        "\n",
        "  input_vals_raw['genre_avg_popularity'] = np.clip(input_vals_raw['genre_avg_popularity'], 0, 100) / 100\n",
        "\n",
        "  input_vals_raw['genre_song_count'] = np.clip(input_vals_raw['genre_song_count'], 0, 30000) / 30000\n",
        "\n",
        "  input_vals_raw['year_avg_popularity'] = np.clip(input_vals_raw['year_avg_popularity'], 0, 100) / 100\n",
        "\n",
        "  input_vals_raw['year_song_count'] = np.clip(input_vals_raw['year_song_count'], 0, 50000) / 50000\n",
        "\n",
        "\n",
        "  # Assemble input for model\n",
        "\n",
        "  #print(user_vals)\n",
        "  #print(input_vals_raw)\n",
        "\n",
        "  input_df = pd.DataFrame([{**input_vals_raw, **user_vals}])\n",
        "  input_df = input_df.reindex(columns=X_train.columns, fill_value=0.0)\n",
        "\n",
        "  prediction = float(model.predict(input_df,verbose=False)[0][0])\n",
        "\n",
        "  display_pred = input_df.loc[:, input_df.ne(0).any(axis=0)]\n",
        "  display_pred.insert(0, 'popularity', prediction)\n",
        "  display(display_pred)\n",
        "  return prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[END]**\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ],
      "metadata": {
        "id": "pFZfySyvc0LA"
      },
      "id": "pFZfySyvc0LA"
    },
    {
      "cell_type": "markdown",
      "id": "VlkvpMds96nV",
      "metadata": {
        "id": "VlkvpMds96nV"
      },
      "source": [
        "#**APPLICATION**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###LOADING"
      ],
      "metadata": {
        "id": "3ePlQzNooM9G"
      },
      "id": "3ePlQzNooM9G"
    },
    {
      "cell_type": "markdown",
      "source": [
        "ensure to load spotidata.csv and model.keras into the environment, and then run Section 1, Section 5 and APPLICATION in order to use the UI."
      ],
      "metadata": {
        "id": "e9ylY7euoYsw"
      },
      "id": "e9ylY7euoYsw"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "model = tf.keras.models.load_model('model.keras')"
      ],
      "metadata": {
        "id": "w8jiMBJdkatj"
      },
      "id": "w8jiMBJdkatj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = train_df.drop(labels='popularity', axis=1)\n",
        "\n",
        "y = train_df['popularity']\n",
        "\n",
        "#Train and Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, shuffle=True, random_state=0)"
      ],
      "metadata": {
        "id": "Qxg8uWXtoGCG"
      },
      "id": "Qxg8uWXtoGCG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###UI"
      ],
      "metadata": {
        "id": "SpcziqfWoWIL"
      },
      "id": "SpcziqfWoWIL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IcD3Y6ADq8e4",
      "metadata": {
        "id": "IcD3Y6ADq8e4"
      },
      "outputs": [],
      "source": [
        "predict_button.on_click(on_predict)\n",
        "display(ui)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "fM-9ubS8TajR",
        "LPFtYj5xSDqv",
        "OIU6zCKySfIQ",
        "F0xekmaQTlOx",
        "GUDi3O8IHwe6",
        "IX3UIu1SIHR1",
        "HZhyPkbaJbwM",
        "V7oSLtdP7KQm",
        "Oh3FVjqd7XI3",
        "dAEZLJae7pD9",
        "z9v9RyiKzCtO",
        "rHhyneaIXmAV",
        "EYaP_uX-92Rz",
        "SYa7QRbLox9K",
        "mc3odKJXol78",
        "o2Bj84VPdJq3",
        "eaW1if4zoIKM",
        "L4scBdq_oyzn",
        "_vGAgzYAztqv",
        "FMDeREgbTxUS",
        "qc7FCYu5-fdk",
        "EtAC3wBL2DA9",
        "aCCnYK-EpDIb",
        "uDXGe8I22VY_",
        "V3dT7plB2b7u",
        "UBbOvY_g5YsJ",
        "0GJ9qfbA5jZm",
        "uvmtP4X7zbL1",
        "f5UG_RYOzxes",
        "oPqCTIQ45xIo",
        "NGb5P-g9wnuV",
        "T-Z0pxczyGFs",
        "ovd54EnXUex7",
        "thMk3Nk4VT77",
        "g13cDV4wgBlr",
        "ygifQpLhbsQC",
        "eFYZ80sUcSP4",
        "qlKDw1KK60Sp",
        "ObfLXlaMr_SR",
        "_d4iBDC-4Zyo",
        "RNrRHqBn4nEh",
        "kJiQf7lb4vLK",
        "Ae9WM3fp42oq",
        "U01xco-_5Aj2",
        "ZkJrP2DA6kQd",
        "Dk5mHWp3mDKN",
        "lG9oVj5fJqmb",
        "fr5d2f_Npid2",
        "A_MT-p0IIhLs",
        "MXJ9AO58I-4H",
        "JHakntTIpxkB",
        "dbjoatMDXpo2",
        "BOor6n932Tfo",
        "4uC8p630N6aj",
        "sqX_nctL2bpj",
        "EPKfVQIIEN3-",
        "ckv6fYRbGOtJ",
        "WMQaGirZGfSy",
        "1I9NIJZlueud",
        "jvLzP3fqpyVz",
        "bquxpr8qqGqW",
        "nXL5MAuz1UAy",
        "oQhk2yB1qwM9",
        "zTh26CcaJIg7",
        "0usxYpmTaTlO",
        "cuLGRbXTalV5",
        "Af1lZ02ahknv",
        "4kmHdDZkh412",
        "ZoxUOt_YiXSN",
        "JN216zbKldI-",
        "57TIxFNolJ5m",
        "OeiC5w_rKNgg",
        "cPENW6RlCWGz",
        "qvKFXHi8Cq-r",
        "tia9-DxyKhE0",
        "8-tOtGuZK0Kt",
        "mOibNzhy1WVG",
        "gSicQZOIsB4Y",
        "h8pVhzO3t2cP",
        "PnN8H_EZuDqF",
        "IcIvX-MbmDMw",
        "C5ixzlgQlXZp",
        "TlxqOcqHHqLq",
        "cfvolmb5ubCq",
        "_kFDhnHnC3p7",
        "2Hz6HLnI9vnd",
        "zm132SUPE3uK",
        "kLyP_KzguVbd",
        "GoE7ZVLT4dRt",
        "J352Utf3FPK8",
        "K4UYiAJorJL1",
        "VlkvpMds96nV",
        "3ePlQzNooM9G",
        "SpcziqfWoWIL"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}